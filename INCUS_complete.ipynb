{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b33d6d7",
   "metadata": {},
   "source": [
    "# Notebook Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9cc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib as mpl\n",
    "#import matplotlib.colors as colors\n",
    "import datetime #\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "#from RAMS_Post_Process import fx_postproc_RAMS as RAMS_fx\n",
    "import cartopy.crs as crs\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "matplotlib.rcParams[\"font.family\"] = \"Roboto\"\n",
    "matplotlib.rcParams[\"font.sans-serif\"] = [\"Roboto\"]  # For non-unicode text\n",
    "#matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 16\n",
    "matplotlib.rcParams['axes.titlesize'] = 16\n",
    "matplotlib.rcParams['xtick.labelsize'] = 16\n",
    "matplotlib.rcParams['ytick.labelsize'] = 16\n",
    "matplotlib.rcParams['legend.fontsize'] = 16\n",
    "matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "#matplotlib.rcParams['hatch.linewidth'] = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib import ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrf import (\n",
    "    CoordPair,\n",
    "    GeoBounds,\n",
    "    cartopy_xlim,\n",
    "    cartopy_ylim,\n",
    "    get_cartopy,\n",
    "    getvar,\n",
    "    interplevel,\n",
    "    interpline,\n",
    "    latlon_coords,\n",
    "    ll_to_xy,\n",
    "    smooth2d,\n",
    "    to_np,\n",
    "    vertcross,\n",
    "    xy_to_ll,\n",
    "    ll_to_xy_proj,\n",
    "    cape_2d\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_colormap():\n",
    "    nws_reflectivity_colors = [\n",
    "    \"#646464\", # ND\n",
    "    \"#ccffff\", # -30\n",
    "    \"#cc99cc\", # -25\n",
    "    \"#996699\", # -20\n",
    "    \"#663366\", # -15\n",
    "    \"#cccc99\", # -10\n",
    "    \"#999966\", # -5\n",
    "    \"#646464\", # 0\n",
    "    \"#04e9e7\", # 5\n",
    "    \"#019ff4\", # 10\n",
    "    \"#0300f4\", # 15\n",
    "    \"#02fd02\", # 20\n",
    "    \"#01c501\", # 25\n",
    "    \"#008e00\", # 30\n",
    "    \"#fdf802\", # 35\n",
    "    \"#e5bc00\", # 40\n",
    "    \"#fd9500\", # 45\n",
    "    \"#fd0000\", # 50\n",
    "    \"#d40000\", # 55\n",
    "    \"#bc0000\", # 60\n",
    "    \"#f800fd\", # 65\n",
    "    \"#9854c6\", # 70\n",
    "    \"#fdfdfd\" # 75\n",
    "    ]\n",
    "\n",
    "    return mpl.colors.ListedColormap(nws_reflectivity_colors)\n",
    "\n",
    "\n",
    "cma1=plt.get_cmap('bwr')\n",
    "cma2=radar_colormap()\n",
    "cma3=plt.get_cmap('tab20c')\n",
    "import nclcmaps as ncm\n",
    "cma4=ncm.cmap(\"WhiteBlueGreenYellowRed\")\n",
    "cma5=plt.get_cmap('gray_r')\n",
    "cma6=plt.get_cmap('rainbow')\n",
    "cma7=plt.get_cmap('Oranges')\n",
    "cma8=plt.get_cmap('coolwarm')\n",
    "#cma9=cma4.reversed()\n",
    "cma10=plt.get_cmap('gist_yarg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19718cfa",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_process_vert(AX, TER, CONTOUR, XY1, XY2, Y_CROSS, CBAR_EXP, TITLESTRING, TIMESTRING, FILENAMESTRING, PROD, UNITS, VERT_CROSS, HEIGHT, YLABEL, IS_PANEL_PLOT):\n",
    "    #F = plt.gcf()  # Gets the current figure\n",
    "    #ax = plt.gca()  # Gets the current axes\n",
    "\n",
    "    if IS_PANEL_PLOT == False:\n",
    "        AX.set_title('%s (%s) \\n %s' % (TITLESTRING, UNITS, TIMESTRING),\n",
    "                  fontsize=16, stretch='normal')\n",
    "\n",
    "    if VERT_CROSS == \"zonal\":\n",
    "        AX.fill_between(xh[XY1:XY2], 0, TER[int(Y_CROSS),\n",
    "                        XY1:XY2]/1000.0, facecolor='wheat')\n",
    "        AX.set_xlabel('x-distance (m)', fontsize=16)\n",
    "\n",
    "    else:\n",
    "        AX.fill_between(yh[XY1:XY2], 0, TER[XY1:XY2,\n",
    "                        int(Y_CROSS)]/1000.0, facecolor='wheat')\n",
    "        AX.set_xlabel('y-distance (m)', fontsize=16)\n",
    "\n",
    "    AX.patch.set_color(\"white\")\n",
    "\n",
    "    if YLABEL:\n",
    "        AX.set_ylabel('Height (km)', fontsize=18)\n",
    "    AX.set_ylim([0, HEIGHT])\n",
    "    AX.set_xlim([XY1*100.0, XY2*100.0])\n",
    "\n",
    "    class OOMFormatter(matplotlib.ticker.ScalarFormatter):\n",
    "        def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n",
    "            self.oom = order\n",
    "            self.fformat = fformat\n",
    "            matplotlib.ticker.ScalarFormatter.__init__(\n",
    "                self, useOffset=offset, useMathText=mathText)\n",
    "\n",
    "        def _set_orderOfMagnitude(self, nothing):\n",
    "            self.orderOfMagnitude = self.oom\n",
    "\n",
    "        def _set_format(self, vmin, vmax):\n",
    "            self.format = self.fformat\n",
    "            if self._useMathText:\n",
    "                self.format = '$%s$' % matplotlib.ticker._mathdefault(\n",
    "                    self.format)\n",
    "\n",
    "    if IS_PANEL_PLOT == False:\n",
    "        if abs(CBAR_EXP):\n",
    "            divider = make_axes_locatable(AX)\n",
    "            cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.6)\n",
    "            bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",\n",
    "                               format=OOMFormatter(CBAR_EXP, mathText=False),extend='both')\n",
    "            bar.ax.tick_params(labelsize=15)\n",
    "            #file_id = '%s_%s' % (PROD, FILENAMESTRING)\n",
    "            #filename = '%s.png' % (file_id)\n",
    "            #print(filename)\n",
    "            # Saves the figure with small margins\n",
    "            #plt.savefig(filename, dpi=my_dpi, bbox_inches='tight')\n",
    "        else:\n",
    "            divider = make_axes_locatable(AX)\n",
    "            cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.66)\n",
    "            bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",extend='both')\n",
    "            bar.ax.tick_params(labelsize=15)\n",
    "            #file_id = '%s_%s' % (PROD, FILENAMESTRING)\n",
    "            #filename = '%s.png' % (file_id)\n",
    "            #print(filename)\n",
    "            # Saves the figure with small margins\n",
    "            #plt.savefig(filename, dpi=my_dpi, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if abs(CBAR_EXP):\n",
    "            divider = make_axes_locatable(AX)\n",
    "            cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.4)\n",
    "            bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",\n",
    "                               format=OOMFormatter(CBAR_EXP, mathText=False),extend='both')\n",
    "            bar.ax.tick_params(labelsize=15)\n",
    "        else:\n",
    "            divider = make_axes_locatable(AX)\n",
    "            cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.4)\n",
    "            bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",extend='both')\n",
    "            bar.ax.tick_params(labelsize=15)\n",
    "    # plt.close() This should remain commented. plt should be closed in the panel plot function\n",
    "    # if export_flag == 1:\n",
    "    # Convert the figure to a gif file\n",
    "    #os.system('convert -render -flatten %s %s.gif' % (filename, file_id))\n",
    "    #os.system('rm -f %s' % filename)\n",
    "    \n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def read_head(headfile,h5file):\n",
    "        # Function that reads header files from RAMS\n",
    "\n",
    "        # Inputs:\n",
    "        #   headfile: header file including full path in str format\n",
    "        #   h5file: h5 datafile including full path in str format\n",
    "\n",
    "        # Returns:\n",
    "        #   zmn: height levels for momentum values (i.e., grid box upper and lower levels)\n",
    "        #   ztn: height levels for thermodynaic values (i.e., grid box centers)\n",
    "        #   nx:: the number of x points for the domain associated with the h5file\n",
    "        #   ny: the number of y points for the domain associated with the h5file\n",
    "        #   npa: the number of surface patches\n",
    "\n",
    "\n",
    "        dom_num = h5file[h5file.index('.h5')-1] # Find index of .h5 to determine position showing which nest domain to use\n",
    "\n",
    "        with open(headfile) as f:\n",
    "            contents = f.readlines()\n",
    "\n",
    "        idx_zmn = contents.index('__zmn0'+dom_num+'\\n')\n",
    "        nz_m = int(contents[idx_zmn+1])\n",
    "        zmn = np.zeros(nz_m)\n",
    "        for i in np.arange(0,nz_m):\n",
    "            zmn[i] =  float(contents[idx_zmn+2+i])\n",
    "\n",
    "        idx_ztn = contents.index('__ztn0'+dom_num+'\\n')\n",
    "        nz_t = int(contents[idx_ztn+1])\n",
    "        ztn = np.zeros(nz_t)\n",
    "        for i in np.arange(0,nz_t):\n",
    "            ztn[i] =  float(contents[idx_ztn+2+i])\n",
    "\n",
    "        ztop = np.max(ztn) # Model domain top (m)\n",
    "\n",
    "        # Grad the size of the horizontal grid spacing\n",
    "        idx_dxy = contents.index('__deltaxn\\n')\n",
    "        dxy = float(contents[idx_dxy+1+int(dom_num)].strip())\n",
    "\n",
    "        idx_npatch = contents.index('__npatch\\n')\n",
    "        npa = int(contents[idx_npatch+2])\n",
    "\n",
    "        idx_ny = contents.index('__nnyp\\n')\n",
    "        idx_nx = contents.index('__nnxp\\n')\n",
    "        ny = np.ones(int(contents[idx_ny+1]))\n",
    "        nx = np.ones(int(contents[idx_ny+1]))\n",
    "        for i in np.arange(0,len(ny)):\n",
    "            nx[i] = int(contents[idx_nx+2+i])\n",
    "            ny[i] = int(contents[idx_ny+2+i])\n",
    "\n",
    "        ny_out = ny[int(dom_num)-1]\n",
    "        nx_out = nx[int(dom_num)-1]\n",
    "\n",
    "        return zmn, ztn, nx_out, ny_out, dxy, npa "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f3c6d-3187-49d7-b99b-e4bf66bdfef5",
   "metadata": {},
   "source": [
    "## Arrange figure files (png) into a panel plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c63588-3c32-4e42-aa51-39cdc1c3c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "\n",
    "def arrange_images_with_wildcard(input_folder, output_file, wildcard_pattern, non_target_string):\n",
    "    # Get a list of PNG images in the input folder matching the wildcard pattern\n",
    "    if non_target_string:\n",
    "        image_files = sorted([f for f in glob.glob(os.path.join(input_folder, wildcard_pattern)) if f.lower().endswith('.png') and non_target_string not in f])[1::2]\n",
    "    else:\n",
    "        image_files = sorted([f for f in glob.glob(os.path.join(input_folder, wildcard_pattern)) if f.lower().endswith('.png')])[1::2]\n",
    "\n",
    "    print('found ',len(image_files),' images')\n",
    "    for fil in image_files:\n",
    "        print(fil)\n",
    "    # Check if there are any matching images\n",
    "    if not image_files:\n",
    "        print(f\"Error: No PNG images matching the wildcard pattern '{wildcard_pattern}' found in the folder.\")\n",
    "        return\n",
    "\n",
    "    # Calculate the number of rows and columns for the matrix\n",
    "    num_images = len(image_files)\n",
    "    num_cols = int(math.sqrt(num_images))\n",
    "    num_rows = math.ceil(num_images / num_cols)\n",
    "\n",
    "    # Create a new image with dimensions for the matrix and reduced white space\n",
    "    img_width, img_height = Image.open(image_files[0]).size\n",
    "    margin = 60  # Adjust this value to control the margin\n",
    "    result_image = Image.new('RGB', (num_cols * (img_width - margin), num_rows * (img_height - margin)))\n",
    "\n",
    "    # Loop through the matching images and paste them onto the result image with reduced white space\n",
    "    for i in range(num_images):\n",
    "        img = Image.open(image_files[i])\n",
    "\n",
    "        # Calculate the position with margin to paste the image\n",
    "        col = i % num_cols\n",
    "        row = i // num_cols\n",
    "        position = (col * (img_width - margin), row * (img_height - margin))\n",
    "\n",
    "        # Paste the image onto the result image\n",
    "        result_image.paste(img, position)\n",
    "\n",
    "    # Save the result image\n",
    "    result_image.save(output_file)\n",
    "\n",
    "# Example usage\n",
    "input_folder = \"./\"\n",
    "output_file = \"WSPD_lev0.png\"\n",
    "wildcard_pattern = \"plan_view_RAMS_*_WSPD_levtype_pressure_lev_750_d03_time_*.png\"  # Replace with your specific wildcard pattern\n",
    "non_target_string = 'ARG1.1'\n",
    "arrange_images_with_wildcard(input_folder, output_file, wildcard_pattern,non_target_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659307c-1ee4-4ef8-bcf6-f997873bc4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/monsoon/MODEL/LES_MODEL_DATA/V0/ARG1.2-WM-V0/G1/wrfout_d01_2018-12-13_20:30:00'\n",
    "da = xr.open_dataset(filepath)\n",
    "da['QICE'][0,80,:,:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62c07f-8440-4a37-8538-8fd1c0e457cc",
   "metadata": {},
   "source": [
    "## Make panel plots to check WRF output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd67bd1-ad9c-4cd0-afa2-6d73d711398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import read_vars_WRF_RAMS\n",
    "\n",
    "domain = '1'\n",
    "# Extract QCLOUD variable\n",
    "variables = ['QC']\n",
    "levels=[800,500,250]\n",
    "simulation_name = 'ARG1.2-WM-V0'\n",
    "filepath = '/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation_name+'/G1/wrfout_d01_2018-12-13_20:30:00'\n",
    "da = Dataset(filepath)\n",
    "for variable in variables:\n",
    "    # Create a 2x2 subplot grid\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10,10))\n",
    "\n",
    "    # Plot QCLOUD variable in each subplot\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(filepath,variable,'WRF',output_height=False,interpolate=True,level=levels[i],interptype='pressure')\n",
    "        #print(z)\n",
    "        #print(np.nanmax(z))\n",
    "        y_dim,x_dim = np.shape(z)\n",
    "        if domain=='1':\n",
    "            dx=1.6\n",
    "        if domain=='2':\n",
    "            dx=0.4\n",
    "        if domain=='3':\n",
    "            dx=0.1\n",
    "        #xx = np.arange(0,dx*x_dim,dx)\n",
    "        #yy = np.arange(0,dx*y_dim,dx)\n",
    "        #print('shape of xx is ',np.shape(xx))\n",
    "        #print('shape of yy is ',np.shape(yy))\n",
    "        #print('shape of z is ',np.shape(z))\n",
    "        timestep_str     = pd.to_datetime(z_time,format='%Y%m%d%H%M%S').strftime('%Y-%m-%d %H:%M:%S')\n",
    "        #vert = getvar(da, \"pres\").values\n",
    "        #print(vert)\n",
    "        #output_var = interpz3d(output_var, vert, level)\n",
    "        #output_var = getvar(da,'QCLOUD')\n",
    "        #print(output_var)\n",
    "        #z = interplevel(output_var, vert, level, meta=False)\n",
    "        ax.set_title(simulation_name+'\\n'+variable+' at pres level '+str(levels[i])+' hPa\\n'+timestep_str)\n",
    "        cont = ax.contourf(z,levels=20,cmap=plt.get_cmap('viridis'),extend='both')#,ax=ax) \n",
    "        ax.set_aspect('equal', 'box')\n",
    "\n",
    "        # Add a common colorbar\n",
    "        #plt.colorbar(cont,label='QC')\n",
    "    #plt.suptitle(variable+' for '+simulation_name+'\\n'+timestep_pd.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af2584-f8e0-42b0-9841-9ee4a5ebb16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as colors\n",
    "import datetime #\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "#from RAMS_Post_Process import fx_postproc_RAMS as RAMS_fx\n",
    "import matplotlib.pyplot as plt\n",
    "import read_vars_WRF_RAMS\n",
    "\n",
    "domain = 'd01'\n",
    "# Extract QCLOUD variable\n",
    "variables = ['TT','RH']\n",
    "levels=[0,10,20]\n",
    "\n",
    "simulation_name = 'ARG1.1-WM-V1'\n",
    "directory_met_em=\"/nobackupp11/isingh2/WPS/run_\"+simulation_name[0:6]+\"_PROD/\"\n",
    "print('searching directory\" ',directory_met_em)\n",
    "fi_list_met_em  = []\n",
    "\n",
    "file_finding_string_met_em = \"met_em.\"+domain+\".*.nc\"\n",
    "fi_list_met_em=sorted(glob.glob(directory_met_em+file_finding_string_met_em))\n",
    "print('number of met_em files found: ',len(fi_list_met_em))\n",
    "#print('WRF files: ',fi_list_met_em)\n",
    "#filepath = fi_list_met_em[int(len(fi_list_met_em)/2)]#'/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation_name+'/G1/wrfout_d01_2018-12-13_20:30:00'\n",
    "#print('choosing the middle file: ',filepath)\n",
    "for fil in fi_list_met_em[::10]:\n",
    "    da = xr.open_dataset(fil)\n",
    "    for variable in variables:\n",
    "        # Create a 2x2 subplot grid\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(10,10))\n",
    "        z = da.variables[variable]\n",
    "        # Plot QCLOUD variable in each subplot\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            y_dim,x_dim = np.shape(z)\n",
    "            if domain=='1':\n",
    "                dx=1.6\n",
    "            if domain=='2':\n",
    "                dx=0.4\n",
    "            if domain=='3':\n",
    "                dx=0.1\n",
    "            timestep_str = da.Times.values[0].decode()\n",
    "            ax.set_title(simulation_name+'\\n'+variable+' at level '+str(levels[i])+'\\n'+timestep_str)\n",
    "            cont = ax.contourf(z,levels=20,cmap=plt.get_cmap('viridis'),extend='both')#,ax=ax) \n",
    "            ax.set_aspect('equal', 'box')\n",
    "            # Add a common colorbar\n",
    "            #plt.colorbar(cont,label='QC')\n",
    "    print('---------\\n')\n",
    "    #plt.suptitle(variable+' for '+simulation_name+'\\n'+timestep_pd.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    png_filename = '/nobackupp11/isingh2/LES_simulations_plots/quick_look_ITC/'+'met_em_'+simulation_name+'_'+domain+'_'+variable+'_'+pd.to_datetime(timestep_str,format='%Y-%m-%d_%H:%M:%S').strftime('%Y%m%d%H%M%S')+'.png'\n",
    "    print('saving image to : ',png_filename)\n",
    "    plt.savefig(png_filename,dpi=150)                                                                                                                                                                                        104,18        Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5060bbc-f1d1-48d4-8eaa-33e03b20c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Load the image\n",
    "#image_path = \"result_image.png\"  # Replace with the path to your image file\n",
    "img = mpimg.imread(output_file)\n",
    "print(\"Shape of the resulting image:\", np.shape(img))\n",
    "#print\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Optional: Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f205949-b655-4c42-8062-4c22c99142d6",
   "metadata": {},
   "source": [
    "## Make plan views of variables RAMS and WRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6659ea-e86f-48ed-8bad-61e3b2355197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile make_plan_views_RAMS.py\n",
    "# make plan views of variables:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as crs\n",
    "import random\n",
    "import skgstat as skg\n",
    "plt.style.use('ggplot')\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import read_vars_WRF_RAMS\n",
    "\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "   \n",
    "def read_head(headfile,h5file):\n",
    "        # Function that reads header files from RAMS\n",
    "\n",
    "        # Inputs:\n",
    "        #   headfile: header file including full path in str format\n",
    "        #   h5file: h5 datafile including full path in str format\n",
    "\n",
    "        # Returns:\n",
    "        #   zmn: height levels for momentum values (i.e., grid box upper and lower levels)\n",
    "        #   ztn: height levels for thermodynaic values (i.e., grid box centers)\n",
    "        #   nx:: the number of x points for the domain associated with the h5file\n",
    "        #   ny: the number of y points for the domain associated with the h5file\n",
    "        #   npa: the number of surface patches\n",
    "\n",
    "\n",
    "        dom_num = h5file[h5file.index('.h5')-1] # Find index of .h5 to determine position showing which nest domain to use\n",
    "\n",
    "        with open(headfile) as f:\n",
    "            contents = f.readlines()\n",
    "\n",
    "        idx_zmn = contents.index('__zmn0'+dom_num+'\\n')\n",
    "        nz_m = int(contents[idx_zmn+1])\n",
    "        zmn = np.zeros(nz_m)\n",
    "        for i in np.arange(0,nz_m):\n",
    "            zmn[i] =  float(contents[idx_zmn+2+i])\n",
    "\n",
    "        idx_ztn = contents.index('__ztn0'+dom_num+'\\n')\n",
    "        nz_t = int(contents[idx_ztn+1])\n",
    "        ztn = np.zeros(nz_t)\n",
    "        for i in np.arange(0,nz_t):\n",
    "            ztn[i] =  float(contents[idx_ztn+2+i])\n",
    "\n",
    "        ztop = np.max(ztn) # Model domain top (m)\n",
    "\n",
    "        # Grad the size of the horizontal grid spacing\n",
    "        idx_dxy = contents.index('__deltaxn\\n')\n",
    "        dxy = float(contents[idx_dxy+1+int(dom_num)].strip())\n",
    "\n",
    "        idx_npatch = contents.index('__npatch\\n')\n",
    "        npa = int(contents[idx_npatch+2])\n",
    "\n",
    "        idx_ny = contents.index('__nnyp\\n')\n",
    "        idx_nx = contents.index('__nnxp\\n')\n",
    "        ny = np.ones(int(contents[idx_ny+1]))\n",
    "        nx = np.ones(int(contents[idx_ny+1]))\n",
    "        for i in np.arange(0,len(ny)):\n",
    "            nx[i] = int(contents[idx_nx+2+i])\n",
    "            ny[i] = int(contents[idx_ny+2+i])\n",
    "\n",
    "        ny_out = ny[int(dom_num)-1]\n",
    "        nx_out = nx[int(dom_num)-1]\n",
    "\n",
    "        return zmn, ztn, nx_out, ny_out, dxy, npa \n",
    "\n",
    "simulations=['USA1.1-R']#PHI1.1-R','PHI2.1-R','WPO1.1-R','BRA1.1-R','DRC1.1-R','AUS1.1-R']\n",
    "domain='3'\n",
    "variables = [['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$'],['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$']]\n",
    "# variables = [['Tk', 0, 'model', '$T_{sfc}^{2} (K^{2})$']             , ['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$'], ['RH', 0, 'model', '$RH_{sfc}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 0, 'model', '$U_{sfc}^{2} (m^{2}s^{-2})$']        , ['V', 0, 'model', '$V_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 0, 'model', '$WSPD_{sfc}^{2} (m^{2}s^{-2})$']  , ['W', 0, 'model', '$W_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['MCAPE', -999, None, '$MCAPE^{2} (J^{2}kg^{-2})$']     , ['MCIN', -999, None, '$MCIN^{2} (J^{2}kg^{-2})$'], \\\n",
    "#              ['Tk', 750, 'pressure', '$T_{750}^{2} (K^{2})$']        , ['THETA', 750, 'pressure', '$Theta_{750}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 750, 'pressure', '$Qvapor_{750}^{2} (kg^{2}kg^{-2})$'], ['RH', 750, 'pressure', '$RH_{750}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 750, 'pressure', '$U_{750}^{2} (m^{2}s^{-2})$']   , ['V', 750, 'pressure', '$V_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 750, 'pressure', '$WSPD_{750}^{2} (m^{2}s^{-2})$'], ['W', 750, 'pressure', '$W_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['Tk', 500, 'pressure', '$T_{500}^{2} (K^{2})$']        , ['THETA', 500, 'pressure', '$Theta_{500}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 500, 'pressure', '$Qvapor_{500}^{2} (kg^{2}kg^{-2})$'], ['RH', 500, 'pressure', '$RH_{500}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 500, 'pressure', '$U_{500}^{2} (m^{2}s^{-2})$']   , ['V', 500, 'pressure', '$V_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 500, 'pressure', '$WSPD_{500}^{2} (m^{2}s^{-2})$'], ['W', 500, 'pressure', '$W_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['Tk', 200, 'pressure', '$T_{200}^{2} (K^{2})$']        , ['THETA', 200, 'pressure', '$Theta_{200}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 200, 'pressure', '$Qvapor_{200}^{2} (kg^{2}kg^{-2})$'], ['RH', 200, 'pressure', '$RH_{200}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 200, 'pressure', '$U_{200}^{2} (m^{2}s^{-2})$']   , ['V', 200, 'pressure', '$V_{200}^{2} (m^{2}s^{-2})$'], \\\n",
    "#              ['WSPD', 200, 'pressure', '$WSPD_{200}^{2} (m^{2}s^{-2})$'], ['W', 200, 'pressure', '$W_{200}^{2} (m^{2}s^{-2})$']]\n",
    "\n",
    "units_dict = {'Tk':'$K$','QV':'$kg kg^{-1}$','RH':'percent','WSPD':'$m s^{-1}$','U':'$m s^{-1}$',\\\n",
    "              'V':'$m s^{-1}$','W':'$m s^{-1}$','MCAPE':'$J kg^{-1}$','MCIN':'$J kg^{-1}$','THETA':'$K$'}\n",
    "vmin_vmax_dict = {'Tk':[290,331,1],'QV':[0.006,0.0024,0.001],'RH':[70,101,1],'WSPD':[1,20,1],'U':[1,20,1],\\\n",
    "              'V':[1,20,1],'W':[-5,21,1],'MCAPE':[100,3100,100],'MCIN':[0,310,10],'THETA':[290,331,1]}\n",
    "\n",
    "colors    =  ['#000000','#E69F00','#56B4E9','#009E73','#F0E442','#0072B2','#D55E00','#CC79A7']\n",
    "\n",
    "def make_plan_view(WHICH_TIME, VARIABLE, SIMULATIONS, DOMAIN):\n",
    "\n",
    "    print('working on ',VARIABLE,'\\n')\n",
    "    \n",
    "    for ii,simulation in enumerate(SIMULATIONS): \n",
    "        fig    = plt.figure(figsize=(8,8))\n",
    "        print('    working on simulation: ',simulation)\n",
    "        if DOMAIN=='1' or DOMAIN =='2':\n",
    "            rams_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+DOMAIN+'/out/'+'a-A-*g'+DOMAIN+'.h5'))# CSU machine\n",
    "        if DOMAIN=='3':\n",
    "            rams_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+DOMAIN+'/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "        print('        total # files = ',len(rams_files))\n",
    "        print('        first file is ',rams_files[0])\n",
    "        print('        last file is ',rams_files[-1])\n",
    "        if WHICH_TIME=='start':\n",
    "            rams_fil    = rams_files[0]\n",
    "        if WHICH_TIME=='middle':\n",
    "            rams_fil    = rams_files[int(len(rams_files)/2)]\n",
    "        if WHICH_TIME=='end':\n",
    "            rams_fil    = rams_files[-1]\n",
    "        print('        choosing the '+WHICH_TIME+' file: ',rams_fil)\n",
    "      \n",
    "        z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(rams_fil,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "        y_dim,x_dim = np.shape(z)\n",
    "        if DOMAIN=='1':\n",
    "            dx=1.6\n",
    "        if DOMAIN=='2':\n",
    "            dx=0.4\n",
    "        if DOMAIN=='3':\n",
    "            dx=0.1\n",
    "        xx = np.arange(0,dx*x_dim,dx)\n",
    "        yy = np.arange(0,dx*y_dim,dx)\n",
    "        print('shape of xx is ',np.shape(xx))\n",
    "        print('shape of yy is ',np.shape(yy))\n",
    "        print('shape of z is ',np.shape(z))\n",
    "        timestep_pd     = pd.to_datetime(z_time,format='%Y%m%d%H%M%S')\n",
    "        plt.contourf(xx,yy,z,levels=20,cmap=plt.get_cmap('viridis'),extend='both')\n",
    "\n",
    "        if VARIABLE[2]:\n",
    "            title_string = simulation+' '+VARIABLE[0]+' ('+units_dict[VARIABLE[0]]+')'+' at '+VARIABLE[2]+' level '+str(int(VARIABLE[1]))+' for d0'+DOMAIN+'\\n'+timestep_pd.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            title_string = simulation+' '+VARIABLE[0]+' ('+units_dict[VARIABLE[0]]+')'+' for d0'+DOMAIN+'\\n'+timestep_pd.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        plt.title(title_string)\n",
    "        plt.xlabel('x (km)')\n",
    "        plt.ylabel('y (km)')\n",
    "        plt.colorbar()\n",
    "        if VARIABLE[2]:\n",
    "            filename = 'plan_view_RAMS_'+simulation+'_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_d0'+DOMAIN+'_time_'+z_time+'.png'\n",
    "        else:\n",
    "            filename = 'plan_view_RAMS_'+simulation+'_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_d0'+DOMAIN+'_time_'+z_time+'.png'\n",
    "        print('saving to png file: ',filename)\n",
    "        plt.savefig(filename,dpi=150)\n",
    "        plt.close()\n",
    "        print('\\n\\n')\n",
    "for var in variables:\n",
    "    make_plan_view('middle', var, simulations, '3')\n",
    "\n",
    "# print('working on domain' ,domain)\n",
    "# #Running on the terminal in parallel\n",
    "# argument = []\n",
    "# for var in variables:\n",
    "#     argument = argument + [('middle',var, simulations, domain)]\n",
    "\n",
    "# print('length of argument is: ',len(argument))\n",
    "\n",
    "\n",
    "# # # ############################### FIRST OF ALL ################################\n",
    "# cpu_count1 = 37 #cpu_count()\n",
    "# print('number of cpus: ',cpu_count1)\n",
    "# # # #############################################################################\n",
    "\n",
    "# def main(FUNCTION, ARGUMENT):\n",
    "#     start_time = time.perf_counter()\n",
    "#     with Pool(processes = (cpu_count1-1)) as pool:\n",
    "#         data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "#     finish_time = time.perf_counter()\n",
    "#     print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "#     #df_all = pd.concat(data, ignore_index=True)\n",
    "#     #thermo_indices_data_csv_file = csv_folder+'thermodynamic_indices_' + DOMAIN + '_comb_track_filt_01_02_50_02_sr5017_setpos.csv'\n",
    "#     #print('saving thermodynamic indices to the file: ',thermo_indices_data_csv_file)\n",
    "#     #df_all.to_csv(thermo_indices_data_csv_file)  # sounding data\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main(make_plan_view, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c748fb",
   "metadata": {},
   "source": [
    "## Xarray-based functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ee1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XARRA\n",
    "\n",
    "Cp=1004.\n",
    "Rd=287.0\n",
    "\n",
    "def plot_zonal_vertcross(FILE, X_1D, Z_3D, Z_1D, DXY, VAR1, DESTAGGER, STAGGER_DIM, LEVELS_VAR1, CMAP_VAR1,\n",
    "                       VAR2, LEVELS_VAR2, VAR2_COLOR,\n",
    "                       VAR3, LEVELS_VAR3, VAR3_COLOR,\n",
    "                       VAR4, LEVELS_VAR4, VAR4_COLOR,\n",
    "                       yy, x1, x2,\n",
    "                       PLOT_WINDS, PANEL_PLOT, HEIGHT, AX, EXP_LABEL, PANEL_LABEL, PLOT_PT_X, PLOT_PT_Z):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "\n",
    "    DATA=xr.open_dataset(FILE,engine='h5netcdf', phony_dims='sort')\n",
    "      \n",
    "    XV1, ZV1 = np.meshgrid(X_1D[x1:x2], Z_1D)\n",
    "    zh=Z_3D\n",
    "\n",
    "    var1 = DATA[VAR1]\n",
    "    terr = DATA.TOPT.values\n",
    "\n",
    "    if DESTAGGER:\n",
    "        print(var1)\n",
    "        var1 = destagger(var1, STAGGER_DIM, meta=True)\n",
    "    else:\n",
    "        print(' ')\n",
    "\n",
    "    \n",
    "    if (isinstance(LEVELS_VAR1, np.ndarray)):\n",
    "        C1 = AX.contourf(XV1, zh[:, yy, x1:x2]/1000.0, var1[:, yy, x1:x2], levels=LEVELS_VAR1,\n",
    "                           cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "    else:\n",
    "        C1 = AX.contourf(XV1, zh[:, yy, x1:x2]/1000.0, var1[:, yy, x1:x2],\n",
    "                           cmap=CMAP_VAR1, extend='both')\n",
    "\n",
    "        \n",
    "    #levels_th = np.arange(290.0, 690.0, 2.0)\n",
    "    #C4 = plt.contour(XV1, zh[:, yy, x1:x2]/1000.0, DATA.THETA.values[:, yy, x1:x2], colors='k', levels=levels_th, axis=AX, linewidth=0.6)\n",
    "    #plt.clabel(C4, inline=1, fontsize=14, fmt='%3.0f')\n",
    "\n",
    "        \n",
    "    if VAR2:\n",
    "        var2 = DATA.variables[VAR2]\n",
    "        if (isinstance(LEVELS_VAR2, np.ndarray)):\n",
    "            C2 = AX.contour(XV1, zh[:, yy, x1:x2]/1000.0, var2[:, yy, x1:x2],\n",
    "                             levels=LEVELS_VAR2, colors=VAR2_COLOR, linewidths=1., linestyles=\"-\")\n",
    "        else:\n",
    "            C2 = AX.contour(XV1, zh[:, yy, x1:x2]/1000.0, var2[:, yy, x1:x2],\n",
    "                             colors='k', linewidths=1., linestyles=\"--\")\n",
    "        AX.clabel(C2, inline=1, fontsize=10, fmt='%3.0f')\n",
    "\n",
    "    if VAR3:\n",
    "        if VAR3==\"RTP-RV\":\n",
    "            \n",
    "            var3 = np.absolute(DATA[\"RTP\"]*1000.-DATA[\"RV\"]*1000.)\n",
    "            #exner = DATA[\"PI\"]/1004.0\n",
    "            #temp  = DATA[\"THETA\"]*exner\n",
    "            #pressure = 100000.0*(exner)**(Cp/Rd)\n",
    "            #density = pressure/(temp*Rd)\n",
    "            #print('density: ',density)\n",
    "            #var3=condensate_mr*density\n",
    "            #print('g/m3: ',var3)\n",
    "        else:\n",
    "            var3 = DATA.variables[VAR3]\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(XV1, zh[:, yy, x1:x2]/1000.0, var3[:, yy, x1:x2],\n",
    "                             levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")  # , linewidths=0.85,linestyles=\"--\")#hatches=[None,None,'.','/']\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            # C3 = plt.contour(XV1,zh[0,:,yy,x1:x2]/1000.0, var3[0,:,yy,x1:x2]*1000.,\\\n",
    "            #          axis=AX,colors='green',linewidths=2.0,linestyles=\"--\")\n",
    "\n",
    "        #plt.clabel(C2, inline=1, fontsize=15, fmt='%3.0f')\n",
    "\n",
    "    if VAR4:\n",
    "        var4 = DATA.variables[VAR4]\n",
    "        if (isinstance(LEVELS_VAR4, np.ndarray)):\n",
    "            C4 = pAXlt.contour(XV1, zh[:, yy, x1:x2]/1000.0, var4[:, yy, x1:x2]*1000.,\n",
    "                             levels=LEVELS_VAR4, colors=VAR4_COLOR, linewidths=2.0, linestyles=\":\")  # , linewidths=0.85,linestyles=\"--\")#hatches=[None,None,'.','/']\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    if PLOT_WINDS:\n",
    "        winds_thin_x = 4\n",
    "        winds_thin_z = 4\n",
    "        XVwind, ZVwind = np.meshgrid(xh[x1:x2:winds_thin_x], z[::winds_thin_z])\n",
    "        u1 = DATA.variables[\"UP\"][::winds_thin_z,\n",
    "                                       yy, x1:x2:winds_thin_x]*1.94384\n",
    "        w1 = DATA.variables[\"WP\"][::winds_thin_z,\n",
    "                                       yy, x1:x2:winds_thin_x]*1.94384\n",
    "        QV1 = AX.barbs(XVwind, zh[::winds_thin_z, yy, x1:x2:winds_thin_x]/1000.0,\n",
    "                       u1, w1, length=7.2, pivot='middle', linewidth=0.60, flip_barb=True)\n",
    "          \n",
    "    C4zs = AX.plot(xh[x1:x2], terr[yy, x1:x2]/1000., color='sienna', linewidth=3.6)\n",
    "    # if DATA.variables['zs']:\n",
    "    #    C5 = plot(xh[x1:x2],zs[0,yy,x1:x2]/1000.0, color = \"black\") # Plot topography\n",
    "\n",
    "    title = 'Vertical cross-section (zonal) of $w$'\n",
    "    prodid = 'tests'+'_'+VAR1+'_th_vcross_zonal_'+'y' + \\\n",
    "        str(yy)+'_x1_'+str(x1)+'_x2_'+str(x2)+'.png'\n",
    "    units = 'm/s'#var1.attrs['units']  # '$ x 10^{-5} $'+\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    # the x coords of this transformation are axes, and the\n",
    "    # y coord are data\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "    #        verticalalignment='top', bbox=props)\n",
    "    AX.text(0.70, 0.94, EXP_LABEL, fontsize=12,\n",
    "            verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "    \n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "        #        verticalalignment='top', bbox=props)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "        \n",
    "    # text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n",
    "    #  verticalalignment='center', transform=ax.transAxes\n",
    "    \n",
    "    AX.scatter(PLOT_PT_X*DXY,PLOT_PT_Z/1000.,marker='+',color='k',s=130.5)\n",
    "    \n",
    "    #AX.scatter(xh[x1:x2], zh[:, yy, x1:x2]/1000.0[var1[:, yy, x1:x2].idmax(dim='phony_dim_3')],marker='o',color='k',s=10.5)\n",
    "    \n",
    "    # plot \n",
    "\n",
    "    fig_process_vert(AX, terr, C1, x1, x2, yy, 0, title, get_time_from_RAMS_file(FILE)[0], get_time_from_RAMS_file(FILE)[1], prodid, units, \"zonal\", HEIGHT, True, PANEL_PLOT)\n",
    "    \n",
    "\n",
    "    # fig_process_vert(CONTOUR,Y_CROSS,CBAR_EXP,TITLESTRING,TIMESTRING,FILENAMESTRING,PROD,UNITS,VERT_CROSS,HEIGHT,IS_PANEL_PLOT):\n",
    "\n",
    "    ###\n",
    "\n",
    "def plot_meridional_vertcross(FILE, Y_1D, Z_3D, Z_1D, DXY, VAR1, DESTAGGER, STAGGER_DIM, LEVELS_VAR1, CMAP_VAR1,\n",
    "                       VAR2, LEVELS_VAR2, VAR2_COLOR,\n",
    "                       VAR3, LEVELS_VAR3, VAR3_COLOR,\n",
    "                       VAR4, LEVELS_VAR4, VAR4_COLOR,\n",
    "                       xx, y1, y2,\n",
    "                       PLOT_WINDS, PANEL_PLOT, HEIGHT, AX, EXP_LABEL, PANEL_LABEL, PLOT_PT_Y, PLOT_PT_Z):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "\n",
    "    DATA=xr.open_dataset(FILE,engine='h5netcdf', phony_dims='sort')\n",
    "    \n",
    "    \n",
    "    YV1, ZV1 = np.meshgrid(Y_1D[y1:y2], Z_1D)\n",
    "    zh=Z_3D\n",
    "\n",
    "    var1 = DATA[VAR1]\n",
    "    terr = DATA.TOPT.values\n",
    "\n",
    "    if DESTAGGER:\n",
    "        print(var1)\n",
    "        var1 = destagger(var1, STAGGER_DIM, meta=True)\n",
    "    else:\n",
    "        print(' ')\n",
    "\n",
    "    \n",
    "    if (isinstance(LEVELS_VAR1, np.ndarray)):\n",
    "        C1 = AX.contourf(YV1, zh[:, y1:y2, xx]/1000.0, var1[:, y1:y2, xx], levels=LEVELS_VAR1,\n",
    "                          cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "    else:\n",
    "        C1 = AX.contourf(YV1, zh[:, y1:y2, xx]/1000.0, var1[:, y1:y2, xx],\n",
    "                          cmap=CMAP_VAR1, extend='both')\n",
    "\n",
    "        \n",
    "    #levels_th = np.arange(290.0, 690.0, 2.0)\n",
    "    #C4 = plt.contour(XV1, zh[:, yy, x1:x2]/1000.0, DATA.THETA.values[:, yy, x1:x2], colors='k', levels=levels_th, axis=AX, linewidth=0.6)\n",
    "    #plt.clabel(C4, inline=1, fontsize=14, fmt='%3.0f')\n",
    "\n",
    "        \n",
    "    if VAR2:\n",
    "        var2 = DATA.variables[VAR2]\n",
    "        if (isinstance(LEVELS_VAR2, np.ndarray)):\n",
    "            C2 = AX.contour(YV1, zh[:, y1:y2, xx]/1000.0, var2[:, y1:y2, xx],\n",
    "                             levels=LEVELS_VAR2, colors=VAR2_COLOR, linewidths=1., linestyles=\"-\")\n",
    "        else:\n",
    "            C2 = AX.contour(YV1, zh[:, y1:y2, xx]/1000.0, var2[:, y1:y2, xx],\n",
    "                             colors='k', linewidths=1., linestyles=\"--\")\n",
    "        AX.clabel(C2, inline=1, fontsize=10, fmt='%3.0f')\n",
    "\n",
    "    if VAR3:\n",
    "        if VAR3==\"RTP-RV\":\n",
    "            var3 = np.absolute(DATA[\"RTP\"]*1000. - DATA[\"RV\"]*1000.)\n",
    "            #exner = DATA[\"PI\"]/1004.0\n",
    "            #temp = DATA[\"THETA\"]*exner\n",
    "            #pressure = 100000.0*(exner)**(Cp/Rd)\n",
    "            #density = pressure/(temp*Rd)\n",
    "            #print('pressure: ',pressure)\n",
    "            #print('temp: ',temp)\n",
    "            #print('density: ',density)\n",
    "            #var3=condensate_mr*density\n",
    "            #print('g/m3: ',var3)\n",
    "        else:\n",
    "            var3 = DATA.variables[VAR3]\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(YV1, zh[:, y1:y2, xx]/1000.0, var3[:, y1:y2, xx],\n",
    "                            levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")  # , linewidths=0.85,linestyles=\"--\")#hatches=[None,None,'.','/']\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            # C2 = plt.contour(XV1,zh[0,:,yy,x1:x2]/1000.0, var3[0,:,yy,x1:x2]*1000.,\\\n",
    "            #          axis=AX,colors='green',linewidths=2.0,linestyles=\"--\")\n",
    "\n",
    "        #plt.clabel(C2, inline=1, fontsize=15, fmt='%3.0f')\n",
    "\n",
    "    if VAR4:\n",
    "        var4 = DATA.variables[VAR4]\n",
    "        if (isinstance(LEVELS_VAR4, np.ndarray)):\n",
    "            C4 = AX.contour(YV1, zh[:, y1:y2, xx]/1000.0, var4[:, y1:y2, xx]*1000.,\n",
    "                            levels=LEVELS_VAR4, colors=VAR4_COLOR, linewidths=2.0, linestyles=\":\")  # , linewidths=0.85,linestyles=\"--\")#hatches=[None,None,'.','/']\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    if PLOT_WINDS:\n",
    "        winds_thin_x = 4\n",
    "        winds_thin_z = 4\n",
    "        YVwind, ZVwind = np.meshgrid(yh[y1:y2:winds_thin_x], z[::winds_thin_z])\n",
    "        v1 = DATA.variables[\"VP\"][::winds_thin_z,y1:y2:winds_thin_x, xx]*1.94384\n",
    "        w1 = DATA.variables[\"WP\"][::winds_thin_z,y1:y2:winds_thin_x, xx]*1.94384\n",
    "        QV1 = AX.barbs(YVwind, zh[::winds_thin_z,y1:y2:winds_thin_x, xx]/1000.0,\n",
    "                       v1, w1, length=7.2, pivot='middle', linewidth=0.60, flip_barb=True)\n",
    "          \n",
    "\n",
    "    C4zs = AX.plot(yh[y1:y2], terr[y1:y2, xx]/1000., color='sienna', linewidth=3.6)\n",
    "    # if DATA.variables['zs']:\n",
    "    #    C5 = plot(xh[x1:x2],zs[0,yy,x1:x2]/1000.0, color = \"black\") # Plot topography\n",
    "\n",
    "    title = 'Vertical cross-section (meridional) of $w$'\n",
    "    prodid = 'tests'+'_'+VAR1+'_th_vcross_zonal_'+'xx' + \\\n",
    "        str(xx)+'_y1_'+str(y1)+'_y2_'+str(y2)+'.png'\n",
    "    units = 'm/s'#var1.attrs['units']  # '$ x 10^{-5} $'+\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    # the x coords of this transformation are axes, and the\n",
    "    # y coord are data\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "    #        verticalalignment='top', bbox=props)\n",
    "    AX.text(0.70, 0.94, EXP_LABEL, fontsize=12,\n",
    "            verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "    \n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "        #        verticalalignment='top', bbox=props)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "        \n",
    "    # text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n",
    "    #  verticalalignment='center', transform=ax.transAxes\n",
    "    \n",
    "    AX.scatter(PLOT_PT_Y*DXY,PLOT_PT_Z/1000.,marker='+',color='k',s=130.5)\n",
    "    \n",
    "    #AX.scatter(xh[x1:x2], zh[:, yy, x1:x2]/1000.0[var1[:, yy, x1:x2].idmax(dim='phony_dim_3')],marker='o',color='k',s=10.5)\n",
    "    \n",
    "    # plot \n",
    "\n",
    "    fig_process_vert(AX, terr, C1, y1, y2, xx, 0, title, get_time_from_RAMS_file(FILE)[0], get_time_from_RAMS_file(FILE)[1], prodid, units, \"meridional\", HEIGHT, False, PANEL_PLOT)\n",
    "    \n",
    "\n",
    "    # fig_process_vert(CONTOUR,Y_CROSS,CBAR_EXP,TITLESTRING,TIMESTRING,FILENAMESTRING,PROD,UNITS,VERT_CROSS,HEIGHT,IS_PANEL_PLOT):\n",
    "\n",
    "    ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097c17a",
   "metadata": {},
   "source": [
    "## h5py-based functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac748247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XARRA\n",
    "\n",
    "Cp=1004.\n",
    "Rd=287.0\n",
    "\n",
    "def plot_zonal_vertcross_h5py(FILE, X_1D, Z_3D, Z_1D, DXY, VAR1, DESTAGGER, STAGGER_DIM, LEVELS_VAR1, CMAP_VAR1,\n",
    "                       VAR2, LEVELS_VAR2, VAR2_COLOR,\n",
    "                       VAR3, LEVELS_VAR3, VAR3_COLOR,\n",
    "                       VAR4, LEVELS_VAR4, VAR4_COLOR,\n",
    "                       yy, x1, x2,\n",
    "                       PLOT_WINDS, PANEL_PLOT, HEIGHT, AX, EXP_LABEL, PANEL_LABEL, PLOT_PT_X, PLOT_PT_Z):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "\n",
    "    DATA = h5py.File(FILE, 'r')\n",
    "    \n",
    "    XV1, ZV1 = np.meshgrid(X_1D[x1:x2], Z_1D)\n",
    "    zh=Z_3D\n",
    "    \n",
    "\n",
    "    var1 = np.array(DATA[VAR1][:])\n",
    "    #print('var1: ',var1)\n",
    "    terr = np.array(DATA['TOPT'][:])#.values\n",
    "    #print('terrain: terr')\n",
    "\n",
    "    if DESTAGGER:\n",
    "        print(var1)\n",
    "        var1 = destagger(var1, STAGGER_DIM, meta=True)\n",
    "    else:\n",
    "        print(' ')\n",
    "\n",
    "    \n",
    "    if (isinstance(LEVELS_VAR1, np.ndarray)):\n",
    "        C1 = AX.contourf(XV1, zh[:, yy, x1:x2]/1000.0, var1[:, yy, x1:x2], levels=LEVELS_VAR1,\n",
    "                          axis=AX, cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "    else:\n",
    "        C1 = AX.contourf(XV1, zh[:, yy, x1:x2]/1000.0, var1[:, yy, x1:x2],\n",
    "                          axis=AX, cmap=CMAP_VAR1, extend='both')\n",
    "\n",
    "        \n",
    "    #levels_th = np.arange(290.0, 690.0, 2.0)\n",
    "    #C4 = plt.contour(XV1, zh[:, yy, x1:x2]/1000.0, DATA.THETA.values[:, yy, x1:x2], colors='k', levels=levels_th, axis=AX, linewidth=0.6)\n",
    "    #plt.clabel(C4, inline=1, fontsize=14, fmt='%3.0f')\n",
    "\n",
    "        \n",
    "    if VAR2:\n",
    "        var2 = np.array(DATA[VAR2][:])\n",
    "        if (isinstance(LEVELS_VAR2, np.ndarray)):\n",
    "            C2 = AX.contour(XV1, zh[:, yy, x1:x2]/1000.0, var2[:, yy, x1:x2],\n",
    "                             axis=AX, levels=LEVELS_VAR2, colors=VAR2_COLOR, linewidths=1., linestyles=\"-\")\n",
    "        else:\n",
    "            C2 = AX.contour(XV1, zh[:, yy, x1:x2]/1000.0, var2[:, yy, x1:x2],\n",
    "                             axis=AX, colors='k', linewidths=1., linestyles=\"--\")\n",
    "        AX.clabel(C2, inline=1, fontsize=10, fmt='%3.0f')\n",
    "\n",
    "    if VAR3:\n",
    "        if VAR3==\"RTP-RV\":\n",
    "            var3 = np.absolute(np.array(DATA[\"RTP\"][:])*1000.-np.array(DATA[\"RV\"][:])*1000.)\n",
    "            #exner = DATA[\"PI\"]/1004.0\n",
    "            #temp  = DATA[\"THETA\"]*exner\n",
    "            #pressure = 100000.0*(exner)**(Cp/Rd)\n",
    "            #density = pressure/(temp*Rd)\n",
    "            #print('density: ',density)\n",
    "            #var3=condensate_mr*density\n",
    "            #print('g/m3: ',var3)\n",
    "        else:\n",
    "            var3 = np.array(DATA[VAR3][:])\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(XV1, zh[:, yy, x1:x2]/1000.0, var3[:, yy, x1:x2],\n",
    "                             axis=AX, levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")  # , linewidths=0.85,linestyles=\"--\")#hatches=[None,None,'.','/']\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            # C3 = plt.contour(XV1,zh[0,:,yy,x1:x2]/1000.0, var3[0,:,yy,x1:x2]*1000.,\\\n",
    "            #          axis=AX,colors='green',linewidths=2.0,linestyles=\"--\")\n",
    "\n",
    "        #plt.clabel(C2, inline=1, fontsize=15, fmt='%3.0f')\n",
    "\n",
    "    if VAR4:\n",
    "        var4 = np.array(DATA[VAR4][:])\n",
    "        if (isinstance(LEVELS_VAR4, np.ndarray)):\n",
    "            C4 = pAXlt.contour(XV1, zh[:, yy, x1:x2]/1000.0, var4[:, yy, x1:x2]*1000.,\n",
    "                             axis=AX, levels=LEVELS_VAR4, colors=VAR4_COLOR, linewidths=2.0, linestyles=\":\")  # , linewidths=0.85,linestyles=\"--\")#hatches=[None,None,'.','/']\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    if PLOT_WINDS:\n",
    "        winds_thin_x = 4\n",
    "        winds_thin_z = 4\n",
    "        XVwind, ZVwind = np.meshgrid(xh[x1:x2:winds_thin_x], z[::winds_thin_z])\n",
    "        u1 = np.array(DATA[\"UP\"][:])[::winds_thin_z,\n",
    "                                       yy, x1:x2:winds_thin_x]*1.94384\n",
    "        w1 = np.array(DATA[\"WP\"][:])[::winds_thin_z,\n",
    "                                       yy, x1:x2:winds_thin_x]*1.94384\n",
    "        QV1 = AX.barbs(XVwind, zh[::winds_thin_z, yy, x1:x2:winds_thin_x]/1000.0,\n",
    "                       u1, w1, length=7.2, pivot='middle', linewidth=0.60, flip_barb=True)\n",
    "          \n",
    "    C4zs = AX.plot(xh[x1:x2], terr[yy, x1:x2]/1000., color='sienna', linewidth=3.6)\n",
    "    # if DATA.variables['zs']:\n",
    "    #    C5 = plot(xh[x1:x2],zs[0,yy,x1:x2]/1000.0, color = \"black\") # Plot topography\n",
    "    \n",
    "    DATA.close()\n",
    "\n",
    "    title = 'Vertical cross-section (zonal) of $w$'\n",
    "    prodid = 'tests'+'_'+VAR1+'_th_vcross_zonal_'+'y' + \\\n",
    "        str(yy)+'_x1_'+str(x1)+'_x2_'+str(x2)+'.png'\n",
    "    units = 'm/s'#var1.attrs['units']  # '$ x 10^{-5} $'+\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    # the x coords of this transformation are axes, and the\n",
    "    # y coord are data\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "    #        verticalalignment='top', bbox=props)\n",
    "    AX.text(0.70, 0.94, EXP_LABEL, fontsize=12,\n",
    "            verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "    \n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "        #        verticalalignment='top', bbox=props)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "        \n",
    "    # text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n",
    "    #  verticalalignment='center', transform=ax.transAxes\n",
    "    \n",
    "    AX.scatter(PLOT_PT_X*DXY,PLOT_PT_Z/1000.,marker='+',color='k',s=130.5)\n",
    "    \n",
    "    #AX.scatter(xh[x1:x2], zh[:, yy, x1:x2]/1000.0[var1[:, yy, x1:x2].idmax(dim='phony_dim_3')],marker='o',color='k',s=10.5)\n",
    "    \n",
    "    # plot \n",
    "\n",
    "    fig_process_vert(AX, terr, C1, x1, x2, yy, 0, title, get_time_from_RAMS_file(FILE)[0], get_time_from_RAMS_file(FILE)[1], prodid, units, \"zonal\", HEIGHT, True, PANEL_PLOT)\n",
    "    \n",
    "\n",
    "    # fig_process_vert(CONTOUR,Y_CROSS,CBAR_EXP,TITLESTRING,TIMESTRING,FILENAMESTRING,PROD,UNITS,VERT_CROSS,HEIGHT,IS_PANEL_PLOT):\n",
    "\n",
    "    ###\n",
    "    \n",
    "\n",
    "def plot_meridional_vertcross_h5py(FILE, Y_1D, Z_3D, Z_1D, DXY, VAR1, DESTAGGER, STAGGER_DIM, LEVELS_VAR1, CMAP_VAR1,\n",
    "                       VAR2, LEVELS_VAR2, VAR2_COLOR,\n",
    "                       VAR3, LEVELS_VAR3, VAR3_COLOR,\n",
    "                       VAR4, LEVELS_VAR4, VAR4_COLOR,\n",
    "                       xx, y1, y2,\n",
    "                       PLOT_WINDS, PANEL_PLOT, HEIGHT, AX, EXP_LABEL, PANEL_LABEL, PLOT_PT_Y, PLOT_PT_Z):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "\n",
    "    DATA = h5py.File(FILE, 'r')\n",
    "    \n",
    "    \n",
    "    YV1, ZV1 = np.meshgrid(Y_1D[y1:y2], Z_1D)\n",
    "    zh=Z_3D\n",
    "\n",
    "    var1 = np.array(DATA[VAR1][:])\n",
    "    terr = np.array(DATA['TOPT'][:])\n",
    "\n",
    "    if DESTAGGER:\n",
    "        print(var1)\n",
    "        var1 = destagger(var1, STAGGER_DIM, meta=True)\n",
    "    else:\n",
    "        print(' ')\n",
    "\n",
    "    \n",
    "    if (isinstance(LEVELS_VAR1, np.ndarray)):\n",
    "        C1 = AX.contourf(YV1, zh[:, y1:y2, xx]/1000.0, var1[:, y1:y2, xx], levels=LEVELS_VAR1,\n",
    "                          axis=AX, cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "    else:\n",
    "        C1 = AX.contourf(YV1, zh[:, y1:y2, xx]/1000.0, var1[:, y1:y2, xx],\n",
    "                          axis=AX, cmap=CMAP_VAR1, extend='both')\n",
    "\n",
    "        \n",
    "    #levels_th = np.arange(290.0, 690.0, 2.0)\n",
    "    #C4 = plt.contour(XV1, zh[:, yy, x1:x2]/1000.0, DATA.THETA.values[:, yy, x1:x2], colors='k', levels=levels_th, axis=AX, linewidth=0.6)\n",
    "    #plt.clabel(C4, inline=1, fontsize=14, fmt='%3.0f')\n",
    "\n",
    "        \n",
    "    if VAR2:\n",
    "        var2 = np.array(DATA[VAR2][:])\n",
    "        if (isinstance(LEVELS_VAR2, np.ndarray)):\n",
    "            C2 = AX.contour(YV1, zh[:, y1:y2, xx]/1000.0, var2[:, y1:y2, xx],\n",
    "                             axis=AX, levels=LEVELS_VAR2, colors=VAR2_COLOR, linewidths=1., linestyles=\"-\")\n",
    "        else:\n",
    "            C2 = AX.contour(YV1, zh[:, y1:y2, xx]/1000.0, var2[:, y1:y2, xx],\n",
    "                             axis=AX, colors='k', linewidths=1., linestyles=\"--\")\n",
    "        AX.clabel(C2, inline=1, fontsize=10, fmt='%3.0f')\n",
    "\n",
    "    if VAR3:\n",
    "        if VAR3==\"RTP-RV\":\n",
    "            var3 = np.absolute(np.array(DATA[\"RTP\"][:])*1000. - np.array(DATA[\"RV\"][:])*1000.)\n",
    "            #exner = DATA[\"PI\"]/1004.0\n",
    "            #temp = DATA[\"THETA\"]*exner\n",
    "            #pressure = 100000.0*(exner)**(Cp/Rd)\n",
    "            #density = pressure/(temp*Rd)\n",
    "            #print('pressure: ',pressure)\n",
    "            #print('temp: ',temp)\n",
    "            #print('density: ',density)\n",
    "            #var3=condensate_mr*density\n",
    "            #print('g/m3: ',var3)\n",
    "        else:\n",
    "            var3 = np.array(DATA[VAR3][:])\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(YV1, zh[:, y1:y2, xx]/1000.0, var3[:, y1:y2, xx],\n",
    "                             axis=AX, levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")  # , linewidths=0.85,linestyles=\"--\")#hatches=[None,None,'.','/']\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            # C2 = plt.contour(XV1,zh[0,:,yy,x1:x2]/1000.0, var3[0,:,yy,x1:x2]*1000.,\\\n",
    "            #          axis=AX,colors='green',linewidths=2.0,linestyles=\"--\")\n",
    "\n",
    "        #plt.clabel(C2, inline=1, fontsize=15, fmt='%3.0f')\n",
    "\n",
    "    if VAR4:\n",
    "        var4 = np.array(DATA[VAR4][:])\n",
    "        if (isinstance(LEVELS_VAR4, np.ndarray)):\n",
    "            C4 = AX.contour(YV1, zh[:, y1:y2, xx]/1000.0, var4[:, y1:y2, xx]*1000.,\n",
    "                             axis=AX, levels=LEVELS_VAR4, colors=VAR4_COLOR, linewidths=2.0, linestyles=\":\")  # , linewidths=0.85,linestyles=\"--\")#hatches=[None,None,'.','/']\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    if PLOT_WINDS:\n",
    "        winds_thin_x = 4\n",
    "        winds_thin_z = 4\n",
    "        YVwind, ZVwind = np.meshgrid(yh[y1:y2:winds_thin_x], z[::winds_thin_z])\n",
    "        v1 = np.array(DATA[\"VP\"][:])[::winds_thin_z,y1:y2:winds_thin_x, xx]*1.94384\n",
    "        w1 = np.array(DATA[\"WP\"][:])[::winds_thin_z,y1:y2:winds_thin_x, xx]*1.94384\n",
    "        QV1 = AX.barbs(YVwind, zh[::winds_thin_z,y1:y2:winds_thin_x, xx]/1000.0,\n",
    "                       v1, w1, length=7.2, pivot='middle', linewidth=0.60, flip_barb=True)\n",
    "          \n",
    "\n",
    "    C4zs = AX.plot(yh[y1:y2], terr[y1:y2, xx]/1000., color='sienna', linewidth=3.6)\n",
    "    # if DATA.variables['zs']:\n",
    "    #    C5 = plot(xh[x1:x2],zs[0,yy,x1:x2]/1000.0, color = \"black\") # Plot topography\n",
    "\n",
    "    DATA.close()\n",
    "    \n",
    "    title = 'Vertical cross-section (meridional) of $w$'\n",
    "    prodid = 'tests'+'_'+VAR1+'_th_vcross_zonal_'+'xx' + \\\n",
    "        str(xx)+'_y1_'+str(y1)+'_y2_'+str(y2)+'.png'\n",
    "    units = 'm/s'#var1.attrs['units']  # '$ x 10^{-5} $'+\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    # the x coords of this transformation are axes, and the\n",
    "    # y coord are data\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "    #        verticalalignment='top', bbox=props)\n",
    "    AX.text(0.70, 0.94, EXP_LABEL, fontsize=12,\n",
    "            verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "    \n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "        #        verticalalignment='top', bbox=props)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "        \n",
    "    # text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n",
    "    #  verticalalignment='center', transform=ax.transAxes\n",
    "    \n",
    "    AX.scatter(PLOT_PT_Y*DXY,PLOT_PT_Z/1000.,marker='+',color='k',s=130.5)\n",
    "    \n",
    "    #AX.scatter(xh[x1:x2], zh[:, yy, x1:x2]/1000.0[var1[:, yy, x1:x2].idmax(dim='phony_dim_3')],marker='o',color='k',s=10.5)\n",
    "    \n",
    "    # plot \n",
    "\n",
    "    fig_process_vert(AX, terr, C1, y1, y2, xx, 0, title, get_time_from_RAMS_file(FILE)[0], get_time_from_RAMS_file(FILE)[1], prodid, units, \"meridional\", HEIGHT, False, PANEL_PLOT)\n",
    "    \n",
    "\n",
    "    # fig_process_vert(CONTOUR,Y_CROSS,CBAR_EXP,TITLESTRING,TIMESTRING,FILENAMESTRING,PROD,UNITS,VERT_CROSS,HEIGHT,IS_PANEL_PLOT):\n",
    "\n",
    "    ###\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef87d6",
   "metadata": {},
   "source": [
    "# Executing code starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202d461-873a-439a-a824-a506db0691da",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be106435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to model data and where to save data\n",
    "path = '/monsoon/MODEL/LES_MODEL_DATA/DRC1.1-R/G1/out/'\n",
    "\n",
    "# Grab all the rams files \n",
    "h5filepath = path+'a-A*g1.h5'\n",
    "h5files1 = sorted(glob.glob(h5filepath))\n",
    "hefilepath = path+'a-A*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "print(len(h5files1))\n",
    "print(len(hefiles1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99fd4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read RAMS header file to get coordinate information\n",
    "zm, zt, nx, ny, dxy, npa = read_head(hefiles1[0],h5files1[0])\n",
    "# print('momentum heights: ',zm)\n",
    "# print('thermody heights: ',zt)\n",
    "# print('nx              : ',nx)\n",
    "# print('ny              : ',ny)\n",
    "# print('#sfc patches    : ',npa)\n",
    "#five_km_hgt_index=np.abs(zm-5000.).argmin()\n",
    "#print(five_km_hgt_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rams_file = h5py.File(h5files1[0], 'r')\n",
    "da = xr.open_dataset(h5files1[10],engine=\"h5netcdf\",phony_dims='sort')\n",
    "print(da)\n",
    "#print(da.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8f24e-63e7-49d3-a273-7a3a328435ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var in da.variables:\n",
    "#     print(var)\n",
    "print(da.SOIL_ENERGY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through files\n",
    "for i in np.arange(0,len(h5files1)):\n",
    "\n",
    "    # Read in RAMS file\n",
    "    rams_file = h5py.File(h5files1[i], 'r')\n",
    "    cur_time = os.path.split(h5files1[i])[1][4:21] # Grab time string from RAMS file\n",
    "    print(cur_time)\n",
    "    pd_time = cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17]\n",
    "    print(pd_time)\n",
    "\n",
    "    # Read lat, lon, topography from RAMS files\n",
    "    lats = np.array(rams_file['GLAT'])\n",
    "    lons = np.array(rams_file['GLON'])\n",
    "    topt = np.array(rams_file['TOPT'][:])\n",
    "\n",
    "    # Calculcate z_coord using rams_tool function (reading it from header files)\n",
    "    zcoords = rams_tools.calc_zcoords(hefiles1[i])\n",
    "    print(zcoords)\n",
    "    dzcoords = np.diff(zcoords)\n",
    "    \n",
    "    # Read in column maximum vertical velocity from RAMS files\n",
    "    pltvar = np.array(np.nanmax(rams_file['WP'][:,:,:],axis=0))\n",
    "\n",
    "    # Close RAMS file\n",
    "    rams_file.close()\n",
    "\n",
    "\n",
    "    # Grab current positions of tracked objects from current time.\n",
    "    tdata_cur = tdata[tdata['timestr'] == pd_time]\n",
    "    tlon = tdata_cur['lon'].values\n",
    "    tlat = tdata_cur['lat'].values\n",
    "\n",
    "    # same as above but in gridpoint space, versus lat/lon space above\n",
    "    tx = tdata_cur['hdim_2'].values\n",
    "    ty = tdata_cur['hdim_1'].values\n",
    "\n",
    "    tlvls = np.arange(0,4001,200) # contours for topography\n",
    "    wlvls = np.arange(-70,70.1,2) # contours for vertical velocity\n",
    "\n",
    "    # Plot vertical velocity with feature locations on lat/lon grid\n",
    "    fig,ax = plt.subplots(1,1,figsize=(12,12))\n",
    "\n",
    "    a = ax.contourf(lons,lats,pltvar,levels=np.arange(-70,70.1,2),cmap=plt.cm.bwr,extend='both')\n",
    "    plt.colorbar(a,ax=ax)\n",
    "    b = ax.contour(lons,lats,topt,levels=tlvls,colors='k')\n",
    "    c = ax.scatter(tlon,tlat,s=5,c='k')\n",
    "\n",
    "    ax.set_title('MAX WP')\n",
    "    ax.grid() \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savepath+savename+'LATLON_WPMAX_'+cur_time+'.png')\n",
    "    plt.close(fig)\n",
    "    del(pltvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through files\n",
    "for i in np.arange(0,len(h5files1)): \n",
    "    #len(h5files1)\n",
    "    # Read in RAMS file\n",
    "    rams_file = h5py.File(h5files1[i], 'r')\n",
    "    cur_time = os.path.split(h5files1[i])[1][4:21] # Grab time string from RAMS file\n",
    "    print(cur_time)\n",
    "    pd_time = cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17]\n",
    "    print(pd_time)\n",
    "\n",
    "    # Read lat, lon, topography from RAMS files\n",
    "    lats = np.array(rams_file['GLAT'])\n",
    "    lons = np.array(rams_file['GLON'])\n",
    "    topt = np.array(rams_file['TOPT'][:])\n",
    "\n",
    "    # Calculcate z_coord using rams_tool function (reading it from header files)\n",
    "    zcoords = rams_tools.calc_zcoords(hefiles1[i])\n",
    "    #print(zcoords)\n",
    "    dzcoords = np.diff(zcoords)\n",
    "    \n",
    "    # Read in column maximum vertical velocity from RAMS files\n",
    "    #pltvar = np.array(np.nanmax(rams_file['WP'][:,:,:],axis=0))\n",
    "    pltvar = np.array(rams_file['WP'][51,:,:])\n",
    "\n",
    "    # Close RAMS file\n",
    "    rams_file.close()\n",
    "\n",
    "\n",
    "    # Grab current positions of tracked objects from current time.\n",
    "    tdata_cur = tdata[tdata['timestr'] == pd_time]\n",
    "    tlon = tdata_cur['lon'].values\n",
    "    tlat = tdata_cur['lat'].values\n",
    "\n",
    "    # same as above but in gridpoint space, versus lat/lon space above\n",
    "    tx = tdata_cur['hdim_2'].values\n",
    "    ty = tdata_cur['hdim_1'].values\n",
    "\n",
    "    tlvls = np.arange(0,4001,200) # contours for topography\n",
    "    wlvls = np.arange(-40,41,1) # contours for vertical velocity\n",
    "\n",
    "    # Plot vertical velocity with feature locations on lat/lon grid\n",
    "    ####\n",
    "    fig = plt.figure(figsize=(11,8))\n",
    "    AX = fig.add_subplot(1, 1, 1, projection=crs.PlateCarree())\n",
    "    C111 = AX.contourf(lons ,lats, pltvar,transform=crs.PlateCarree(),levels=wlvls,cmap=plt.get_cmap('bwr'),extend='both')#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "    C_terr = AX.contour(lons, lats, topt, levels=tlvls,linewidths=1.4,colors=\"saddlebrown\",transform=crs.PlateCarree())\n",
    "    c_tracks = AX.scatter(tlon,tlat,s=3,c='k')\n",
    "    plt.colorbar(C111)\n",
    "    gl = AX.gridlines()#color=\"gray\",alpha=0.5, linestyle='--',draw_labels=True,linewidth=2)\n",
    "    AX.coastlines(resolution='110m')\n",
    "    gl.xlines = True\n",
    "    gl.ylines = True\n",
    "    LATLON_LABELS=True\n",
    "    if LATLON_LABELS:\n",
    "        print('LATLON labels are on')\n",
    "        gl.xlabels_top = True\n",
    "        gl.ylabels_right = False\n",
    "        gl.ylabels_left = True\n",
    "        gl.ylabels_bottom = True\n",
    "    else:\n",
    "        gl.xlabels_top = False\n",
    "        gl.ylabels_right = False\n",
    "        gl.ylabels_left = False\n",
    "        gl.ylabels_bottom = True\n",
    "    #gl.xlines = False\n",
    "    #gl.xlocator = mticker.FixedLocator([-67, -66, 0, 45, 180])\n",
    "    #gl.xformatter = LONGITUDE_FORMATTER\n",
    "    #gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    gl.ylabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    \n",
    "    AX.set_title('WP (m/s) at 5 km')\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    png_filename=savename+'W_5km_'+cur_time+'.png'\n",
    "    print(png_filename)\n",
    "    plt.savefig(savepath+png_filename,dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    del(pltvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a990b6-6e16-4aab-939d-77518bd02043",
   "metadata": {},
   "source": [
    "# WRF Special functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43071e5e-9dc6-4943-bd52-905c3c10dc3f",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f2e9e-d43b-41a0-9676-ad2df5994b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2dvar_from_3dvar(VARIABLE_NAM,COORDINATE_VAR_ARRAY,THREED_VAR,COPIED_ARR,LVL_TYPE,LVL_VALUE,OUTPUT_VAR_DESCRIPTION,OUTPUT_VAR_UNITS):\n",
    "    \n",
    "    # if the 3D array passed as input for interpolation on a pressure or height surface is not a DataArray,\n",
    "    # convert it into one before interpolation\n",
    "    if not isinstance(THREED_VAR, xr.core.dataarray.DataArray):\n",
    "        print('This 3D array passed (for interpolation onto a surface) is a ',type(THREED_VAR))\n",
    "        print('... converting to Xarray Dataarray')\n",
    "        THREED_VAR = COPIED_ARR.copy(data=THREED_VAR)\n",
    "        if (LVL_TYPE == 'pressure'):\n",
    "            THREED_VAR.rename(VARIABLE_NAM+'_at_plev_'+str(LEVEL_VALUE))\n",
    "        elif LVL_TYPE == 'height':\n",
    "            THREED_VAR.rename(VARIABLE_NAM+'_at_zlev_'+str(LEVEL_VALUE))\n",
    "        elif LVL_TYPE == 'model':\n",
    "            THREED_VAR.rename(VARIABLE_NAM+'_at_mlev_'+str(LEVEL_VALUE))\n",
    "        elif LVL_TYPE == 'pv':\n",
    "            THREED_VAR.rename(VARIABLE_NAM+'_at_pvlev_'+str(LEVEL_VALUE))\n",
    "        elif LVL_TYPE == 'theta':\n",
    "            THREED_VAR.rename(VARIABLE_NAM+'_at_thetalev_'+str(LEVEL_VALUE))\n",
    "        elif LVL_TYPE == 'vert_avgd_pressure_levs':\n",
    "            THREED_VAR.rename(VARIABLE_NAM+'_vert_avgd_'+str(max(LEVEL_VALUE))+'-'+str(min(LEVEL_VALUE))+'hPa')\n",
    "        else: \n",
    "            print('<<<LVL_TYPE>>> INPUT INVALID!!!')\n",
    "        \n",
    "    if LVL_TYPE == 'pressure':\n",
    "        \n",
    "        print('interpolating ',VARIABLE_NAM,'to pressure level ',LVL_VALUE)\n",
    "        VAR2D = interplevel(THREED_VAR, COORDINATE_VAR_ARRAY, LVL_VALUE)\n",
    "        \n",
    "        if OUTPUT_VAR_DESCRIPTION:\n",
    "            VAR2D.attrs['description'] = OUTPUT_VAR_DESCRIPTION + ' at '+str(LVL_VALUE)+' hPa'\n",
    "        else:\n",
    "            print('no description attribute found. Setting description to variable name ...')\n",
    "            VAR2D.attrs['description'] = VARIABLE_NAM + ' at '+str(LVL_VALUE)+' hPa'\n",
    "            \n",
    "        VAR2D.attrs['units'] = OUTPUT_VAR_UNITS\n",
    "        \n",
    "    elif LVL_TYPE == 'height':\n",
    "        \n",
    "        print('interpolating ',VARIABLE_NAM,'to height level ',LVL_VALUE)\n",
    "        VAR2D = interplevel(THREED_VAR, COORDINATE_VAR_ARRAY, LVL_VALUE)\n",
    "        \n",
    "        if OUTPUT_VAR_DESCRIPTION:\n",
    "            VAR2D.attrs['description'] = OUTPUT_VAR_DESCRIPTION + ' at height level '+str(LVL_VALUE)+' m'\n",
    "        else:\n",
    "            VAR2D.attrs['description'] = VARIABLE_NAM + ' at height level '+str(LVL_VALUE)+' m'\n",
    "\n",
    "        VAR2D.attrs['units'] = OUTPUT_VAR_UNITS\n",
    "        \n",
    "    elif LVL_TYPE == 'pv':\n",
    "        \n",
    "        print('interpolating ',VARIABLE_NAM,'to pv level ',LVL_VALUE)\n",
    "        VAR2D = interplevel(THREED_VAR, COORDINATE_VAR_ARRAY, LVL_VALUE)\n",
    "        \n",
    "        if OUTPUT_VAR_DESCRIPTION:\n",
    "            VAR2D.attrs['description'] = OUTPUT_VAR_DESCRIPTION + ' at PV level '+str(LVL_VALUE)+' PVU'\n",
    "        else:\n",
    "            VAR2D.attrs['description'] = VARIABLE_NAM + ' at PV level '+str(LVL_VALUE)+' PVU'\n",
    "    \n",
    "        VAR2D.attrs['units'] = OUTPUT_VAR_UNITS\n",
    "        \n",
    "    elif LVL_TYPE == 'theta':\n",
    "        \n",
    "        print('interpolating ',VARIABLE_NAM,'to theta level ',LVL_VALUE)\n",
    "        #print(COORDINATE_VAR_ARRAY)\n",
    "        #print(THREED_VAR)\n",
    "        #print(THREED_VAR[10:,:,:])\n",
    "        #print(np.shape(THREED_VAR[10:,:,:]))\n",
    "        VAR2D = interplevel(THREED_VAR[::-1], COORDINATE_VAR_ARRAY[::-1], LVL_VALUE)\n",
    "        \n",
    "        if OUTPUT_VAR_DESCRIPTION:\n",
    "            VAR2D.attrs['description'] = OUTPUT_VAR_DESCRIPTION + ' at theta level '+str(LVL_VALUE)+' K'\n",
    "        else:\n",
    "            VAR2D.attrs['description'] = VARIABLE_NAM + ' at theta level '+str(LVL_VALUE)+' K'\n",
    "    \n",
    "        VAR2D.attrs['units'] = OUTPUT_VAR_UNITS\n",
    "        \n",
    "    elif LVL_TYPE == 'model':\n",
    "        \n",
    "        print('getting ',VARIABLE_NAM,'at model level ',LVL_VALUE+1)\n",
    "        VAR2D = THREED_VAR[LVL_VALUE,:,:]\n",
    "        \n",
    "        if OUTPUT_VAR_DESCRIPTION:\n",
    "            VAR2D.attrs['description'] = OUTPUT_VAR_DESCRIPTION + ' at model level '+str(LVL_VALUE)\n",
    "        else:\n",
    "            VAR2D.attrs['description'] = VARIABLE_NAM + ' at model level '+str(LVL_VALUE)\n",
    "        \n",
    "        VAR2D.attrs['units'] = OUTPUT_VAR_UNITS\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        print('<<<LVL_VALUE>>> INPUT INVALID!!!')\n",
    "        \n",
    "    return VAR2D\n",
    "\n",
    "def set_val_attrs_2darray(VARIABLE_NAM,VARIABLE_DATA,COPIED_ARR,LVL_TYPE,LVL_VALUE,VAR_DESCRIPTION,VAR_UNITS): \n",
    "    \n",
    "    VAR2D = COPIED_ARR.copy(data=VARIABLE_DATA)\n",
    "\n",
    "    if LVL_TYPE == 'pressure':\n",
    "        VAR2D = VAR2D.rename(VARIABLE_NAM+'_at_plev_'+str(LVL_VALUE))\n",
    "        if not hasattr(VAR2D,'description'):\n",
    "            VAR2D.attrs['description']=VARIABLE_NAM+' at '+ str(LVL_VALUE) + ' hPa'\n",
    "        else:\n",
    "            VAR2D.attrs['description']=VAR_DESCRIPTION+' at '+ str(LVL_VALUE) + ' hPa'\n",
    "    elif LVL_TYPE == 'height':\n",
    "        VAR2D = VAR2D.rename(VARIABLE_NAM+'_at_zlev_'+str(LVL_VALUE))\n",
    "        if not hasattr(VAR2D,'description'):\n",
    "            VAR2D.attrs['description']=VARIABLE_NAM+' at height level '+ str(LVL_VALUE) + ' m'\n",
    "        else:\n",
    "            VAR2D.attrs['description']=VAR_DESCRIPTION+' at height level'+ str(LVL_VALUE) + ' m'\n",
    "    elif LVL_TYPE == 'pv':\n",
    "        VAR2D = VAR2D.rename(VARIABLE_NAM+'_at_pvlev_'+str(LVL_VALUE))\n",
    "        if not hasattr(VAR2D,'description'):\n",
    "            VAR2D.attrs['description']=VARIABLE_NAM+' at PV level '+ str(LVL_VALUE) + ' PVU'\n",
    "        else:\n",
    "            VAR2D.attrs['description']=VAR_DESCRIPTION+' at PV level'+ str(LVL_VALUE) + ' PVU'\n",
    "    elif LVL_TYPE == 'theta':\n",
    "        VAR2D = VAR2D.rename(VARIABLE_NAM+'_at_thetalev_'+str(LVL_VALUE))\n",
    "        if not hasattr(VAR2D,'description'):\n",
    "            VAR2D.attrs['description']=VARIABLE_NAM+' at theta level '+ str(LVL_VALUE) + ' K'\n",
    "        else:\n",
    "            VAR2D.attrs['description']=VAR_DESCRIPTION+' at theta level'+ str(LVL_VALUE) + ' K'\n",
    "    elif LVL_TYPE == 'vert_avgd_pressure_levs':\n",
    "        VAR2D = VAR2D.rename(VARIABLE_NAM+'_vert_avgd_'+str(max(LVL_VALUE))+'-'+str(min(LVL_VALUE))+'hPa')\n",
    "        if not hasattr(VAR2D,'description'):\n",
    "            VAR2D.attrs['description']=VARIABLE_NAM+' vertically averaged b/w '+ str(max(LVL_VALUE))+'-'+str(min(LVL_VALUE))+ ' hPa'\n",
    "        else:\n",
    "            VAR2D.attrs['description']=VAR_DESCRIPTION+' vertically averaged b/w '+ str(max(LVL_VALUE))+'-'+str(min(LVL_VALUE))+ ' hPa'  \n",
    "    else:\n",
    "        print('This variable is only available at pressure, height, theta, and PV levels')\n",
    "\n",
    "    VAR2D.attrs['units']=VAR_UNITS\n",
    "    \n",
    "    return VAR2D\n",
    "\n",
    "def get_var_at_level_from_wrfoutput(NC_FILE,VARIABLE_NAME,LEVEL_TYPE,LEVEL_VALUE,TWOD_TEMPLATE_DATAARRAY,THREED_TEMPLATE_DATAARRAY):\n",
    "    \n",
    "    if (LEVEL_TYPE == 'pressure') or (LEVEL_TYPE == 'vert_avgd_pressure_levs'):\n",
    "        coord_var_array = getvar(NC_FILE, \"pressure\")\n",
    "    elif LEVEL_TYPE == 'height':\n",
    "        coord_var_array = getvar(NC_FILE, \"height\")\n",
    "    elif LEVEL_TYPE == 'pv':\n",
    "        coord_var_array = getvar(NC_FILE, \"pvo\")\n",
    "    elif LEVEL_TYPE == 'theta':\n",
    "        coord_var_array = getvar(NC_FILE, \"theta\")\n",
    "    elif LEVEL_TYPE == 'model':\n",
    "        coord_var_array = None\n",
    "    elif LEVEL_TYPE == '2D':\n",
    "        coord_var_array = None\n",
    "    else:\n",
    "        print('<<<LEVEL_TYPE>>> INPUT INVALID!!! (pressure or height ot pvo or theta or model or 2D)')\n",
    "        \n",
    "    # We first cover special variables that are neither in the WRF file nor in the wrf-python \n",
    "    # diagnostics table.\n",
    "    # All 2D variables passed into the plt.contour later have to xarray Dataarray objects with \n",
    "    # description and units. These attributes are passed into the title string\n",
    "    # The variable name is passed into the image file name string.\n",
    "    \n",
    "    if (LEVEL_TYPE == 'pressure') or (LEVEL_TYPE == 'height') or (LEVEL_TYPE == 'model') or (LEVEL_TYPE == 'pv') or (LEVEL_TYPE == 'theta'):\n",
    "        if VARIABLE_NAME=='rel_vort':\n",
    "            print('Variable: ',VARIABLE_NAME)\n",
    "            var3d =  (getvar(NC_FILE, \"avo\")*(10**-5) - getvar(NC_FILE, \"F\")) \n",
    "            var2d = get_2dvar_from_3dvar(VARIABLE_NAME,coord_var_array,var3d,THREED_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'relative vorticity','1/s')\n",
    "\n",
    "        if VARIABLE_NAME=='cape_3d':\n",
    "            print('Variable: ',VARIABLE_NAME)\n",
    "            var3d,temp =  getvar(NC_FILE, \"cape_3d\")\n",
    "            var2d = get_2dvar_from_3dvar(VARIABLE_NAME,coord_var_array,var3d,THREED_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'3D CAPE','J/kg')\n",
    "            \n",
    "        elif VARIABLE_NAME=='frontogenesis':\n",
    "            theta = getvar(NC_FILE,'theta')\n",
    "            u,v = getvar(NC_FILE,'uvmet')\n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "\n",
    "            theta_lev = interplevel(theta, coord_var_array, LEVEL_VALUE).values*units('K')\n",
    "            u_lev = interplevel(u, coord_var_array, LEVEL_VALUE).values*units('m/s')\n",
    "            v_lev = interplevel(v, coord_var_array, LEVEL_VALUE).values*units('m/s')\n",
    "\n",
    "            fronto = mpcalc.frontogenesis(theta_lev, u = u_lev, v = v_lev, dx=dx, dy=dy)*1.08*(10**9)\n",
    "\n",
    "            var2d = set_val_attrs_2darray(VARIABLE_NAME,fronto.magnitude,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'Frontogenesis','K/100km/3h')\n",
    "        \n",
    "        elif VARIABLE_NAME=='d_wspd_dt':\n",
    "            print('CURRENTLY SET TO D02; CHANGE DOMAIN and TIMESTEP MANUALLY IN THE FUNCTION')\n",
    "            directory=\"/glade/u/home/isingh9/scratch/WRF_4.3_run_theta_tend_template/LHT_ON_70_vertlevs_maxdom2/\"\n",
    "            time_step=20. # mins\n",
    "            wrf_timess=getvar(NC_FILE,'times')\n",
    "            search_string = (pd.to_datetime(wrf_timess.values) -  pd.Timedelta(minutes=time_step)).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "            second_file = sorted(glob.glob(directory+\"wrfout_d02_\"+search_string))\n",
    "            print('second file is ',second_file)\n",
    "            wspd1 = getvar(NC_FILE,'wspd')\n",
    "            wspd1_lev = interplevel(wspd1, coord_var_array, LEVEL_VALUE)\n",
    "            NC_FILE1 = Dataset(second_file[0])\n",
    "            wspd2 = getvar(NC_FILE1,'wspd')\n",
    "            wspd2_lev = interplevel(wspd2, coord_var_array, LEVEL_VALUE)\n",
    "            d_wspd_dt = (wspd1_lev.values - wspd2_lev.values)/time_step\n",
    "            var2d = set_val_attrs_2darray(VARIABLE_NAME,d_wspd_dt,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'wind speed tendency','m/s/min')\n",
    "            NC_FILE1.close()\n",
    "    \n",
    "        elif VARIABLE_NAME=='meridional_index':\n",
    "            v   = getvar(NC_FILE,'va')\n",
    "            print('calculating meridional index at '+str(LEVEL_VALUE)+' hPa')\n",
    "            var2d_values = np.abs(interplevel(v, coord_var_array, LEVEL_VALUE).values)#.values*units('kg/kg')\n",
    "            #print('mean value is ',np.mean(var2d_values))\n",
    "            var2d = set_val_attrs_2darray(VARIABLE_NAME,var2d_values,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'Absolute val of meridional wind','m/s')\n",
    "        \n",
    "        elif VARIABLE_NAME=='moisture_conv':\n",
    "            qq = getvar(NC_FILE,'QVAPOR')\n",
    "            u,v = getvar(NC_FILE,'uvmet')\n",
    "\n",
    "            qq_lev = interplevel(qq, coord_var_array, LEVEL_VALUE)#.values*units('kg/kg')\n",
    "            u_lev = interplevel(u, coord_var_array, LEVEL_VALUE)#.values*units('m/s')\n",
    "            v_lev = interplevel(v, coord_var_array, LEVEL_VALUE)#.values*units('m/s')\n",
    "            moist_conv = -1000.0* qq_plev *(np.gradient(u_plev,GRID_SPACING_XY,axis=1) + np.gradient(v_plev,GRID_SPACING_XY,axis=0)) * 86400.0 \n",
    "\n",
    "            var2d = set_val_attrs_2darray(VARIABLE_NAME,moist_conv.magnitude,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'moisture convergence','g/kg d^{-1}')\n",
    "\n",
    "        elif VARIABLE_NAME=='pv_adv':\n",
    "            pvo = getvar(NC_FILE,'pvo')\n",
    "            u,v = getvar(NC_FILE,'uvmet')\n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "            #print(dx)\n",
    "            pv_lev = interplevel(pvo, coord_var_array, LEVEL_VALUE)#.values*units('kg/kg')\n",
    "            u_lev = interplevel(u, coord_var_array, LEVEL_VALUE)#.values*units('m/s')\n",
    "            v_lev = interplevel(v, coord_var_array, LEVEL_VALUE)#.values*units('m/s')\n",
    "\n",
    "            #var2d = (u_plev*np.gradient(pv_plev,GRID_SPACING_XY,axis=1) + v_plev*np.gradient(pv_plev,GRID_SPACING_XY,axis=0)) * -3600.\n",
    "            pv_adv = mpcalc.advection(pv_lev.values, u=u_lev.values, v=v_lev.values,dx=dx.magnitude, dy=dy.magnitude)*3600.\n",
    "\n",
    "            var2d = set_val_attrs_2darray(VARIABLE_NAME,pv_adv.magnitude,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'PV advection','PVU/hr')\n",
    "               \n",
    "        elif VARIABLE_NAME=='smoothed_divergence':\n",
    "            u_3d = getvar(NC_FILE,'ua')\n",
    "            v_3d = getvar(NC_FILE,'va')\n",
    "            \n",
    "            u_2d_above    = interplevel(u_3d       , coord_var_array, LEVEL_VALUE-100)\n",
    "            v_2d_above    = interplevel(v_3d       , coord_var_array, LEVEL_VALUE-100)\n",
    "            \n",
    "            u_2d_main     = interplevel(u_3d       , coord_var_array, LEVEL_VALUE)\n",
    "            v_2d_main     = interplevel(v_3d       , coord_var_array, LEVEL_VALUE)\n",
    "            \n",
    "            u_2d_below    = interplevel(u_3d       , coord_var_array, LEVEL_VALUE+100)\n",
    "            v_2d_below    = interplevel(v_3d       , coord_var_array, LEVEL_VALUE+100)\n",
    "            \n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "            \n",
    "            div_above = mpcalc.divergence(u_2d_above.values*units('m/s'), v_2d_above.values*units('m/s'), dx=dx, dy=dy)\n",
    "            div_main  = mpcalc.divergence(u_2d_main.values*units('m/s'), v_2d_main.values*units('m/s'), dx=dx, dy=dy)\n",
    "            div_below = mpcalc.divergence(u_2d_below.values*units('m/s'), v_2d_below.values*units('m/s'), dx=dx, dy=dy)\n",
    "\n",
    "            div_mean = (div_above + div_main + div_below)/3.0\n",
    "            \n",
    "            print(np.shape(div_mean))\n",
    "            \n",
    "            n_reps = 20\n",
    "            div_smooth = mpcalc.smooth_n_point(div_mean, 9, n_reps)\n",
    "            var2d = set_val_attrs_2darray(VARIABLE_NAME,div_smooth.magnitude,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,\\\n",
    "                                          'Divergence (smoothed) avgd. over 200 hPa cent. at ','1/s')\n",
    "\n",
    "        elif VARIABLE_NAME=='pv_adv_ageo':\n",
    "            pvo = getvar(NC_FILE,'pvo')\n",
    "            pv_lev = interplevel(pvo, coord_var_array, LEVEL_VALUE)#.values*units('kg/kg')\n",
    "            \n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "            #print(dx)\n",
    "            \n",
    "            geo_hgts_3d = getvar(NC_FILE,'height')\n",
    "            geo_hgts = interplevel(geo_hgts_3d, coord_var_array, LEVEL_VALUE)\n",
    "            geo_hgts = ndimage.gaussian_filter(geo_hgts.values, sigma=1.5, order=0)\n",
    "            \n",
    "            u_3d = getvar(NC_FILE,'ua')\n",
    "            v_3d = getvar(NC_FILE,'va')\n",
    "            u_2d     = interplevel(u_3d       , coord_var_array, LEVEL_VALUE)\n",
    "            v_2d     = interplevel(v_3d       , coord_var_array, LEVEL_VALUE)\n",
    "            #ageo_wind_u, ageo_wind_v = mpcalc.ageostrophic_wind(geo_hgts.values*units.m,u=u_2d, v = v_2d, latitude=np.deg2rad(lat_2d.values),dx=20000.*units.m,dy=20000.*units.m)\n",
    "            ageo_wind_u, ageo_wind_v = mpcalc.ageostrophic_wind(geo_hgts*units.m,u=u_2d, v = v_2d, latitude=np.deg2rad(lat_2d.values),dx=dx,dy=dy)\n",
    "            \n",
    "            #var2d = (u_plev*np.gradient(pv_plev,GRID_SPACING_XY,axis=1) + v_plev*np.gradient(pv_plev,GRID_SPACING_XY,axis=0)) * -3600.\n",
    "            pv_adv = mpcalc.advection(pv_lev.values, u=ageo_wind_u, v=ageo_wind_v,dx=dx.magnitude, dy=dy.magnitude)*3600.\n",
    "\n",
    "            var2d = set_val_attrs_2darray(VARIABLE_NAME,pv_adv.magnitude,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'PV advection by ageostrophic winds','PVU/hr')\n",
    "            print('min values : ',np.nanmin(var2d.values))\n",
    "            print('max values : ',np.nanmax(var2d.values))\n",
    "            \n",
    "        elif VARIABLE_NAME=='NBE':\n",
    "            radius_earth = 6378100.0\n",
    "            omega = 2*np.pi/(24*60*60)\n",
    "            u,v = getvar(NC_FILE,'uvmet')\n",
    "            geopotential = getvar(NC_FILE,'geopotential')\n",
    "            cor_param = getvar(NC_FILE, \"F\")\n",
    "            rel_vort =  (getvar(NC_FILE, \"avo\")*(10**-5) - cor_param) \n",
    "            \n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "\n",
    "            rossby_param = 2.0*omega*np.cos(lat_2d.values*np.pi/180.0)/radius_earth\n",
    "            \n",
    "            u_lev = interplevel(u, coord_var_array, LEVEL_VALUE)#.values*units('m/s')\n",
    "            v_lev = interplevel(v, coord_var_array, LEVEL_VALUE)#.values*units('m/s')\n",
    "            geopot_lev = interplevel(geopotential, coord_var_array, LEVEL_VALUE)\n",
    "            rel_vort_lev = interplevel(rel_vort, coord_var_array, LEVEL_VALUE)\n",
    "            \n",
    "            d_u_dy = mpcalc.first_derivative(u_lev.values, axis=0, delta=dy).magnitude\n",
    "            d_u_dx = mpcalc.first_derivative(u_lev.values, axis=1, delta=dx).magnitude\n",
    "            \n",
    "            d_v_dy = mpcalc.first_derivative(v_lev.values, axis=0, delta=dy).magnitude\n",
    "            d_v_dx = mpcalc.first_derivative(v_lev.values, axis=1, delta=dx).magnitude \n",
    "            \n",
    "            laplacian_geopot = mpcalc.laplacian(geopot_lev.values,deltas=[dy.magnitude,dx.magnitude])\n",
    "            \n",
    "            nbe = 2.0*(d_u_dx*d_v_dy - d_v_dx*d_u_dy) + cor_param.values*rel_vort_lev.values \\\n",
    "                  -1.0*rossby_param*u_lev.values - laplacian_geopot\n",
    "            \n",
    "            nbe = ndimage.gaussian_filter(nbe, sigma=1.5, order=0) # smoothen\n",
    "            #var2d = (u_plev*np.gradient(pv_plev,GRID_SPACING_XY,axis=1) + v_plev*np.gradient(pv_plev,GRID_SPACING_XY,axis=0)) * -3600.\n",
    "\n",
    "            var2d = set_val_attrs_2darray(VARIABLE_NAME,nbe*10**6,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'NBE residual','s^-2')\n",
    "\n",
    "        elif VARIABLE_NAME=='pv_diabatic_tendency':\n",
    "            p1 = getvar(NC_FILE,'pres',units='Pa')\n",
    "            theta_tend = getvar(NC_FILE,'H_DIABATIC')\n",
    "            ua = getvar(NC_FILE,'ua')\n",
    "            va = getvar(NC_FILE,'va')\n",
    "            wa = getvar(NC_FILE,'wa')\n",
    "            planetary_vort_vert = getvar(NC_FILE,'F')\n",
    "            \n",
    "            print('shape of ua is ',np.shape(ua))\n",
    "\n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "            dp =  p1[1:,:,:].values - p1[:-1,:,:].values\n",
    "            #print('dp',dp)\n",
    "            dx = dx[None,:,:]\n",
    "            dy = dy[None,:,:]\n",
    "\n",
    "            d_theta_dp = mpcalc.first_derivative(theta_tend.values, axis=0, delta=dp).magnitude\n",
    "            d_theta_dy = mpcalc.first_derivative(theta_tend.values, axis=1, delta=dy).magnitude\n",
    "            d_theta_dx = mpcalc.first_derivative(theta_tend.values, axis=2, delta=dx).magnitude\n",
    "\n",
    "            d_ua_dp = mpcalc.first_derivative(ua.values, axis=0, delta=dp).magnitude\n",
    "            d_ua_dy = mpcalc.first_derivative(ua.values, axis=1, delta=dy).magnitude\n",
    "            d_ua_dx = mpcalc.first_derivative(ua.values, axis=2, delta=dx).magnitude\n",
    "\n",
    "            d_va_dp = mpcalc.first_derivative(va.values, axis=0, delta=dp).magnitude\n",
    "            d_va_dy = mpcalc.first_derivative(va.values, axis=1, delta=dy).magnitude\n",
    "            d_va_dx = mpcalc.first_derivative(va.values, axis=2, delta=dx).magnitude\n",
    "\n",
    "            # may need to omit this part\n",
    "            d_wa_dp = 0.0 #mpcalc.first_derivative(wa.values, axis=0, delta=dp).magnitude\n",
    "            d_wa_dy = mpcalc.first_derivative(wa.values, axis=1, delta=dy).magnitude\n",
    "            d_wa_dx = mpcalc.first_derivative(wa.values, axis=2, delta=dx).magnitude\n",
    "            ###\n",
    "            \n",
    "            var3d = -9.81*((d_theta_dx*(d_wa_dy - d_va_dp)) + (d_theta_dy*(d_ua_dp - d_wa_dx)) + (d_theta_dp*(d_va_dx - d_ua_dy + planetary_vort_vert.values[None,:,:])))*(10**6)*3600.\n",
    "            #var3d = (1.0/density)*((d_theta_dz*(d_va_dx - d_ua_dy + planetary_vort_vert.values[None,:,:])))#*(10**6)*3600.0\n",
    "            print('shape of 3d pv dia tend is ',np.shape(var3d))\n",
    "            \n",
    "            \n",
    "            temp3d=ua.copy()\n",
    "            print(np.shape(temp3d))\n",
    "            temp3d.values=var3d\n",
    "            \n",
    "            var2d_data = interplevel(temp3d, coord_var_array, LEVEL_VALUE)#.values*units('kg/kg')\n",
    "            \n",
    "            var2d = set_val_attrs_2darray(VARIABLE_NAME,var2d_data,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'PV debataic tendency','PVU/hr')\n",
    "\n",
    "        elif VARIABLE_NAME=='geostrophic_wind':\n",
    "            geo_hgts_3d = getvar(NC_FILE,'height')\n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "\n",
    "            geo_hgts = interplevel(geo_hgts_3d, coord_var_array, LEVEL_VALUE)\n",
    "            geo_hgts = ndimage.gaussian_filter(geo_hgts.values, sigma=1.5, order=0) # smoothen the heights\n",
    "\n",
    "            geo_wind_u, geo_wind_v = mpcalc.geostrophic_wind(geo_hgts*units.m,latitude=np.deg2rad(lat_2d.values),dx=dx[:,:],dy=dy[:,:])\n",
    "            var2d_data = np.sqrt(geo_wind_u.magnitude**2 + geo_wind_v.magnitude**2)\n",
    "\n",
    "            var2d = set_val_attrs_2darray(VARIABLE_NAME,var2d_data,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'geostrophic wind','m/s')\n",
    "\n",
    "        elif VARIABLE_NAME=='ageostrophic_wind':\n",
    "            geo_hgts_3d = getvar(NC_FILE,'height')\n",
    "            u_3d = getvar(NC_FILE,'ua')\n",
    "            v_3d = getvar(NC_FILE,'va')\n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "\n",
    "            geo_hgts = interplevel(geo_hgts_3d, coord_var_array, LEVEL_VALUE)\n",
    "            u_2d     = interplevel(u_3d       , coord_var_array, LEVEL_VALUE)\n",
    "            v_2d     = interplevel(v_3d       , coord_var_array, LEVEL_VALUE)\n",
    "\n",
    "            geo_hgts = ndimage.gaussian_filter(geo_hgts.values, sigma=1.5, order=0)\n",
    "\n",
    "            #ageo_wind_u, ageo_wind_v = mpcalc.ageostrophic_wind(geo_hgts.values*units.m,u=u_2d, v = v_2d, latitude=np.deg2rad(lat_2d.values),dx=20000.*units.m,dy=20000.*units.m)\n",
    "            ageo_wind_u, ageo_wind_v = mpcalc.ageostrophic_wind(geo_hgts*units.m,u=u_2d, v = v_2d, latitude=np.deg2rad(lat_2d.values),dx=dx,dy=dy)\n",
    "            var2d_data = np.sqrt(ageo_wind_u.magnitude**2 + ageo_wind_v.magnitude**2)\n",
    "\n",
    "            var2d = set_val_attrs_2darray(VARIABLE_NAME,var2d_data,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'ageostrophic wind','m/s')\n",
    "            \n",
    "            \n",
    "        elif VARIABLE_NAME=='Q-vector_div':\n",
    "            n_reps = 150\n",
    "            \n",
    "            geo_hgts_3d = getvar(NC_FILE,'height')\n",
    "            p      = getvar(NC_FILE, \"pressure\")\n",
    "            #u_3d   = getvar(NC_FILE,'ua')\n",
    "            #v_3d   = getvar(NC_FILE,'va')\n",
    "            tk_3d  = getvar(NC_FILE,'tk')\n",
    "            \n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "\n",
    "            geo_hgts = interplevel(geo_hgts_3d, p, LEVEL_VALUE).values\n",
    "            geo_hgts = ndimage.gaussian_filter(geo_hgts, sigma=10, order=0) # smoothen the heights\n",
    "            u_2d_s, v_2d_s = mpcalc.geostrophic_wind(geo_hgts*units.m,latitude=np.deg2rad(lat_2d.values),dx=dx[:,:],dy=dy[:,:])\n",
    "\n",
    "            #u_2d     = interplevel(u_3d       , p, LEVEL_VALUE).values*units('m/s')\n",
    "            #v_2d     = interplevel(v_3d       , p, LEVEL_VALUE).values*units('m/s')\n",
    "            tk_2d_s    = interplevel(tk_3d      , p, LEVEL_VALUE).values\n",
    "            #print('temp ',tk_2d)\n",
    "\n",
    "            #u_2d_s   = mpcalc.smooth_gaussian(u_2d,  8)\n",
    "            #v_2d_s   = mpcalc.smooth_gaussian(v_2d,  8)\n",
    "            #tk_2d_s  = mpcalc.smooth_gaussian(tk_2d, 8)\n",
    "            \n",
    "            #u_2d_s   = mpcalc.smooth_n_point(u_2d,  9, n_reps)\n",
    "            #v_2d_s   = mpcalc.smooth_n_point(v_2d,  9, n_reps)\n",
    "            #tk_2d_s  = mpcalc.smooth_n_point(tk_2d, 9, n_reps)\n",
    "            tk_2d_s = ndimage.gaussian_filter(tk_2d_s, sigma=10, order=0)\n",
    "            \n",
    "            uqvect, vqvect = mpcalc.q_vector(u=u_2d_s, v=v_2d_s, temperature=tk_2d_s*units('K'  ), pressure=LEVEL_VALUE*units('hPa'), dx=dx, dy=dy)\n",
    "            print('u component of q vector ',uqvect)\n",
    "            var2d_vals = -2.0*mpcalc.divergence(uqvect, vqvect, dx=dx, dy=dy).m*1e16\n",
    "            #var2d_vals = np.sign(var2d_vals) * np.log10(np.abs(var2d_vals))\n",
    "            var2d_vals = ndimage.gaussian_filter(var2d_vals, sigma=2, order=0) # smoothen the heights\n",
    "            var2d = set_val_attrs_2darray(VARIABLE_NAME,var2d_vals,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'Q-vector divergence','10^16 m kg-1 s-1')\n",
    "\n",
    "        # Add more special variables here\n",
    "        elif ( (VARIABLE_NAME=='qg_omega_termA') | (VARIABLE_NAME=='qg_omega_termB') | (VARIABLE_NAME=='qg_omega_RHS')):\n",
    "            \n",
    "            print('NOTE: only works at pressure levels')\n",
    "            # Set constant values that will be needed in computations\n",
    "            \n",
    "            # Number of repetitions of smoothing function\n",
    "            n_reps = 50\n",
    "            pres_diff_layers = 200\n",
    "            \n",
    "            # Set default static stability value\n",
    "            sigma = 2.5* 10**-6 * units('m^2 Pa^-2 s^-2')\n",
    "\n",
    "            # Set f-plane at typical synoptic f0 value\n",
    "            f0 = -0.771 * 1e-4 * units('s^-1')\n",
    "\n",
    "            # Use dry gas constant from MetPy constants\n",
    "            Rd = mpconstants.Rd\n",
    "            #print(Rd)\n",
    "            \n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "            \n",
    "            p1 = getvar(NC_FILE,'pressure')\n",
    "            \n",
    "            geo_hgts_3d = getvar(NC_FILE,'height')\n",
    "            \n",
    "            geo_hgts_main_level      = interplevel(geo_hgts_3d, p1, LEVEL_VALUE      )\n",
    "            geo_hgts_one_level_above = interplevel(geo_hgts_3d, p1, LEVEL_VALUE - pres_diff_layers)\n",
    "            geo_hgts_one_level_below = interplevel(geo_hgts_3d, p1, LEVEL_VALUE + pres_diff_layers)\n",
    "            #print('geo_hgts_main_level',geo_hgts_main_level)\n",
    "            #print('geo_hgts_one_level_below',geo_hgts_one_level_below)\n",
    "            #print('geo_hgts_one_level_above',geo_hgts_one_level_above)\n",
    "            \n",
    "            geo_hgts_mains   = mpcalc.smooth_n_point(geo_hgts_main_level.values, 9, n_reps)\n",
    "            geo_hgts_aboves  = mpcalc.smooth_n_point(geo_hgts_one_level_above.values, 9, n_reps)\n",
    "            geo_hgts_belows  = mpcalc.smooth_n_point(geo_hgts_one_level_below.values, 9, n_reps)\n",
    "            \n",
    "            #geo_hgts_mains  = ndimage.gaussian_filter(geo_hgts_main_level.values, sigma=1.5, order=0) # smoothen the heights\n",
    "            #geo_hgts_aboves = ndimage.gaussian_filter(geo_hgts_one_level_above.values, sigma=1.5, order=0) # smoothen the heights\n",
    "            #geo_hgts_belows = ndimage.gaussian_filter(geo_hgts_one_level_below.values, sigma=1.5, order=0) # smoothen the heights\n",
    "            #print('geo_hgts_belows',geo_hgts_belows)\n",
    "            \n",
    "            #uwnd_mains,  vwnd_mains  = mpcalc.geostrophic_wind(geo_hgts_mains*units.m,latitude=np.deg2rad(lat_2d.values),dx=dx[:,:],dy=dy[:,:])\n",
    "            #uwnd_aboves, vwnd_aboves = mpcalc.geostrophic_wind(geo_hgts_aboves*units.m,latitude=np.deg2rad(lat_2d.values),dx=dx[:,:],dy=dy[:,:])\n",
    "            #uwnd_belows, vwnd_belows = mpcalc.geostrophic_wind(geo_hgts_belows*units.m,latitude=np.deg2rad(lat_2d.values),dx=dx[:,:],dy=dy[:,:])\n",
    "            #print(np.shape(uwnd_aboves))\n",
    "            #print('uwnd_aboves',uwnd_aboves)\n",
    "            #\n",
    "            tk_3d              = getvar(NC_FILE,'tk')\n",
    "            tk_main_level      = interplevel(tk_3d, p1, LEVEL_VALUE       ).values*units('K')\n",
    "            print('tk_main_level',tk_main_level)\n",
    "            tmpk_mains  = mpcalc.smooth_n_point(tk_main_level, 9, n_reps)#.metpy.unit_array\n",
    "            #print('tk_mains',tmpk_mains)\n",
    "            #print('shape of tmpk_mains',np.shape(tmpk_mains))\n",
    "            #tmpc_mains  = tmpk_mains.to('degC')\n",
    "            \n",
    "            uwnd_3d              = getvar(NC_FILE,'ua')\n",
    "            uwnd_one_level_above = interplevel(uwnd_3d, p1, LEVEL_VALUE  - pres_diff_layers).values*units('m/s')\n",
    "            uwnd_main_level      = interplevel(uwnd_3d, p1, LEVEL_VALUE       ).values*units('m/s')\n",
    "            uwnd_one_level_below = interplevel(uwnd_3d, p1, LEVEL_VALUE  + pres_diff_layers).values*units('m/s')\n",
    "            print('uwnd_one_level_below',uwnd_one_level_below)\n",
    "            \n",
    "            vwnd_3d              = getvar(NC_FILE,'va')\n",
    "            vwnd_one_level_above = interplevel(vwnd_3d, p1, LEVEL_VALUE  - pres_diff_layers).values*units('m/s')\n",
    "            vwnd_main_level      = interplevel(vwnd_3d, p1, LEVEL_VALUE       ).values*units('m/s')\n",
    "            vwnd_one_level_below = interplevel(vwnd_3d, p1, LEVEL_VALUE  + pres_diff_layers).values*units('m/s')\n",
    "\n",
    "            uwnd_mains = mpcalc.smooth_n_point(uwnd_main_level, 9, n_reps)#.metpy.unit_array\n",
    "            vwnd_mains = mpcalc.smooth_n_point(vwnd_main_level, 9, n_reps)#.metpy.unit_array\n",
    "\n",
    "            uwnd_aboves = mpcalc.smooth_n_point(uwnd_one_level_above, 9, n_reps)#.metpy.unit_array\n",
    "            vwnd_aboves = mpcalc.smooth_n_point(vwnd_one_level_above, 9, n_reps)#.metpy.unit_array\n",
    "\n",
    "            uwnd_belows = mpcalc.smooth_n_point(uwnd_one_level_below, 9, n_reps)#.metpy.unit_array\n",
    "            vwnd_belows = mpcalc.smooth_n_point(vwnd_one_level_below, 9, n_reps)#.metpy.unit_array\n",
    "            \n",
    "            # Absolute Vorticity Calculation\n",
    "            #avo_3d              = getvar(NC_FILE,'avo') *10**-5\n",
    "            #avo_one_level_above = interplevel(avo_3d, coord_var_array, LEVEL_VALUE  - pres_diff_layers).values*units('1/s')\n",
    "            #avo_one_level_below = interplevel(avo_3d, coord_var_array, LEVEL_VALUE  + pres_diff_layers).values*units('1/s')\n",
    "            cor_param = getvar(NC_FILE, \"F\").values\n",
    "            avo_one_level_above = mpcalc.vorticity(u=uwnd_aboves, v=vwnd_aboves, dx=dx, dy=dy) + cor_param*units('1/s')\n",
    "            avo_one_level_below = mpcalc.vorticity(u=uwnd_belows, v=vwnd_belows, dx=dx, dy=dy) + cor_param*units('1/s')\n",
    "            #print('avo_one_level_below',avo_one_level_below)\n",
    "\n",
    "            # Advection of Absolute Vorticity\n",
    "            vortadv_one_level_above = mpcalc.advection(avo_one_level_above,u=uwnd_aboves,v=vwnd_aboves,dx=dx,dy=dy).to_base_units()\n",
    "            vortadv_one_level_below = mpcalc.advection(avo_one_level_below,u=uwnd_belows,v=vwnd_belows,dx=dx,dy=dy).to_base_units()\n",
    "\n",
    "            # Differential Vorticity Advection between two levels\n",
    "            diff_avor = ((vortadv_one_level_below - vortadv_one_level_above)/(2.0*pres_diff_layers*100. * units.Pa)).to_base_units()\n",
    "\n",
    "            # Calculation of final differential vorticity advection term\n",
    "            print('calculating Term A in the omega equation: differential vorticity advection')\n",
    "            term_A = (-f0 / sigma * diff_avor).to_base_units()\n",
    "            #print(term_A.units)\n",
    "            \n",
    "            # 700-hPa Temperature Advection\n",
    "            tadv_main_level = mpcalc.advection(tmpk_mains,u=uwnd_mains,v=vwnd_mains,dx=dx,dy=dy).to_base_units()\n",
    "            #print(tadv_main_level)\n",
    "            #print(np.shape(tadv_main_level.magnitude))\n",
    "            # Laplacian of Temperature Advection\n",
    "            #print(dy)\n",
    "            lap_tadv_main_level = mpcalc.laplacian(tadv_main_level, deltas=[dy, dx])\n",
    "            \n",
    "            # Final term B calculation with constants\n",
    "            print('calculating Term B in the omega equation: temperature advection')\n",
    "            term_B = (-Rd / (sigma * (LEVEL_VALUE * 100. * units.Pa)) * lap_tadv_main_level).to_base_units()\n",
    "            \n",
    "            print('units of tmpk_mains',tmpk_mains.units)\n",
    "            print('units of avo_one_level_above',avo_one_level_above.units)\n",
    "            print('units of diff_avor',diff_avor.units)\n",
    "            print('units of term_A',term_A.units)\n",
    "            print('units of tadv_main_level',tadv_main_level.units)\n",
    "            print('units of lap_tadv_main_level',lap_tadv_main_level.units)\n",
    "            print('units of term_B',term_B.units)\n",
    "            \n",
    "            RHS = term_A + term_B\n",
    "            \n",
    "            if VARIABLE_NAME   =='qg_omega_termA':\n",
    "                var2d = set_val_attrs_2darray(VARIABLE_NAME,term_A*10**12,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'QG Omega Forcing: Diff. Vorticity advection','10^12 kg m^-3 s^-3')\n",
    "            elif VARIABLE_NAME =='qg_omega_termB':\n",
    "                var2d = set_val_attrs_2darray(VARIABLE_NAME,term_B*10**12,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'QG Omega Forcing: Temperature advection','10^12 kg m^-3 s^-3')\n",
    "            elif VARIABLE_NAME =='qg_omega_RHS':\n",
    "                var2d = set_val_attrs_2darray(VARIABLE_NAME,RHS*10**12,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,'QG Omega Forcing: Total','10^12 kg m^-3 s^-3')\n",
    "            else:\n",
    "                print('Enter a valid variable name: qg_omega_termA or qg_omega_termB or qg_omega_RHS')\n",
    "        else:\n",
    "        #print('not a special variable')\n",
    "            var3d = getvar(NC_FILE,VARIABLE_NAME)\n",
    "            var2d = get_2dvar_from_3dvar(VARIABLE_NAME,coord_var_array,var3d,THREED_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,var3d.description,var3d.units)\n",
    "\n",
    "    elif LEVEL_TYPE=='2D':\n",
    "        #MCAPE/MCIN/LCL/LFC\n",
    "        if VARIABLE_NAME=='rain':\n",
    "            RAINC = getvar(NC_FILE,'RAINC')\n",
    "            var2d = RAINC + getvar(NC_FILE,'RAINNC') + getvar(NC_FILE,'RAINSH')\n",
    "            var2d.attrs['description'] = 'total rain'\n",
    "            var2d.attrs['units'] = RAINC.units\n",
    "        elif VARIABLE_NAME=='cape':\n",
    "            var2d,temp2,temp3,temp4 = getvar(NC_FILE,'cape_2d')\n",
    "            var2d.attrs['description'] = 'CAPE'\n",
    "            var2d.attrs['units'] = 'J/kg'\n",
    "        elif VARIABLE_NAME=='cin':\n",
    "            temp1,var2d,temp3,temp4  = getvar(NC_FILE,'cape_2d')\n",
    "            var2d.attrs['description'] = 'CIN'\n",
    "            var2d.attrs['units'] = 'J/kg'\n",
    "        elif VARIABLE_NAME=='lcl':\n",
    "            temp1,temp2,var2d,temp4 = getvar(NC_FILE,'cape_2d')\n",
    "            var2d.attrs['description'] = 'LCL'\n",
    "            var2d.attrs['units'] = 'm'\n",
    "        elif VARIABLE_NAME=='lfc':\n",
    "            temp1,temp2,temp3,var2d = getvar(NC_FILE,'cape_2d')\n",
    "            var2d.attrs['description'] = 'LFC'\n",
    "            var2d.attrs['units'] = 'm'\n",
    "        elif VARIABLE_NAME=='T2':\n",
    "            var2d = getvar(NC_FILE,'T2') - 273.15\n",
    "            var2d.attrs['description'] = '2-m temperature'\n",
    "            var2d.attrs['units'] = 'deg C'\n",
    "        elif VARIABLE_NAME=='0-6km_shear':\n",
    "            print(' NOT completeted !!!')\n",
    "            var2d = getvar(NC_FILE,'T2') - 273.15\n",
    "            var2d.attrs['description'] = '0-6 km shear'\n",
    "            var2d.attrs['units'] = 'knots'\n",
    "        elif VARIABLE_NAME=='vint_mfc':\n",
    "            qq = getvar(NC_FILE,'QVAPOR')[0:-1,:,:]\n",
    "            u,v = getvar(NC_FILE,'uvmet')\n",
    "            u = u[0:-1,:,:]\n",
    "            v = v[0:-1,:,:]\n",
    "            tk = getvar(NC_FILE,'tk')[0:-1,:,:]\n",
    "            p = getvar(NC_FILE,'p',units='Pa')[0:-1,:,:]\n",
    "            heights = getvar(NC_FILE,'z')\n",
    "            rho = p.values/(281.0 * tk.values)\n",
    "            \n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "            dx = dx[None,:,:]\n",
    "            dy = dy[None,:,:]\n",
    "            dz =  heights[1:,:,:].values - heights[:-1,:,:].values\n",
    "            div = mpcalc.first_derivative(rho*u.values*qq.values, axis=2, delta=dx).magnitude + \\\n",
    "                  mpcalc.first_derivative(rho*v.values*qq.values, axis=1, delta=dy).magnitude\n",
    "            var2d_vals = np.sum(div*dz,axis=0)\n",
    "            var2d = TWOD_TEMPLATE_DATAARRAY.copy(data=var2d_vals)\n",
    "            var2d.attrs['description'] = 'Vint MFC'\n",
    "            var2d.attrs['units'] = 'kg m^-2 s^-1'\n",
    "            #print(var2d)\n",
    "            #print(np.shape(var2d))\n",
    "        elif VARIABLE_NAME=='IVT':\n",
    "            qq = getvar(NC_FILE,'QVAPOR')[0:-1,:,:]\n",
    "            #print()\n",
    "            u = getvar(NC_FILE,'ua')\n",
    "            v = getvar(NC_FILE,'va')\n",
    "            u = u[0:-1,:,:]\n",
    "            v = v[0:-1,:,:]\n",
    "            tk = getvar(NC_FILE,'tk')[0:-1,:,:]\n",
    "            p = getvar(NC_FILE,'p',units='Pa')[0:-1,:,:]\n",
    "            heights = getvar(NC_FILE,'z')\n",
    "            rho = p.values/(281.0 * tk.values)\n",
    "            \n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "            dx = dx[None,:,:]\n",
    "            dy = dy[None,:,:]\n",
    "            dz =  heights[1:,:,:].values - heights[:-1,:,:].values\n",
    "            #print('dz: ',dz)\n",
    "            ivt_3d = qq*rho*np.sqrt(u**2 + v**2)*dz\n",
    "            #print('shape of ivt 3d: ', np.shape(ivt_3d))\n",
    "            var2d_vals = np.sum(ivt_3d,axis=0)\n",
    "            var2d = TWOD_TEMPLATE_DATAARRAY.copy(data=var2d_vals)\n",
    "            var2d.attrs['description'] = 'Integrated Water Vapor Transport'\n",
    "            var2d.attrs['units'] = '$kg m^{-1} s^{-1}$'\n",
    "            #print(var2d)\n",
    "            #print(np.shape(var2d))\n",
    "        elif VARIABLE_NAME=='rain_rate':\n",
    "            directory='/glade/u/home/isingh9/scratch/WRF_4.3_run_theta_tend_template/LHT_ON_70_vertlevs_maxdom2/'\n",
    "            time_diff_files_mins = 60.\n",
    "            print('plotting rain rate ...')\n",
    "            print('using the directory <<',directory,'>> to search for previous file to calculate rain rate')\n",
    "            print('using a time difference of ',time_diff_files_mins,' mins to calculate rain rate in mm/min')\n",
    "            wrf_time_rr = getvar(NC_FILE,'times')\n",
    "            search_string = (pd.to_datetime(wrf_time_rr.values) -  pd.Timedelta(minutes=60)).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "            #print('main file: ',NC_FILE)\n",
    "            #print('search string: ',search_string)\n",
    "            second_file = sorted(glob.glob(directory+\"wrfout_d01_\"+search_string))\n",
    "            print('second file used to calculate rr: ',second_file)\n",
    "            rainc = getvar(NC_FILE,'RAINC')\n",
    "            rain = rainc + getvar(NC_FILE,'RAINNC') + getvar(NC_FILE,'RAINSH')\n",
    "            rain.attrs['description'] = 'total rain'\n",
    "            rain.attrs['units'] = rainc.units\n",
    "\n",
    "            NC_FILE1 = Dataset(second_file[0])\n",
    "            rainc1 = getvar(NC_FILE1,'RAINC')\n",
    "            rain1 = rainc1 + getvar(NC_FILE1,'RAINNC') + getvar(NC_FILE1,'RAINSH')\n",
    "            rain1.attrs['description'] = 'total rain'\n",
    "            rain1.attrs['units'] = rainc1.units\n",
    "\n",
    "            NC_FILE1.close()\n",
    "            \n",
    "            var2d_vals = (rain - rain1)/time_diff_files_mins\n",
    "            var2d = TWOD_TEMPLATE_DATAARRAY.copy(data=var2d_vals)\n",
    "            var2d.attrs['description'] = 'rain rate'\n",
    "            var2d.attrs['units'] = 'mm/min'\n",
    "            #print(rr)\n",
    "            #print('max value  of rr =',rr.max())\n",
    "            #print('min value  of rr =',rr.min())\n",
    "        elif VARIABLE_NAME=='vert_avgd_H_DIABATIC':\n",
    "            theta_tend_microphys = getvar(NC_FILE,'H_DIABATIC')\n",
    "            #print(theta_tend_microphys)\n",
    "            var2d_vals = theta_tend_microphys.mean(dim='bottom_top').values*3600.\n",
    "            var2d = TWOD_TEMPLATE_DATAARRAY.copy(data=var2d_vals)\n",
    "            var2d.attrs['description'] = 'vertically averaged theta tendency (microphysics)'\n",
    "            var2d.attrs['units'] = 'K/hr'\n",
    "            \n",
    "        elif VARIABLE_NAME=='500-200hpa_H_DIABATIC':\n",
    "            theta_tend_microphys = getvar(NC_FILE,'H_DIABATIC')\n",
    "            p_levs = [200,250,300,350,400,450,500]\n",
    "            pressure = getvar(NC_FILE,'pressure')\n",
    "            hdia_levs = interplevel(theta_tend_microphys, pressure, p_levs)#.values*units('kg/kg')\n",
    "            print('hdia_levs at p levs: ',hdia_levs)\n",
    "            var2d_vals = np.nanmean(hdia_levs.values*3600.,axis=0)\n",
    "            var2d = TWOD_TEMPLATE_DATAARRAY.copy(data=var2d_vals)\n",
    "            var2d.attrs['description'] = '500-200 hPa vertically averaged theta tendency (microphysics)'\n",
    "            var2d.attrs['units'] = 'K/hr'\n",
    "            \n",
    "        \n",
    "        elif VARIABLE_NAME=='height_of_max_heating_mp':\n",
    "            theta_tend_microphys = getvar(NC_FILE,'H_DIABATIC')\n",
    "            #print(np.shape(theta_tend_microphys))\n",
    "            heights = getvar(NC_FILE,'height').values\n",
    "            max_heating_ind = np.argmax(theta_tend_microphys.values, axis=0)\n",
    "            #print('max_heating_ind shape: ',np.shape(max_heating_ind))\n",
    "            ax_1 = np.arange(heights.shape[1])[:,None]\n",
    "            ax_2 = np.arange(heights.shape[2])[None,:]\n",
    "            \n",
    "            #print('shape of ax_1', np.shape(ax_1))\n",
    "            #print(ax_1)\n",
    "            #print('shape of ax_2', np.shape(ax_2))\n",
    "            #print(ax_2)\n",
    "            \n",
    "            var2d_vals = heights[max_heating_ind, ax_1, ax_2]\n",
    "            var2d = TWOD_TEMPLATE_DATAARRAY.copy(data=var2d_vals)\n",
    "            var2d.attrs['description'] = 'Height of maximum microphysical heating'\n",
    "            var2d.attrs['units'] = 'm'\n",
    "            \n",
    "        elif VARIABLE_NAME=='250-150hpa_pv_adv_ageo':\n",
    "            p_levs = [250,200,150]\n",
    "            pvo = getvar(NC_FILE,'pvo')\n",
    "            pressure = getvar(NC_FILE,'pressure')\n",
    "            pv_levs = interplevel(pvo, pressure, p_levs)#.values*units('kg/kg')\n",
    "            \n",
    "        \n",
    "            lat_2d = getvar(NC_FILE,'lat')\n",
    "            lon_2d = getvar(NC_FILE,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "            #print(dx)\n",
    "            dx=dx[None,:,:]\n",
    "            dy=dy[None,:,:]\n",
    "            \n",
    "            geo_hgts_3d = getvar(NC_FILE,'height')\n",
    "            geo_hgts = interplevel(geo_hgts_3d, pressure, p_levs)\n",
    "            geo_hgts = ndimage.gaussian_filter(geo_hgts.values, sigma=1.5, order=0)\n",
    "            \n",
    "            u_3d = getvar(NC_FILE,'ua')\n",
    "            v_3d = getvar(NC_FILE,'va')\n",
    "            u_2d     = interplevel(u_3d       , pressure, p_levs)\n",
    "            v_2d     = interplevel(v_3d       , pressure, p_levs)\n",
    "            \n",
    "            #ageo_wind_u, ageo_wind_v = mpcalc.ageostrophic_wind(geo_hgts.values*units.m,u=u_2d, v = v_2d, latitude=np.deg2rad(lat_2d.values),dx=20000.*units.m,dy=20000.*units.m)\n",
    "            ageo_wind_u, ageo_wind_v = mpcalc.ageostrophic_wind(geo_hgts*units.m,u=u_2d, v = v_2d, latitude=np.deg2rad(lat_2d.values),dx=dx,dy=dy)\n",
    "            \n",
    "            #var2d = (u_plev*np.gradient(pv_plev,GRID_SPACING_XY,axis=1) + v_plev*np.gradient(pv_plev,GRID_SPACING_XY,axis=0)) * -3600.\n",
    "            pv_adv = mpcalc.advection(pv_levs.values, u=ageo_wind_u, v=ageo_wind_v,dx=dx.magnitude, dy=dy.magnitude)*3600.\n",
    "            \n",
    "            var2d = TWOD_TEMPLATE_DATAARRAY.copy(data=np.nanmean(pv_adv.magnitude,axis=0))\n",
    "            var2d.attrs['description'] = 'Vertically averaged (250-150 hPa) PV advection by ageostrophic winds'\n",
    "            var2d.attrs['units'] = 'PV/hr'\n",
    "\n",
    "        else:\n",
    "            var2d = getvar(NC_FILE,VARIABLE_NAME)\n",
    "            \n",
    "    elif LEVEL_TYPE=='vert_avgd_pressure_levs':\n",
    "        #print('Variable for vertical averaging: ',VARIABLE_NAME)\n",
    "        var3d = getvar(NC_FILE, VARIABLE_NAME)\n",
    "        var_at_levs=[]\n",
    "        #print('getting variable at ',len(LEVEL_VALUE),' levels')\n",
    "        for lev in LEVEL_VALUE:\n",
    "            #print('level : ',str(lev)+' hPa')\n",
    "            var3d = getvar(NC_FILE,VARIABLE_NAME)\n",
    "            var2d_temp = get_2dvar_from_3dvar(VARIABLE_NAME,coord_var_array,var3d,THREED_TEMPLATE_DATAARRAY,'pressure',lev,var3d.description,var3d.units)\n",
    "            #print(var2d_temp)\n",
    "            var_at_levs.append(var2d_temp.values)\n",
    "            \n",
    "        print('shape of the var at multiple levels ',np.shape(var_at_levs))\n",
    "        var2d_vals = np.nanmean(np.array(var_at_levs),axis=0)\n",
    "        var2d = set_val_attrs_2darray(VARIABLE_NAME,var2d_vals,TWOD_TEMPLATE_DATAARRAY,LEVEL_TYPE,LEVEL_VALUE,var3d.description,var3d.units)\n",
    "    else:\n",
    "        print('unknown LEVEL/VARIABLE TYPE !!!')\n",
    "   \n",
    "    #print(var2d)\n",
    "    return var2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d68dc-e63f-4fc5-be05-76d3a0ac5599",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff53413-6ccf-42ae-856c-b8dbf7d860b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform calculations on a sub-domain to speed things up\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import time\n",
    "\n",
    "def plot_WRF_3dvar_parallel(RUN, DOMAIN,SUPTITLE,PANEL_TEXT,FILE,GRID_SPACING_XY,VARTYPE,VARNAME,LEVEL,VAR_SMOOTH,\\\n",
    "                   MIN_LEV,MAX_LEV,STEP_LEV,NUMLEVELS,COLORMAP,\\\n",
    "                   AREA_TAG,LONS,LATS,LATLON_LABELS,\\\n",
    "                   PLOT_ANOTHER_VAR_CONT,\\\n",
    "                   PLOT_WINDS,WINDS_INFO,PLOT_REFL,PLOT_UH,PLOT_UPDRAFT,PLOT_RAIN,PLOT_RAIN_RATE,PLOT_MSLP,PLOT_HGTS,PLOT_HGTS1,\\\n",
    "                   HATCHING,\\\n",
    "                   WINDS_THIN_XY,\\\n",
    "                   PLOT_LINE,PT1,PT2,\\\n",
    "                   TERR):\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(16,12)) # Create a figure\n",
    "    AX = fig.add_subplot(1, 1, 1, projection=crs.PlateCarree())\n",
    "    \n",
    "    # Open the NetCDF file\n",
    "    nc_file = Dataset(FILE)\n",
    "\n",
    "    wrf_times = getvar(nc_file,'times')\n",
    "    tiempo = np.datetime_as_string(wrf_times.values, timezone='UTC',unit='m')\n",
    "        \n",
    "    if VARTYPE!='2D':\n",
    "        var2d = get_var_at_level_from_wrfoutput(nc_file,VARNAME,VARTYPE,LEVEL,terr,temp)\n",
    "        #print(var2d)\n",
    "        save_string =  DOMAIN+'_'+VARNAME+'_at_'+VARTYPE+'_'+str(LEVEL)+'_'+tiempo+'_'+AREA_TAG+'.png'\n",
    "        title_string = var2d.description + ' (' +var2d.units + ') \\n'\n",
    "        #title_string = title_string.replace ('at ', '\\nat ')\n",
    "    elif VARTYPE=='2D':\n",
    "        var2d = get_var_at_level_from_wrfoutput(nc_file,VARNAME,VARTYPE,LEVEL,terr,temp)\n",
    "        save_string = VARNAME\n",
    "        title_string = var2d.description + ' (' +var2d.units + ') \\n'\n",
    "    else:\n",
    "        print('The values of VARTYPE can only be <2D> or <pressure> or <height> or <model> or <pv> !')\n",
    "        \n",
    "    print('minimum  value of var: ',np.nanmin(var2d.values))\n",
    "    print('maximum  value of var: ',np.nanmax(var2d.values))\n",
    "        \n",
    "    if VAR_SMOOTH:\n",
    "        print('smoothing the primary variable...')\n",
    "        var2d.values = ndimage.gaussian_filter(var2d.values, sigma=1.5, order=0)\n",
    "    #print(var2d)\n",
    "    \n",
    "    if PLOT_ANOTHER_VAR_CONT[0]:\n",
    "        var2d_1 = get_var_at_level_from_wrfoutput(nc_file,PLOT_ANOTHER_VAR_CONT[1],PLOT_ANOTHER_VAR_CONT[2],PLOT_ANOTHER_VAR_CONT[3],terr,temp)\n",
    "        \n",
    "        if PLOT_ANOTHER_VAR_CONT[10]:\n",
    "            print('smoothing the secondary variable...')\n",
    "            var2d_1.values = ndimage.gaussian_filter(var2d_1.values, sigma=1.5, order=0)\n",
    "            \n",
    "        if PLOT_ANOTHER_VAR_CONT[5]:\n",
    "            if isinstance(PLOT_ANOTHER_VAR_CONT[7],str):\n",
    "                levels  = np.arange(PLOT_ANOTHER_VAR_CONT[4],PLOT_ANOTHER_VAR_CONT[5],PLOT_ANOTHER_VAR_CONT[6])\n",
    "                levels = levels[levels!=0]\n",
    "                #np.where(levels >= 0, \"-\", \"--\")\n",
    "                C111 = AX.contour(LONS ,LATS, var2d_1.values,transform=crs.PlateCarree(),levels=levels,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles='-',linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            elif isinstance(PLOT_ANOTHER_VAR_CONT[7],matplotlib.colors.LinearSegmentedColormap):\n",
    "                levels  = np.arange(PLOT_ANOTHER_VAR_CONT[4],PLOT_ANOTHER_VAR_CONT[5],PLOT_ANOTHER_VAR_CONT[6])\n",
    "                levels = levels[levels!=0]\n",
    "                C111 = AX.contour(LONS ,LATS, var2d_1.values,transform=crs.PlateCarree(),levels=levels,cmap=PLOT_ANOTHER_VAR_CONT[7],linewidths=PLOT_ANOTHER_VAR_CONT[8],linestyles='-')#,linestyles='--')\n",
    "                #C111.monochrome = True\n",
    "                #for col, ls in zip(C111.collections, C111._process_linestyles()):\n",
    "                #    col.set_linestyle(ls)\n",
    "            else:\n",
    "                print('Need to specify correct color/colormap for the secondary contour')\n",
    "                #break\n",
    "        else:\n",
    "            #print('no max value given for secondary variable')\n",
    "            levels  = np.arange(PLOT_ANOTHER_VAR_CONT[4],PLOT_ANOTHER_VAR_CONT[5],PLOT_ANOTHER_VAR_CONT[6])\n",
    "            C111 = AX.contour(LONS ,LATS, var2d_1.values,transform=crs.PlateCarree(),levels=[PLOT_ANOTHER_VAR_CONT[4]],colors=PLOT_ANOTHER_VAR_CONT[7],linestyles='--',linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "        \n",
    "        if PLOT_ANOTHER_VAR_CONT[9]:\n",
    "            plt.clabel(C111, inline=1, fontsize=10, fmt=\"%i\")\n",
    "            \n",
    "        \n",
    "    if PLOT_UH:\n",
    "        uh = getvar(nc_file,'updraft_helicity')\n",
    "        C0 = AX.contour(LONS, LATS, uh.values,transform=crs.PlateCarree(),\\\n",
    "                 levels = [-250.],colors='blue',linestyles='-')\n",
    "    \n",
    "    if PLOT_UPDRAFT:\n",
    "        wa_ud = getvar(nc_file,'wa')\n",
    "        ww = wa_ud[12:31,:,:].mean(axis=0)\n",
    "        #print(ww)\n",
    "        AX.contour(LONS, LATS, ww.values,levels=[8.],\\\n",
    "                     linewidths=1.3,colors=\"green\",transform=crs.PlateCarree()) #fuchsia\n",
    "        \n",
    "    if PLOT_RAIN:\n",
    "        RAINC = getvar(nc_file,'RAINC')\n",
    "        rain = RAINC + getvar(nc_file,'RAINNC') + getvar(nc_file,'RAINSH')\n",
    "        rain.attrs['description'] = 'total rain'\n",
    "        rain.attrs['units'] = RAINC.units\n",
    "        AX.contour(LONS ,LATS, rain.values,transform=crs.PlateCarree(),\\\n",
    "             cmap=cma4,linewidths=1.3)\n",
    "        \n",
    "    if PLOT_RAIN_RATE:\n",
    "        search_string = (pd.to_datetime(wrf_times.values) -  pd.Timedelta(minutes=60)).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        second_file = sorted(glob.glob(directory+\"wrfout_d01_\"+search_string))\n",
    "        \n",
    "        rainc = getvar(nc_file,'RAINC')\n",
    "        rain = rainc + getvar(nc_file,'RAINNC') + getvar(nc_file,'RAINSH')\n",
    "        rain.attrs['description'] = 'total rain'\n",
    "        rain.attrs['units'] = rainc.units\n",
    "        \n",
    "        nc_file1 = Dataset(second_file[0])\n",
    "        rainc1 = getvar(nc_file1,'RAINC')\n",
    "        rain1 = rainc1 + getvar(nc_file1,'RAINNC') + getvar(nc_file1,'RAINSH')\n",
    "        rain1.attrs['description'] = 'total rain'\n",
    "        rain1.attrs['units'] = rainc1.units\n",
    "        \n",
    "        rr = (rain - rain1)/60.\n",
    "        rr.attrs['description'] = 'rain rate'\n",
    "        rr.attrs['units'] = 'mm/min'\n",
    "        \n",
    "        nc_file1.close()\n",
    "        \n",
    "        #print(rr)\n",
    "        #print('max value  of rr =',rr.max())\n",
    "        #print('min value  of rr =',rr.min())\n",
    "        \n",
    "        C4_1 = AX.contour(LONS ,LATS, rr.values,levels=np.arange(0.4,2.8,0.4),transform=crs.PlateCarree(),\\\n",
    "             cmap=cma4,linewidths=1.3,extend=True) # cma4 plt.get_cmap('autumn')\n",
    "        #\n",
    "    \n",
    "    if MIN_LEV!=None:\n",
    "        levels  = np.arange(MIN_LEV,MAX_LEV,STEP_LEV)\n",
    "        C1 = AX.contourf(LONS ,LATS, var2d.values,transform=crs.PlateCarree(),levels=levels,\\\n",
    "             extend='both',cmap=COLORMAP)\n",
    "    else:\n",
    "        var2d_without_inf = np.where((var2d==np.inf),-1.0*10**-34,var2d)\n",
    "        var2d_without_inf = np.where((var2d_without_inf==np.nan),-1.0*10**-34,var2d_without_inf)\n",
    "        if (np.nanmin(var2d_without_inf))==0.0 and (np.nanmax(var2d_without_inf))==0.0:\n",
    "            C1 = AX.contourf(LONS, LATS, var2d.values,transform=crs.PlateCarree(),\\\n",
    "                 extend='both',cmap=COLORMAP)\n",
    "        else:\n",
    "            C1 = AX.contourf(LONS, LATS, var2d.values,transform=crs.PlateCarree(),\\\n",
    "                 levels = np.linspace(np.amin(var2d.values),np.amax(var2d.values),NUMLEVELS),extend='both',\\\n",
    "                 cmap=COLORMAP)\n",
    "    AX.background_patch.set_facecolor('gray')\n",
    "    \n",
    "    # Terrain\n",
    "    C2 = AX.contour(LONS, LATS, TERR, np.array([1000.]),\\\n",
    "                     linewidths=1.4,colors=\"saddlebrown\",transform=crs.PlateCarree())\n",
    "    #np.array([500.,1000.,1500.,2000.,2500.])\n",
    "    \n",
    "    if PLOT_WINDS:\n",
    "        n=2\n",
    "        winds_thin_x=WINDS_THIN_XY\n",
    "        \n",
    "        if WINDS_INFO[0]=='regular':\n",
    "            if WINDS_INFO[1]=='sfc':\n",
    "                print('plotting wind barbs at the surface')\n",
    "                u10,v10=getvar(nc_file,'uvmet10')\n",
    "                C3=AX.barbs(LONS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x], LATS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       u10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       v10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],linewidth=0.9, length=5, pivot='middle',transform=crs.PlateCarree(),\\\n",
    "                            flip_barb=True)\n",
    "\n",
    "                #if WIND_STYLE=='sl':\n",
    "                #    C3=AX.streamplot(LONS, LATS, u10.values, v10.values, density=1, linewidth=0.9, color='k', cmap=None,\\\n",
    "                #              norm=None, arrowsize=1.5, arrowstyle='-|>', minlength=0.1, transform=crs.PlateCarree(),\\\n",
    "                #              zorder=None, start_points=None, integration_direction='both',data=None)\n",
    "\n",
    "            elif WINDS_INFO[1]=='p_level':\n",
    "                print('plotting wind barbs at pressure level ',WINDS_INFO[2])\n",
    "                u, v = getvar(nc_file,'uvmet')\n",
    "                p = getvar(nc_file, \"pressure\")\n",
    "                u10 = interplevel(u, p, WINDS_INFO[2])\n",
    "                v10 = interplevel(v, p, WINDS_INFO[2])\n",
    "                C3=AX.barbs(LONS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x], LATS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       u10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       v10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],linewidth=0.9, length=5, pivot='middle',transform=crs.PlateCarree(),\\\n",
    "                            flip_barb=True)  \n",
    "\n",
    "            elif WINDS_INFO[1]=='z_level':\n",
    "                print('plotting wind barbs at height level ',WINDS_INFO[2])\n",
    "                u, v = getvar(nc_file,'uvmet')\n",
    "                z = getvar(nc_file, \"z\")\n",
    "                u10 = interplevel(u, z, WINDS_INFO[2])\n",
    "                v10 = interplevel(v, z, WINDS_INFO[2])\n",
    "                C3=AX.barbs(LONS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x], LATS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       u10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       v10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],linewidth=0.9, length=5, pivot='middle',transform=crs.PlateCarree(),\\\n",
    "                            flip_barb=True)\n",
    "                \n",
    "        elif WINDS_INFO[0]=='geostrophic':\n",
    "            if WINDS_INFO[1]=='p_level':\n",
    "                print('plotting geostrophic wind barbs at pressure level ',WINDS_INFO[2])\n",
    "                lat_2d = getvar(nc_file,'lat')\n",
    "                p = getvar(nc_file, \"pressure\")\n",
    "                geo_hgts_3d = getvar(nc_file,'height')\n",
    "                geo_hgts = interplevel(geo_hgts_3d, p, WINDS_INFO[2])\n",
    "                geo_wind_u, geo_wind_v = mpcalc.geostrophic_wind(geo_hgts.values*units.m,latitude=np.deg2rad(lat_2d.values),dx=20000.*units.m,dy=20000.*units.m)\n",
    "                u10 = geo_wind_u.magnitude\n",
    "                v10 = geo_wind_v.magnitude\n",
    "                C3=AX.barbs(LONS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x], LATS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       u10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       v10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],linewidth=0.9, length=5, pivot='middle',transform=crs.PlateCarree(),\\\n",
    "                            flip_barb=True)\n",
    "            elif WINDS_INFO[1]=='z_level':\n",
    "                print('plotting geostrophic wind barbs at height level ',WINDS_INFO[2])\n",
    "                #z = getvar(nc_file, \"z\")\n",
    "                lat_2d = getvar(nc_file,'lat')\n",
    "                geo_hgts_3d = getvar(nc_file,'height')\n",
    "                geo_hgts = interplevel(geo_hgts_3d, geo_hgts_3d, WINDS_INFO[2])\n",
    "                geo_wind_u, geo_wind_v = mpcalc.geostrophic_wind(geo_hgts.values*units.m,latitude=np.deg2rad(lat_2d.values),dx=20000.*units.m,dy=20000.*units.m)\n",
    "                u10 = geo_wind_u.magnitude\n",
    "                v10 = geo_wind_v.magnitude\n",
    "                C3=AX.barbs(LONS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x], LATS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       u10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       v10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],linewidth=0.9, length=5, pivot='middle',transform=crs.PlateCarree(),\\\n",
    "                            flip_barb=True)\n",
    "        elif WINDS_INFO[0]=='ageostrophic':\n",
    "            if WINDS_INFO[1]=='p_level':\n",
    "                print('plotting  ageostrophic wind barbs at pressure level ',WINDS_INFO[2])\n",
    "                lat_2d = getvar(nc_file,'lat')\n",
    "                p = getvar(nc_file, \"pressure\")\n",
    "                geo_hgts_3d = getvar(nc_file,'height')\n",
    "                u_3d = getvar(nc_file,'ua')\n",
    "                v_3d = getvar(nc_file,'va')\n",
    "                geo_hgts = interplevel(geo_hgts_3d, p, WINDS_INFO[2])\n",
    "                u_2d     = interplevel(u_3d       , p, WINDS_INFO[2])\n",
    "                v_2d     = interplevel(v_3d       , p, WINDS_INFO[2])\n",
    "                ageo_wind_u, ageo_wind_v = mpcalc.ageostrophic_wind(geo_hgts.values*units.m,u=u_2d, v = v_2d, latitude=np.deg2rad(lat_2d.values),dx=20000.*units.m,dy=20000.*units.m)\n",
    "                u10 = ageo_wind_u.magnitude\n",
    "                v10 = ageo_wind_v.magnitude\n",
    "                C3=AX.barbs(LONS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x], LATS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       u10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       v10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],linewidth=0.9, length=5, pivot='middle',transform=crs.PlateCarree(),\\\n",
    "                            flip_barb=True)\n",
    "            elif WINDS_INFO[1]=='z_level':\n",
    "                print('plotting ageostrophic wind barbs at height level ',WINDS_INFO[2])\n",
    "                #z = getvar(nc_file, \"z\")\n",
    "                lat_2d = getvar(nc_file,'lat')\n",
    "                geo_hgts_3d = getvar(nc_file,'height')\n",
    "                u_3d = getvar(nc_file,'ua')\n",
    "                v_3d = getvar(nc_file,'va')\n",
    "                geo_hgts = interplevel(geo_hgts_3d, geo_hgts_3d, WINDS_INFO[2])\n",
    "                u_2d     = interplevel(u_3d       , geo_hgts_3d, WINDS_INFO[2])\n",
    "                v_2d     = interplevel(v_3d       , geo_hgts_3d, WINDS_INFO[2])\n",
    "                ageo_wind_u, ageo_wind_v = mpcalc.ageostrophic_wind(geo_hgts.values*units.m,u=u_2d, v = v_2d, latitude=np.deg2rad(lat_2d.values),dx=20000.*units.m,dy=20000.*units.m)\n",
    "                u10 = ageo_wind_u.magnitude\n",
    "                v10 = ageo_wind_v.magnitude\n",
    "                C3=AX.barbs(LONS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x], LATS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       u10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       v10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],linewidth=0.9, length=5, pivot='middle',transform=crs.PlateCarree(),\\\n",
    "                            flip_barb=True)\n",
    "                \n",
    "        elif WINDS_INFO[0]=='Q-vector':\n",
    "            winds_thin_x=10\n",
    "            n_reps_quiver = 50\n",
    "            if WINDS_INFO[1]=='p_level':\n",
    "                print('plotting Q-vectors at ',WINDS_INFO[2],' hPa')\n",
    "                p      = getvar(nc_file, \"pressure\")\n",
    "                z      = getvar(nc_file, \"height\")\n",
    "                #theta  = getvar(nc_file, \"theta\")\n",
    "                u_3d   = getvar(nc_file,'ua')\n",
    "                v_3d   = getvar(nc_file,'va')\n",
    "                tk_3d  = getvar(nc_file,'tk')\n",
    "                \n",
    "                u_2d     = interplevel(u_3d       , p, WINDS_INFO[2]).values*units('m/s')\n",
    "                v_2d     = interplevel(v_3d       , p, WINDS_INFO[2]).values*units('m/s')\n",
    "                tk_2d    = interplevel(tk_3d      , p, WINDS_INFO[2]).values*units('K'  )\n",
    "                print('temp ',tk_2d)\n",
    "                \n",
    "                u_2d_s   = mpcalc.smooth_n_point(u_2d,  9, n_reps_quiver)\n",
    "                v_2d_s   = mpcalc.smooth_n_point(v_2d,  9, n_reps_quiver)\n",
    "                tk_2d_s  = mpcalc.smooth_n_point(tk_2d, 9, n_reps_quiver)\n",
    "                \n",
    "                lat_2d = getvar(nc_file,'lat')\n",
    "                lon_2d = getvar(nc_file,'lon')\n",
    "                dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "                \n",
    "                #dp     = (p[1:,:,:].values*100.0 - p[:-1,:,:].values*100.0)\n",
    "                #sigma    = (mpconstants.Rd.magnitude * tk_2d_s.magnitude/WINDS_INFO[2]*100.0)*mpcalc.first_derivative(np.log(theta.values), axis=0, delta=dp)\n",
    "                #sigma_2d = interplevel(sigma       , p.values, WINDS_INFO[2]).units('m^2 Pa^-2 s^-2')\n",
    "                #print(sigma) \n",
    "                \n",
    "                u10, v10 = mpcalc.q_vector(u_2d_s, v_2d_s, tk_2d_s, WINDS_INFO[2]*units('hPa'), dx=dx, dy=dy)\n",
    "                print(u10)\n",
    "                C3231=AX.quiver(LONS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x], LATS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                       u10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x].m,\\\n",
    "                       v10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x].m,pivot='middle',scale=2e-11, scale_units='inches',transform=crs.PlateCarree())\n",
    "                #qk = AX.quiverkey(C3231, 0.49, 0.95, 500, r'$ 500 kg m^{-1} s^{-1}$', labelpos='E',\n",
    "                #       coordinates='figure')\n",
    "                \n",
    "        elif WINDS_INFO[0]=='IVT':\n",
    "            print('plotting IVT vectors...')\n",
    "            qq = getvar(nc_file,'QVAPOR')[0:-1,:,:]\n",
    "            #print()\n",
    "            u,v = getvar(nc_file,'uvmet')\n",
    "            #v = getvar(NC_FILE,'va')\n",
    "            u = u[0:-1,:,:]\n",
    "            v = v[0:-1,:,:]\n",
    "            tk = getvar(nc_file,'tk')[0:-1,:,:]\n",
    "            p = getvar(nc_file,'p',units='Pa')[0:-1,:,:]\n",
    "            heights = getvar(nc_file,'z')\n",
    "            rho = p.values/(281.0 * tk.values)\n",
    "\n",
    "            lat_2d = getvar(nc_file,'lat')\n",
    "            lon_2d = getvar(nc_file,'lon')\n",
    "            dx, dy = mpcalc.lat_lon_grid_deltas(lon_2d.values*units('degrees'),lat_2d.values*units('degrees'))\n",
    "            dx = dx[None,:,:]\n",
    "            dy = dy[None,:,:]\n",
    "            dz =  heights[1:,:,:].values - heights[:-1,:,:].values\n",
    "            #print('dz: ',dz)\n",
    "            u10 = np.sum(qq*rho*u*dz,axis=0) \n",
    "            v10 = np.sum(qq*rho*v*dz,axis=0) \n",
    "            #print('shape of ivt 3d: ', np.shape(ivt_3d))\n",
    "            C3231=AX.quiver(LONS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x], LATS[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                   u10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],\\\n",
    "                   v10[n:-1*n:winds_thin_x,n:-1*n:winds_thin_x],pivot='middle',transform=crs.PlateCarree())\n",
    "            qk = AX.quiverkey(C3231, 0.49, 0.95, 500, r'$ 500 kg m^{-1} s^{-1}$', labelpos='E',\n",
    "                   coordinates='figure')\n",
    "\n",
    "        else:\n",
    "            print('please provide correct value of WINDS_TYPE: sfc, p_level, or z_level')\n",
    "        \n",
    "    if PLOT_REFL[0]:\n",
    "        print('plotting '+str(int(PLOT_REFL[1]))+'-dBZ reflectivity contours')\n",
    "        refl = getvar(nc_file,'dbz')[0,:,:].values\n",
    "        refl[refl < -5.0] = 0.0\n",
    "        #print(np.max(refl))\n",
    "        C4 = AX.contour(LONS ,LATS, refl,transform=crs.PlateCarree(),levels=[PLOT_REFL[1]],colors=PLOT_REFL[2],linestyle='-')\n",
    "   \n",
    "    if PLOT_MSLP:\n",
    "        print('plotting mslp')\n",
    "        slp = getvar(nc_file,'slp')\n",
    "        C41 = AX.contour(LONS ,LATS, slp.values,transform=crs.PlateCarree(),levels=np.arange(990,1010,2),colors='k',linestyles='-',linewidths=0.5)\n",
    "        plt.clabel(C41,fontsize=9)\n",
    "        \n",
    "    if PLOT_HGTS[0]:\n",
    "        p = getvar(nc_file, \"pressure\")\n",
    "        z = getvar(nc_file, \"z\", units=\"m\")\n",
    "        z_prs_lvl = interplevel(z, p, PLOT_HGTS[1])\n",
    "        if PLOT_HGTS[1]==850:\n",
    "            levs=np.arange(1000,3500,40)\n",
    "        elif PLOT_HGTS[1]==700:\n",
    "            levs=np.arange(2000,4500,40)\n",
    "        elif PLOT_HGTS[1]==500:\n",
    "            levs=np.arange(5000,6000,40)\n",
    "        elif PLOT_HGTS[1]==300:\n",
    "            levs=np.arange(8000,10000,40)\n",
    "        elif PLOT_HGTS[1]==250:\n",
    "            levs=np.arange(10000,13000,40)\n",
    "        elif PLOT_HGTS[1]==200:\n",
    "            levs=np.arange(10000,13000,40)\n",
    "        else:\n",
    "            levs=np.arange(1000,13000,40)\n",
    "        C42 = AX.contour(LONS ,LATS, z_prs_lvl.values,transform=crs.PlateCarree(),levels=levs,colors=PLOT_HGTS[2],linestyle='-',linewidth=0.4)\n",
    "        plt.clabel(C42, inline=1, fontsize=10, fmt=\"%i\")\n",
    "        \n",
    "    if PLOT_HGTS1[0]:\n",
    "        print('plotting additional heights for ',PLOT_HGTS1[1],' hPa')\n",
    "        p = getvar(nc_file, \"pressure\")\n",
    "        z = getvar(nc_file, \"z\", units=\"m\")\n",
    "        z_prs_lvl = interplevel(z, p, PLOT_HGTS1[1])\n",
    "        #print(z_prs_lvl)\n",
    "        if PLOT_HGTS1[1]==850:\n",
    "            levs=np.arange(1000,3500,20)\n",
    "        elif PLOT_HGTS1[1]==700:\n",
    "            levs=np.arange(2000,4500,40)\n",
    "        elif PLOT_HGTS1[1]==500:\n",
    "            levs=np.arange(5000,6000,40)\n",
    "        elif PLOT_HGTS1[1]==300:\n",
    "            levs=np.arange(8000,10000,40)\n",
    "        elif PLOT_HGTS1[1]==250:\n",
    "            levs=np.arange(10000,13000,40)\n",
    "        elif PLOT_HGTS1[1]==200:\n",
    "            levs=np.arange(10000,13000,40)\n",
    "        else:\n",
    "            levs=np.arange(1000,13000,40)\n",
    "        C422 = AX.contour(LONS ,LATS, z_prs_lvl.values,transform=crs.PlateCarree(),levels=levs,colors=PLOT_HGTS1[2],linestyle='-',linewidth=0.4)\n",
    "        plt.clabel(C422, inline=1, fontsize=10, fmt=\"%i\")\n",
    "        \n",
    "        \n",
    "    if HATCHING[0]:\n",
    "        print('hatching '+HATCHING[1]+' values at pressure level ',HATCHING[2],' hPa')\n",
    "        p = getvar(nc_file, \"pressure\")\n",
    "        hatch_var = getvar(nc_file, HATCHING[1])\n",
    "        hatch_var_prs_lvl = interplevel(hatch_var, p, HATCHING[2]).values\n",
    "        C423 = AX.contourf(LONS ,LATS, hatch_var_prs_lvl, levels=np.arange(50,90,10), transform=crs.PlateCarree(),colors=HATCHING[3],alpha=0.09)#,hatches=['', '..'],  alpha=0.25)\n",
    "        C4231 = AX.contour(LONS ,LATS, hatch_var_prs_lvl, levels=np.arange(50,90,10), transform=crs.PlateCarree(),colors=HATCHING[3],alpha=0.35,linewidths=1.8)\n",
    "\n",
    "        #plt.clabel(C423, inline=1, fontsize=10, fmt=\"%i\")\n",
    "    \n",
    "    if PLOT_LINE:\n",
    "        AX.plot([PT1[1],PT2[1]],[PT1[0],PT2[0]],transform=crs.PlateCarree(),color='black')\n",
    "        \n",
    "    #AX.add_patch(mpatches.Rectangle(xy=[-90, -50], width=50, height=30,\n",
    "    #                                facecolor='blue',\n",
    "    #                                alpha=0.2,\n",
    "    #                                transform=crs.PlateCarree()))\n",
    "\n",
    "    # Add the gridlines\n",
    "    gl = AX.gridlines()#color=\"gray\",alpha=0.5, linestyle='--',draw_labels=True,linewidth=2)\n",
    "    AX.coastlines(resolution='110m')\n",
    "    gl.xlines = True\n",
    "    gl.ylines = True\n",
    "    if LATLON_LABELS:\n",
    "        print('LATLON labels are on')\n",
    "        gl.xlabels_top = True\n",
    "        gl.ylabels_right = False\n",
    "        gl.ylabels_left = True\n",
    "        gl.ylabels_bottom = True\n",
    "    else:\n",
    "        gl.xlabels_top = False\n",
    "        gl.ylabels_right = False\n",
    "        gl.ylabels_left = False\n",
    "        gl.ylabels_bottom = True\n",
    "    #gl.xlines = False\n",
    "    #gl.xlocator = mticker.FixedLocator([-67, -66, 0, 45, 180])\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    gl.ylabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    #MP_PHYSICS=ncfile.MP_PHYSICS\n",
    "    #RA_LW_PHYSICS=ncfile.RA_LW_PHYSICS\n",
    "    #RA_SW_PHYSICS=ncfile.RA_SW_PHYSICS\n",
    "    #SF_SFCLAY_PHYSICS=ncfile.SF_SFCLAY_PHYSICS\n",
    "    #SF_SURFACE_PHYSICS=ncfile.SF_SURFACE_PHYSICS\n",
    "    #BL_PBL_PHYSICS=ncfile.BL_PBL_PHYSICS\n",
    "    if SUPTITLE:\n",
    "        title_string = title_string + ' (' + SUPTITLE + ')'\n",
    "        #plt.suptitle(SUPTITLE, x=0.67,y=0.84, fontsize=8)\n",
    "    \n",
    "    AX.set_title(title_string + ' ' + tiempo, fontsize=16)\n",
    "    \n",
    "    #AX.text(x, y, s, bbox=dict(facecolor='red', alpha=0.5))\n",
    "    #AX.text(x, y, s, bbox=dict(facecolor='red', alpha=0.5))\n",
    "    if PANEL_TEXT:\n",
    "        AX.annotate(PANEL_TEXT, xy=(0.04, 0.96), fontsize=12,\n",
    "            xycoords='axes fraction', textcoords='offset points',\n",
    "            bbox=dict(facecolor='white', alpha=0.9, boxstyle='round'),\n",
    "            horizontalalignment='left', verticalalignment='top') #xytext=(-15, -15)\n",
    "    #else:\n",
    "    #    suptitle='MP:'+wrf_mp_physics[MP_PHYSICS]+'\\n'+'SFC:'+wrf_sfc_physics[SF_SURFACE_PHYSICS]+'\\n'+'Init. '+SIMULATION_START_DATE\n",
    "   \n",
    "    AX.set_extent([-85, -35, -50, -10], crs=crs.PlateCarree())\n",
    "    if DOMAIN=='d02':\n",
    "        AX.set_extent([-75, -50, -42, -20], crs=crs.PlateCarree())\n",
    "    #if DOMAIN=='d01':\n",
    "    #    AX.set_extent([-55, -30, -55, -10], crs=crs.PlateCarree())\n",
    "    #AX.set_extent([-80, -45, -45, -10], crs=crs.PlateCarree())\n",
    "    #AX.set_extent([-70, -55, -40, -20], crs=crs.PlateCarree())\n",
    "    nc_file.close()\n",
    "    print('------- X -------')\n",
    "    \n",
    "    cbar = fig.colorbar(C1,  orientation = 'horizontal',fraction=0.09,shrink=0.6,aspect=30,pad=0.02) #cax=cb_ax,\n",
    "    #cbar.formatter.set_powerlimits((0,0))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename= (RUN + '_' + domain + '_' + VARNAME + '_' + VARTYPE + '_' + str(LEVEL) +'_' + tiempo + '.png').replace(\":\", \"-\")\n",
    "    print(filename)   \n",
    "    plt.savefig(filename,dpi=150,bbox_inches='tight')\n",
    "    # plt.close()\n",
    "    #return C1, tiempo\n",
    "\n",
    "combo ='surface'\n",
    "domain='d02'\n",
    "#'LHT_exps_two_lh_on_'\n",
    "\n",
    "if domain=='d01':\n",
    "    grid_spacing = 9000.\n",
    "    winds_thin=15\n",
    "elif domain=='d02':\n",
    "    grid_spacing=3000.\n",
    "    winds_thin=25\n",
    "else:\n",
    "    print('provide correct value of grid spacing!!!')\n",
    "\n",
    "if combo == 'surface':\n",
    "    main_variable            = 'eth'\n",
    "    lev_min                  = 285 # have to provide this\n",
    "    lev_max                  = 381\n",
    "    lev_spacing              = 1\n",
    "    main_var_level           = 0\n",
    "    main_var_level_type      = 'model'\n",
    "    panel_plot               = False\n",
    "    save_png                 = True\n",
    "    variable_and_level       = main_variable + '_' + main_var_level_type + '_' + str(main_var_level)\n",
    "    common_cmap              = cma4\n",
    "    secondary_variable_plot  = True\n",
    "    secondary_variable_name  = 'slp'\n",
    "    secondary_variable_leveltype = '2D'\n",
    "    secondary_variable_level = ''\n",
    "    secondary_variable_min   = 980\n",
    "    secondary_variable_step  = 3\n",
    "    secondary_variable_max   = 1010\n",
    "    secondary_variable_cmap_or_color = 'k'\n",
    "    secondary_variable_lw    = 1.6\n",
    "    secondary_variable_clabel= True \n",
    "    secondary_variable_smoothing = True \n",
    "    plot_winds               = True\n",
    "    winds_type               = 'regular'\n",
    "    winds_level_type         = 'sfc'\n",
    "    winds_level              = ''\n",
    "    plot_refl                = True\n",
    "    refl_threshold           = 30\n",
    "    refl_cont_color          = 'magenta'\n",
    "    plot_uh                  = False\n",
    "    plot_rain                = False\n",
    "    plot_rainrate            = False\n",
    "    plot_mslp                = False\n",
    "    plot_hgts1               = False\n",
    "    plot_hgts1_level         = False\n",
    "    plot_hgts1_color         = False\n",
    "    plot_hgts2               = False\n",
    "    plot_hgts2_level         = False\n",
    "    plot_hgts2_color         = False  \n",
    "    hatching                 = False\n",
    "    hatching_variable        = 'wspd'\n",
    "    hatching_var_level       = 300\n",
    "    hatching_var_cont_color  ='green'\n",
    "    \n",
    "elif combo == '250hpa':\n",
    "    main_variable            = 'wspd'\n",
    "    lev_min                  = 35 # have to provide this\n",
    "    lev_max                  = 81\n",
    "    lev_spacing              = 1\n",
    "    main_var_level           = 250\n",
    "    main_var_level_type      = 'pressure'\n",
    "    main_var_smooth          = False\n",
    "    panel_plot               = False\n",
    "    save_png                 = False\n",
    "    variable_and_level       = main_variable + '_' + main_var_level_type + '_' + str(main_var_level)\n",
    "    common_cmap              = cma4\n",
    "    secondary_variable_plot  = True\n",
    "    secondary_variable_name  = 'z'\n",
    "    secondary_variable_leveltype='pressure'\n",
    "    secondary_variable_level = 250\n",
    "    secondary_variable_min   = 8000\n",
    "    secondary_variable_step  = 40\n",
    "    secondary_variable_max   = 16000\n",
    "    secondary_variable_cmap_or_color = 'k'\n",
    "    secondary_variable_lw    = 1.6\n",
    "    secondary_variable_clabel= True\n",
    "    secondary_variable_smoothing=False\n",
    "    plot_winds               = True\n",
    "    winds_type               = 'regular'\n",
    "    winds_level_type         = 'p_level'\n",
    "    winds_level              = 250\n",
    "    plot_refl                = True\n",
    "    refl_threshold           = 30\n",
    "    refl_cont_color          = 'magenta'\n",
    "    plot_uh                  = False\n",
    "    plot_rain                = False\n",
    "    plot_rainrate            = False\n",
    "    plot_mslp                = False\n",
    "    plot_hgts1               = False\n",
    "    plot_hgts1_level         = False\n",
    "    plot_hgts1_color         = False\n",
    "    plot_hgts2               = False\n",
    "    plot_hgts2_level         = False\n",
    "    plot_hgts2_color         = False  \n",
    "    hatching                 = False\n",
    "    hatching_variable        = 'wspd'\n",
    "    hatching_var_level       = 250\n",
    "    hatching_var_cont_color  = 'green'\n",
    "    \n",
    "elif combo == '200hpa':\n",
    "    main_variable            = 'wspd'\n",
    "    lev_min                  = 35 # have to provide this\n",
    "    lev_max                  = 81\n",
    "    lev_spacing              = 1\n",
    "    main_var_level           = 200\n",
    "    main_var_level_type      = 'pressure'\n",
    "    panel_plot               = False\n",
    "    save_png                 = False\n",
    "    variable_and_level       = main_variable + '_' + main_var_level_type + '_' + str(main_var_level)\n",
    "    common_cmap              = cma4\n",
    "    secondary_variable_plot  = True\n",
    "    secondary_variable_name  = 'z'\n",
    "    secondary_variable_leveltype='pressure'\n",
    "    secondary_variable_level = 200\n",
    "    secondary_variable_min   = 8000\n",
    "    secondary_variable_step  = 40\n",
    "    secondary_variable_max   = 18000\n",
    "    secondary_variable_cmap_or_color = 'k'\n",
    "    secondary_variable_lw    = 1.6\n",
    "    secondary_variable_clabel= True\n",
    "    secondary_variable_smoothing=False\n",
    "    plot_winds               = True\n",
    "    winds_type               = 'regular'\n",
    "    winds_level_type         = 'p_level'\n",
    "    winds_level              = 200\n",
    "    plot_refl                = True\n",
    "    refl_threshold           = 30\n",
    "    refl_cont_color          = 'magenta'\n",
    "    plot_uh                  = False\n",
    "    plot_rain                = False\n",
    "    plot_rainrate            = False\n",
    "    plot_mslp                = False\n",
    "    plot_hgts1               = False\n",
    "    plot_hgts1_level         = False\n",
    "    plot_hgts1_color         = False\n",
    "    plot_hgts2               = False\n",
    "    plot_hgts2_level         = False\n",
    "    plot_hgts2_color         = False  \n",
    "    hatching                 = False\n",
    "    hatching_variable        = 'wspd'\n",
    "    hatching_var_level       = 200\n",
    "    hatching_var_cont_color  = 'green'\n",
    "    \n",
    "elif combo=='850hpa':\n",
    "    main_variable='wspd'\n",
    "    lev_min                  = 8 # have to provide this\n",
    "    lev_max                  = 31\n",
    "    lev_spacing              = 1\n",
    "    main_var_level           = 850\n",
    "    main_var_level_type      = 'pressure'\n",
    "    main_var_smooth          = False\n",
    "    panel_plot               = False\n",
    "    save_png                 = False\n",
    "    experiment_name          = 'LT'#'LHT_exps_two_lh_on_'\n",
    "    run_name                 = 'latent_heat_on_exps'\n",
    "    variable_and_level       = main_variable + '_' + main_var_level_type + '_' + str(main_var_level)\n",
    "    common_cmap              = cma4\n",
    "    secondary_variable_plot  = True\n",
    "    secondary_variable_name  = 'z'\n",
    "    secondary_variable_leveltype='pressure'\n",
    "    secondary_variable_level = 850\n",
    "    secondary_variable_min   = 100\n",
    "    secondary_variable_step  = 20\n",
    "    secondary_variable_max   = 15000\n",
    "    secondary_variable_cmap_or_color='k'\n",
    "    secondary_variable_lw    = 1.3\n",
    "    secondary_variable_clabel= True\n",
    "    secondary_variable_smoothing = True\n",
    "    plot_winds               = True\n",
    "    winds_type               = 'regular'\n",
    "    winds_level_type         = 'p_level'\n",
    "    winds_level              = 850\n",
    "    plot_refl                = True\n",
    "    refl_threshold           = 30\n",
    "    refl_cont_color          = 'red'\n",
    "    plot_uh                  = False\n",
    "    plot_rain                = False\n",
    "    plot_rainrate            = False\n",
    "    plot_mslp                = False\n",
    "    plot_hgts1               = False\n",
    "    plot_hgts1_level         = False\n",
    "    plot_hgts1_color         = False\n",
    "    plot_hgts2               = False\n",
    "    plot_hgts2_level         = False\n",
    "    plot_hgts2_color         = False  \n",
    "    hatching                 = True\n",
    "    hatching_variable        = 'wspd'\n",
    "    hatching_var_level       = 250\n",
    "    hatching_var_cont_color  = 'magenta'\n",
    "    \n",
    "elif combo == 'pv':\n",
    "    main_variable            = 'pvo'\n",
    "    lev_min                  = -9 # have to provide this\n",
    "    lev_max                  = 9.1\n",
    "    lev_spacing              = .1\n",
    "    main_var_level           = 250\n",
    "    main_var_level_type      = 'pressure'\n",
    "    main_var_smooth          = True\n",
    "    panel_plot               = False\n",
    "    variable_and_level       = main_variable + '_' + main_var_level_type + '_' + str(main_var_level)\n",
    "    common_cmap              = cma1\n",
    "    secondary_variable_plot  = True\n",
    "    secondary_variable_name  = 'z'\n",
    "    secondary_variable_leveltype='pressure'\n",
    "    secondary_variable_level = 250\n",
    "    secondary_variable_min   = 8000\n",
    "    secondary_variable_step  = 40\n",
    "    secondary_variable_max   = 16000\n",
    "    secondary_variable_cmap_or_color = 'k'\n",
    "    secondary_variable_lw    = 1.6\n",
    "    secondary_variable_clabel= True\n",
    "    secondary_variable_smoothing=False\n",
    "    plot_winds               = True\n",
    "    winds_type               = 'regular'\n",
    "    winds_level_type         = 'p_level'\n",
    "    winds_level              = 250\n",
    "    plot_refl                = True\n",
    "    refl_threshold           = 30\n",
    "    refl_cont_color          = 'magenta'\n",
    "    plot_uh                  = False\n",
    "    plot_rain                = False\n",
    "    plot_rainrate            = False\n",
    "    plot_mslp                = False\n",
    "    plot_hgts1               = False\n",
    "    plot_hgts1_level         = False\n",
    "    plot_hgts1_color         = False\n",
    "    plot_hgts2               = False\n",
    "    plot_hgts2_level         = False\n",
    "    plot_hgts2_color         = False  \n",
    "    hatching                 = True\n",
    "    hatching_variable        = 'wspd'\n",
    "    hatching_var_level       = 250\n",
    "    hatching_var_cont_color  = 'green'\n",
    "      \n",
    "elif combo == 'hdiabatic_pv':\n",
    "    print('###^^^^^###')\n",
    "    main_variable            = 'vert_avgd_H_DIABATIC'\n",
    "    lev_min                  = -4 # have to provide this\n",
    "    lev_max                  = 4.1\n",
    "    lev_spacing              = .1\n",
    "    main_var_level           = None\n",
    "    main_var_level_type      = '2D'\n",
    "    main_var_smooth          = False\n",
    "    panel_plot               = False\n",
    "    save_png                 = False\n",
    "    variable_and_level       = main_variable + '_' + main_var_level_type + '_' + str(main_var_level)\n",
    "    common_cmap              = cma1\n",
    "    secondary_variable_plot  = True\n",
    "    secondary_variable_name  = 'pvo'\n",
    "    secondary_variable_leveltype='pressure'\n",
    "    secondary_variable_level = 250\n",
    "    secondary_variable_min   = -9\n",
    "    secondary_variable_step  = -2\n",
    "    secondary_variable_max   = -1\n",
    "    secondary_variable_cmap_or_color = 'k'\n",
    "    secondary_variable_lw    = 1.6\n",
    "    secondary_variable_clabel= True\n",
    "    secondary_variable_smoothing=True\n",
    "    plot_winds               = True\n",
    "    winds_type               = 'regular'\n",
    "    winds_level_type         = 'p_level'\n",
    "    winds_level              = 250\n",
    "    plot_refl                = False\n",
    "    refl_threshold           = 30\n",
    "    refl_cont_color          = 'magenta'\n",
    "    plot_uh                  = False\n",
    "    plot_rain                = False\n",
    "    plot_rainrate            = False\n",
    "    plot_mslp                = False\n",
    "    plot_hgts1               = False\n",
    "    plot_hgts1_level         = False\n",
    "    plot_hgts1_color         = False\n",
    "    plot_hgts2               = False\n",
    "    plot_hgts2_level         = False\n",
    "    plot_hgts2_color         = False  \n",
    "    hatching                 = True\n",
    "    hatching_variable        = 'wspd'\n",
    "    hatching_var_level       = 250\n",
    "    hatching_var_cont_color  = 'green'\n",
    "\n",
    "else:\n",
    "    print('no')\n",
    "    \n",
    "    \n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "directory='/glade/u/home/isingh9/scratch/WRF_4.3_run_theta_tend_template/'\n",
    "#directory = '/glade/u/home/isingh9/scratch/Zhixiao_WRF_run/'\n",
    "ncfile = Dataset(directory+'wrfout_'+domain+'_2018-12-13_12:00:00')\n",
    "#print(ncfile)\n",
    "MP_PHYSICS=ncfile.MP_PHYSICS\n",
    "RA_LW_PHYSICS=ncfile.RA_LW_PHYSICS\n",
    "RA_SW_PHYSICS=ncfile.RA_SW_PHYSICS\n",
    "SF_SFCLAY_PHYSICS=ncfile.SF_SFCLAY_PHYSICS\n",
    "SF_SURFACE_PHYSICS=ncfile.SF_SURFACE_PHYSICS\n",
    "BL_PBL_PHYSICS=ncfile.BL_PBL_PHYSICS\n",
    "SIMULATION_START_DATE=ncfile.SIMULATION_START_DATE\n",
    "DX=ncfile.DX\n",
    "DY=ncfile.DY\n",
    "\n",
    "temp = getvar(ncfile,'ua')\n",
    "terr = getvar(ncfile,'ter')\n",
    "lats, lons = latlon_coords(temp) # Get the latitude and longitude points\n",
    "#lats = to_np(lats)\n",
    "#lons = to_np(lons)\n",
    "cart_proj = get_cartopy(temp) # Get the cartopy mapping object\n",
    "ncfile.close()\n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a294af-a9b2-407a-8001-8241affcaf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory1=\"/glade/u/home/isingh9/scratch/WRF_4.3_run_theta_tend_template/LHT_ON_70_vertlevs_maxdom2/\" #LHT_ON_70_vertlevs_maxdom2/'\n",
    "#directory2='/glade/u/home/isingh9/scratch/WRF_4.3_run_theta_tend_template/LHT_OFF_70_vertlevs_maxdom2/'\n",
    "directory='/glade/u/home/isingh9/scratch/WRF_4.3_run_theta_tend_template/'\n",
    "#directory = '/glade/u/home/isingh9/scratch/Zhixiao_WRF_run/'\n",
    "file_finding_string = \"wrfout_\"+domain+\"_2018-12-13*\"\n",
    "experiment_name= '13-14dec_LHT_exps_lh_on'\n",
    "fi_list1=(sorted(glob.glob(directory+file_finding_string)))\n",
    "print(fi_list1)\n",
    "\n",
    "argument = []\n",
    "for fil in fi_list1:\n",
    "    argument = argument + [(experiment_name,domain,'','(a)',fil, grid_spacing, main_var_level_type,main_variable,main_var_level,main_var_smooth,\\\n",
    "                    lev_min,lev_max,lev_spacing,20,common_cmap,\\\n",
    "                    'small_area',lons,lats,True,\\\n",
    "                    [secondary_variable_plot,secondary_variable_name,secondary_variable_leveltype,secondary_variable_level,secondary_variable_min,secondary_variable_max,secondary_variable_step,secondary_variable_cmap_or_color,secondary_variable_lw,secondary_variable_clabel,secondary_variable_smoothing],\\\n",
    "                    plot_winds,[winds_type,winds_level_type,winds_level],[plot_refl,refl_threshold,refl_cont_color],plot_uh,plot_uh,plot_rain,plot_rainrate,plot_mslp,[plot_hgts1,plot_hgts1_level,plot_hgts1_color],[plot_hgts2,plot_hgts2_level,plot_hgts2_color],\\\n",
    "                    [hatching,hatching_variable,hatching_var_level,hatching_var_cont_color],\\\n",
    "                    winds_thin,\\\n",
    "                    False,None,None,\\\n",
    "                    terr)]\n",
    "\n",
    "#print(argument)\n",
    "print(len(argument))\n",
    "    \n",
    "\n",
    "def main(FUNCTION, ARGUMENT):\n",
    "    pool = Pool(12)\n",
    "    start_time = time.perf_counter()\n",
    "    results = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(plot_WRF_3dvar_parallel, argument)\n",
    "#print(argument)\n",
    "#era5_time_range.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d529cb",
   "metadata": {},
   "source": [
    "# PSD of RAMS/WRF data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb84cd2",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate(x, y):\n",
    "    sm = 0\n",
    "    for i in range(1, len(x)):\n",
    "        h = x[i] - x[i-1]\n",
    "        sm += h * (y[i-1] + y[i]) / 2.0\n",
    "    return sm\n",
    "\n",
    "\n",
    "def calc_avg_kinetic_energy_1D(U_1D,V_1D):\n",
    "    return np.mean(0.5*(U_1D**2 + V_1D**2))\n",
    "\n",
    "def calc_avg_kinetic_energy_2D(U_2D,V_2D):\n",
    "    return np.mean(0.5*(U_2D**2 + V_2D**2))\n",
    "\n",
    "def calc_avg_kinetic_energy_mean_removed_1D(U_1D,V_1D):\n",
    "    return np.mean(0.5*((U_1D-np.mean(U_1D))**2 + (V_1D-np.mean(V_1D))**2))\n",
    "\n",
    "def calc_avg_kinetic_energy_mean_removed_2D(U_2D,V_2D):\n",
    "    return np.mean(0.5*((U_2D-np.mean(U_2D))**2 + (V_2D-np.mean(V_2D))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24177a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas = xr.open_dataset('/Users/isingh/SVH/INCUS/sample_LES_data/DRC1.1-R/a-L-2016-12-30-112130-g3.h5',engine=\"h5netcdf\",phony_dims='sort')\n",
    "aas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87afc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft, fftfreq, fftshift\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "image=aas['UP'][50,1300,:].values\n",
    "\n",
    "N=len(image)\n",
    "print('length of sample is ',N)\n",
    "\n",
    "dx=100.0 # m \n",
    "y = image\n",
    "w = signal.windows.hann(N)\n",
    "Cw=np.mean(w*w)\n",
    "print('Hann correction factor for energy: ',1/Cw)\n",
    "yw=y*w\n",
    "ydl = signal.detrend(y,type='linear')\n",
    "ydm = signal.detrend(y,type='constant')\n",
    "ydmw=ydm*w\n",
    "ydlw=ydl*w\n",
    "\n",
    "print('\\n')\n",
    "print('----------------------------')\n",
    "print('mean of unmodified 1d input array: ',np.mean(y))\n",
    "print('mean of the unmodified-windowed 1d input array: ',np.mean(yw))\n",
    "print('mean of the detrended (linear trend) 1d input array: ',np.mean(ydl))\n",
    "print('mean of the detrended (mean removal) 1d input array: ',np.mean(ydm))\n",
    "print('mean of the detrended-windowed (linear trend) 1d input array: ',np.mean(ydlw))\n",
    "print('mean of the detrended-windowed (mean removal) 1d input array: ',np.mean(ydmw))\n",
    "print('----------------------------')\n",
    "print('\\n')\n",
    "\n",
    "f   = fft(y)\n",
    "fw  = fft(yw)\n",
    "fdl = fft(ydl)\n",
    "fdm = fft(ydm)\n",
    "fdlw = fft(ydlw)\n",
    "fdmw= fft(ydmw)\n",
    "\n",
    "\n",
    "\n",
    "wavenums = fftfreq(N, dx)[0:N//2]*2.0*np.pi\n",
    "print('----- wavenumber info ------')\n",
    "print('length of the full wavenumber array: ',np.shape(wavenums))\n",
    "print('length of the full FFT array: ',np.shape(ydwf))\n",
    "print('lowest wavenumber (non-negative): ',wavenums[0])\n",
    "print('highest positive wavenumber ',wavenums[(N//2)-1])\n",
    "print('----------------------------')\n",
    "print('\\n')\n",
    "print('----------------------------')\n",
    "print('DC component in f: ',(np.abs(f[0]))**2)\n",
    "print('DC component in fw: ',(np.abs(fw[0]))**2)\n",
    "print('DC component in fdl: ',(np.abs(fdl[0]))**2)\n",
    "print('DC component in fdm: ',(np.abs(fdm[0]))**2)\n",
    "print('DC component in fdlw: ',(np.abs(fdlw[0]))**2)\n",
    "print('DC component in fdmw: ',(np.abs(fdmw[0]))**2)\n",
    "print('----------------------------')\n",
    "print('\\n')\n",
    "print('----------------------------')\n",
    "print('DC component in e:    ',(dx/(2*np.pi*N)) *(np.abs(f[0]))**2)\n",
    "print('DC component in ew:   ',(dx/(2*np.pi*N)) *(np.abs(fw[0]))**2)\n",
    "print('DC component in edl:  ',(dx/(2*np.pi*N)) *(np.abs(fdl[0]))**2)\n",
    "print('DC component in edm:  ',(dx/(2*np.pi*N)) *(np.abs(fdm[0]))**2)\n",
    "print('DC component in edlw: ',(dx/(2*np.pi*N)) *(np.abs(fdlw[0]))**2)\n",
    "print('DC component in edmw: ',(dx/(2*np.pi*N)) *(np.abs(fdmw[0]))**2)\n",
    "print('----------------------------')\n",
    "e    = (dx/(2*np.pi*N)) * (np.abs(f   [0:N//2]))**2\n",
    "ew   = (dx/(2*np.pi*N)) * (np.abs(fw  [0:N//2]))**2\n",
    "edl  = (dx/(2*np.pi*N)) * (np.abs(fdl [0:N//2]))**2\n",
    "edm  = (dx/(2*np.pi*N)) * (np.abs(fdm [0:N//2]))**2\n",
    "edmw = (dx/(2*np.pi*N)) * (np.abs(fdmw[0:N//2]))**2\n",
    "edlw = (dx/(2*np.pi*N)) * (np.abs(fdlw[0:N//2]))**2\n",
    "#dale_durran_formula_2017[-1]=dale_durran_formula_2017[-1]/2.0\n",
    "\n",
    "\n",
    "print('-------Parsivel Theorem info --------')\n",
    "print('Mean total kinetic energy : ',np.mean(0.5*(y**2)))\n",
    "print('Mean anomaly kinetic energy : ',np.mean(0.5*((y-np.mean(y))**2)))\n",
    "print('\\n')\n",
    "print('Area under curve/Total spectral energy for e                 : ',integrate(wavenums, e))\n",
    "print('Area under curve/Total spectral energy for ew                : ',integrate(wavenums, ew))\n",
    "print('Area under curve/Total spectral energy for ew (corrected)    : ',integrate(wavenums, ew/Cw))\n",
    "print('Area under curve/Total spectral energy for edl               : ',integrate(wavenums, edl))\n",
    "print('Area under curve/Total spectral energy for edm               : ',integrate(wavenums, edm))\n",
    "print('Area under curve/Total spectral energy for edmw (uncorrected): ',integrate(wavenums, edmw))\n",
    "print('Area under curve/Total spectral energy for edlw (uncorrected): ',integrate(wavenums, edlw))\n",
    "print('Area under curve/Total spectral energy for edmw (corrected)  : ',integrate(wavenums, edmw/Cw))\n",
    "print('Area under curve/Total spectral energy for edlw (corrected)  : ',integrate(wavenums, edlw/Cw))\n",
    "print('----------------------------')\n",
    "print('\\n')\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.plot(y,label='y')\n",
    "plt.plot(yw,label='yw')\n",
    "plt.plot(ydl,label='ydl')\n",
    "plt.plot(ydm,label='ydm')\n",
    "plt.plot(ydlw,label='ydlw')\n",
    "plt.plot(ydmw,label='ydmw')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.semilogy(wavenums[0:N//2],  e    ,label='e')#, '-b')\n",
    "# plt.semilogy(wavenums[0:N//2], 10* ew   ,label='ew')#, '-r')\n",
    "# plt.semilogy(wavenums[0:N//2], 100* edl  ,label='edl')#, '-g')\n",
    "# plt.semilogy(wavenums[0:N//2], 1000* edm  ,label='edm')#, '-k')\n",
    "# plt.semilogy(wavenums[0:N//2], 10000*edlw ,label='edlw')#,'-k')\n",
    "# plt.semilogy(wavenums[0:N//2], 100000* edmw ,label='edmw')#,'-k')\n",
    "\n",
    "#plt.legend(['FFT detrended', 'FFT detrended w. window'])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029260e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency characteristics of different windows\n",
    "N_w              = 100\n",
    "hann             = signal.detrend(signal.windows.hann(N_w),type='constant')\n",
    "blackman         = signal.detrend(signal.windows.blackman(N_w),type='constant')\n",
    "tukey            = signal.detrend(signal.windows.tukey(N_w),type='constant')\n",
    "boxcar           = signal.detrend(signal.windows.boxcar(N_w),type='constant')\n",
    "blackmanharris   = signal.detrend(signal.windows.blackmanharris(N_w),type='constant')\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(8,8))\n",
    "plt.plot(hann,label='hann')\n",
    "plt.plot(blackman,label='blackman')\n",
    "plt.plot(tukey,label='tukey')\n",
    "plt.plot(boxcar,label='boxcar')\n",
    "plt.plot(blackmanharris,label='blackmanharris')\n",
    "plt.legend()\n",
    "\n",
    "fr_w       = fftfreq(N_w) [0:N_w//2]\n",
    "\n",
    "power_hann = np.abs(fft(hann)[0:N_w//2])**2\n",
    "power_blackman = np.abs(fft(blackman)[0:N_w//2])**2\n",
    "power_tukey = np.abs(fft(tukey)[0:N_w//2])**2\n",
    "power_boxcar = np.abs(fft(boxcar)[0:N_w//2])**2\n",
    "power_blackmanharris = np.abs(fft(blackmanharris)[0:N_w//2])**2\n",
    "\n",
    "print('dc component of hann is ',fft(hann)[0])\n",
    "\n",
    "fig=plt.figure(figsize=(8,8))\n",
    "plt.plot(fr_w,power_hann,label='hann')\n",
    "plt.plot(fr_w,power_blackman,label='blackman')\n",
    "plt.plot(fr_w,power_tukey,label='tukey')\n",
    "plt.plot(fr_w,power_boxcar,label='boxcar')\n",
    "plt.plot(fr_w,power_blackmanharris,label='blackmanharris')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af5fd1",
   "metadata": {},
   "source": [
    "## Use Hanning window\n",
    "1. https://journals.ametsoc.org/view/journals/apme/54/5/jamc-d-14-0047.1.xml\n",
    "<br>The paper above uses Hann window and applies a correction to the energy spectrum.\n",
    "![image-2.png](attachment:image-2.png)\n",
    "![image-3.png](attachment:image-3.png)\n",
    "![image.png](attachment:image.png)\n",
    "![image-4.png](attachment:image-4.png)\n",
    "\n",
    "2. https://download.ni.com/evaluation/pxi/Understanding%20FFTs%20and%20Windowing.pdf\n",
    "<br>The document above suggests using Hann window for most cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe25e2",
   "metadata": {},
   "source": [
    "## Final version of 1-D KE spectrum calculation from aperiodic LES output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73061ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft, fftfreq, fftshift\n",
    "from scipy import signal\n",
    "#import cmath\n",
    "#import numba as nb\n",
    "# Number of sample points\n",
    "#image=0.5*(aas['UP'][50,1300,:].values**2) + (aas['VP'][50,1300,:].values**2)\n",
    "\n",
    "rams_filename = \"/nobackup/pmarines/PROD/AUS1.1-R/G12/out/\"+\"a-A-2006-01-23-090000-g1.h5\"\n",
    "aas=xr.open_dataset(rams_filename,engine='h5netcdf', phony_dims='sort') \n",
    "print('working on file: ',rams_filename)\n",
    "\n",
    "def get_spectral_density(one_d_u_velocity,one_d_v_velocity=None,DX=100.0):\n",
    "\n",
    "    N=len(one_d_u_velocity)\n",
    "    w = signal.windows.hann(N)\n",
    "    Cw=np.mean(w*w)\n",
    "    \n",
    "    xf = fftfreq(N, DX)[0:N//2] # get positive incl 0 frequencies; \n",
    "    #zero frequency (or the DC signal) was removed by detrending, TO DO: make sure the value of fourier coeff for k=0 are small\n",
    "    if isinstance(one_d_v_velocity,np.ndarray):\n",
    "        #print('two 1D var provided')\n",
    "        detrended_u = signal.detrend(one_d_u_velocity,type='linear')\n",
    "        detrended_v = signal.detrend(one_d_v_velocity,type='linear')\n",
    "        udwf = fft(detrended_u*w)\n",
    "        vdwf = fft(detrended_v*w)\n",
    "        dale_durran_formula_2017 = (DX/(2.0*np.pi*N))*((np.abs(udwf[0:N//2]))**2 + (np.abs(vdwf[0:N//2]))**2)*(1.0/Cw)\n",
    "    else:\n",
    "        #print('only one 1D var provided')\n",
    "        detrended_u = signal.detrend(one_d_u_velocity,type='linear')\n",
    "        udwf = fft(detrended_u*w)\n",
    "        dale_durran_formula_2017 = (DX/(2.0*np.pi*N))*((np.abs(udwf[0:N//2]))**2)*(1.0/Cw)\n",
    "    #dale_durran_formula_2017[-1]=dale_durran_formula_2017[-1]/2.0\n",
    "    return xf, dale_durran_formula_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86edc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c4a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try in the meridional direction\n",
    "# try dct\n",
    "spec_den_list = []\n",
    "print(aas.phony_dim_2.values)\n",
    "#for iii in aas.phony_dim_1.values:\n",
    "for iii in aas.phony_dim_2.values:\n",
    "    print(iii)\n",
    "    #non_zero_pos_freq,one_d_Ek=get_spectral_density(aas['WP'][50,iii,:].values)\n",
    "    #non_zero_pos_freq,one_d_Ek=get_spectral_density(aas['UP'][50,iii,:].values,aas['VP'][50,iii,:].values)\n",
    "    non_zero_pos_freq,one_d_Ek=get_spectral_density(aas['UP'][50,:,iii].values,aas['VP'][50,:,iii].values)\n",
    "    spec_den_list.append(one_d_Ek)\n",
    "                                                    \n",
    "spec_den_np = np.stack(spec_den_list, axis=0)\n",
    "print(np.shape(spec_den_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c987f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.rcParams[\"font.family\"] = \"helvetica\"\n",
    "\n",
    "mean_spec_density=spec_den_np.mean(axis=0)\n",
    "\n",
    "\n",
    "def wavenumber_to_lambda(k):\n",
    "    return 2*np.pi/k\n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "wavenums=non_zero_pos_freq*2*np.pi\n",
    "\n",
    "ax1.semilogy(wavenums[1:], mean_spec_density[1:]  , '-k')\n",
    "ax1.set_xscale(\"log\") \n",
    "ax1.plot(non_zero_pos_freq[25:-120]*2*np.pi,(25*non_zero_pos_freq[25:-120]*2*np.pi)**(-5/3) , \\\n",
    "         linestyle='--',label=r'$E(k) \\propto k^{-5/3}$', linewidth=1.5, color='r')\n",
    "ax1.set_ylabel(r'KE spectral density $(m^{3} s^{-2})$')\n",
    "ax1.set_xlabel(r'Wavenumber (radian/m)')\n",
    "ax1.axvline(2*np.pi/700., color='g', alpha=0.5,label='')\n",
    "secax = ax1.secondary_xaxis('top', functions=(lambda x: 2*np.pi/x, lambda x: 2*np.pi/x))\n",
    "secax.set_xlabel(r\"Wavelength (m)\")\n",
    "\n",
    "plt.legend(loc=('upper left'))\n",
    "#plt.savefig('KE_uv_lev_50_west_to_east_1D_spectral_density_linear_detrend_then_window.png')\n",
    "#plt.savefig('KE_uv_lev_50_north_to_south_1D_spectral_density_linear_detrend_then_window.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1fd400",
   "metadata": {},
   "source": [
    "## Parsivel Theorem testing of the PSD above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate area under the curve of spectral density; \n",
    "# must match the average the kinetic energy of the domain at that level\n",
    "    \n",
    "u_2d=aas['UP'][50,:,:].values\n",
    "v_2d=aas['VP'][50,:,:].values\n",
    "\n",
    "print('Mean spectral energy: ',integrate(wavenums, mean_spec_density))\n",
    "print('-------\\n')\n",
    "print('Mean total -- DC component removed -- KE (spatial domain): ',np.mean(0.5*(u_2d**2 + v_2d**2)))\n",
    "print('Mean anomaly KE (spatial domain): ',np.mean(0.5*((u_2d-np.mean(u_2d))**2 + (v_2d-np.mean(v_2d))**2)))\n",
    "u_2d_detrended_linear = signal.detrend(u_2d,axis=0)\n",
    "v_2d_detrended_linear = signal.detrend(v_2d,axis=0)\n",
    "print('Mean linear detrended KE (spatial domain): ',0.5*np.mean(u_2d_detrended_linear**2 + v_2d_detrended_linear**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926162e6",
   "metadata": {},
   "source": [
    "## Parallel implementaion of 1-D spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53d773b",
   "metadata": {},
   "source": [
    "### WRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde96db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import glob\n",
    "import hdf5plugin\n",
    "import h5py\n",
    "import numpy as np\n",
    "import datetime \n",
    "import pandas as pd\n",
    "import csv\n",
    "#import wrf\n",
    "#from netCDF4 import Dataset, num2date\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wrf import (\n",
    "    CoordPair,\n",
    "    GeoBounds,\n",
    "    cartopy_xlim,\n",
    "    cartopy_ylim,\n",
    "    get_cartopy,\n",
    "    getvar,\n",
    "    interplevel,\n",
    "    interpline,\n",
    "    latlon_coords,\n",
    "    ll_to_xy,\n",
    "    smooth2d,\n",
    "    to_np,\n",
    "    vertcross,\n",
    "    xy_to_ll,\n",
    "    ll_to_xy_proj\n",
    ")\n",
    "\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "from scipy.fft import fft, fft2, fftfreq, fftshift\n",
    "from scipy import signal\n",
    "\n",
    "def get_time_from_WRF_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][11:30] # Grab time string from WRF file\n",
    "    pd_time = pd.to_datetime(cur_time[0:9]+' '+cur_time[11:18])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def get_spectral_density(one_d_u_velocity,one_d_v_velocity=None,DX=100.0):\n",
    "\n",
    "    N=len(one_d_u_velocity)\n",
    "    w = signal.windows.hann(N)\n",
    "    Cw=np.mean(w*w)\n",
    "    \n",
    "    xf = fftfreq(N, DX)[0:N//2] # get positive incl 0 frequencies; \n",
    "    #zero frequency (or the DC signal) was removed by detrending, TO DO: make sure the value of fourier coeff for k=0 are small\n",
    "    if isinstance(one_d_v_velocity,np.ndarray):\n",
    "        #print('two 1D var provided')\n",
    "        detrended_u = signal.detrend(one_d_u_velocity,type='linear')\n",
    "        detrended_v = signal.detrend(one_d_v_velocity,type='linear')\n",
    "        udwf = fft(detrended_u*w)\n",
    "        vdwf = fft(detrended_v*w)\n",
    "        dale_durran_formula_2017 = (DX/(2.0*np.pi*N))*((np.abs(udwf[0:N//2]))**2 + (np.abs(vdwf[0:N//2]))**2)*(1.0/Cw)\n",
    "    else:\n",
    "        #print('only one 1D var provided')\n",
    "        detrended_u = signal.detrend(one_d_u_velocity,type='linear')\n",
    "        udwf = fft(detrended_u*w)\n",
    "        dale_durran_formula_2017 = (DX/(2.0*np.pi*N))*((np.abs(udwf[0:N//2]))**2)*(1.0/Cw)\n",
    "    #dale_durran_formula_2017[-1]=dale_durran_formula_2017[-1]/2.0\n",
    "    return xf, dale_durran_formula_2017\n",
    "\n",
    "\n",
    "def get_KE_spectral_density_wes_to_east_1D_from_xr(FILENAME,MODEL_LEVEL,DX=100.0):\n",
    "\n",
    "    spectral_density_all_rows=[]\n",
    "    \n",
    "    print('working on file: ',FILENAME)\n",
    "    ncfile = Dataset(FILENAME)\n",
    "    UP = getvar(ncfile,'ua')\n",
    "    VP = getvar(ncfile,'va')\n",
    "\n",
    "    nlevs,nrows,ncols=np.shape(UP)\n",
    "    \n",
    "    for row in range(nrows):\n",
    "        print('row: ',row)\n",
    "        one_d_u_velocity=UP[MODEL_LEVEL,row,:].values\n",
    "        one_d_v_velocity=VP[MODEL_LEVEL,row,:].values\n",
    "        \n",
    "        N=len(one_d_u_velocity)\n",
    "        w = signal.windows.hann(N)\n",
    "        Cw=np.mean(w*w)\n",
    "        \n",
    "        #xf = fftfreq(N, DX)[0:N//2]*2.0*np.pi # get positive incl 0 frequencies; \n",
    "        #zero frequency (or the DC signal) was removed by detrending, TO DO: make sure the value of fourier coeff for k=0 are small\n",
    "        detrended_u = signal.detrend(one_d_u_velocity,type='linear')\n",
    "        detrended_v = signal.detrend(one_d_v_velocity,type='linear')\n",
    "        udwf = fft(detrended_u*w)\n",
    "        vdwf = fft(detrended_v*w)\n",
    "        dale_durran_formula_2017 = (DX/(2.0*np.pi*N))*((np.abs(udwf[0:N//2]))**2 + (np.abs(vdwf[0:N//2]))**2)*(1.0/Cw)\n",
    "        #dale_durran_formula_2017[-1]=dale_durran_formula_2017[-1]/2.0\n",
    "        spectral_density_all_rows.append(dale_durran_formula_2017)\n",
    "    \n",
    "    \n",
    "    return np.array(spectral_density_all_rows).mean(axis=0)\n",
    "\n",
    "domain='AUS1.1-R'\n",
    "dx=1600.0\n",
    "model_lev=60\n",
    "les_files = sorted(glob.glob('/nobackupp11/isingh2/WRF_final_for_testing/WRF/run/FIRST_RUN/wrfout_d01_2006-01-23_10:30:00'))  #g1\n",
    "print('# files/timesteps to be processed: ',len(les_files))\n",
    "print('first file is : ',les_files[0])\n",
    "ds_les_0 = Dataset(les_files[0])\n",
    "uu = getvar(ds_les_0,'ua')[model_lev,10,:]\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('getting times from all the files ...')\n",
    "start_time=get_time_from_WRF_file(les_files[0])[0]\n",
    "end_time=get_time_from_WRF_file(les_files[-1])[0]\n",
    "print('times vary from ',start_time,' to ',end_time)\n",
    "time_suffix=get_time_from_WRF_file(les_files[0])[1]+'-'+get_time_from_WRF_file(les_files[-1])[1]\n",
    "times_list=[get_time_from_WRF_file(filio)[0] for filio in les_files]\n",
    "print(times_list)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('getting wavenumbers from the first file...')\n",
    "len_uu = len(uu)\n",
    "print('length of 1D u velocity array: ',len(uu))\n",
    "wavenums = fftfreq(len_uu, dx)[0:len_uu//2]*2.0*np.pi # get positive incl 0 frequencies; \n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "print('creating arguments for passing on to multiprocessing.starmap ...')\n",
    "total_ke_density_all_timesteps=[]\n",
    "cpu_count1 = cpu_count()\n",
    "\n",
    "argument=[]\n",
    "for fil in les_files:\n",
    "    argument = argument + [(fil,model_lev,dx)]\n",
    "    #print(argument)\n",
    "\n",
    "print('# arguments: ',len(argument))\n",
    "print('first argument is ',argument[0])\n",
    "\n",
    "spec_density_list=[]\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('number of processors = ',cpu_count1)\n",
    "print('starting the multiprocessing part ...')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(cpu_count1) as p:\n",
    "        for result in p.starmap(get_KE_spectral_density_wes_to_east_1D_from_xr,argument):\n",
    "            #wavenumber_list    = result[0]\n",
    "            total_ke_density_all_timesteps += [result]\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "total_ke_density_all_timesteps = np.array(total_ke_density_all_timesteps)\n",
    "print('length of KE PSD array for all timesteps is : ',np.shape(total_ke_density_all_timesteps))\n",
    "\n",
    "\n",
    "spectral_density_text_filename=\"KE_hor_psd_WRF_west_to_east_1D_\"+domain+\"_model_lev_\"+str(model_lev)+\"_multiple_timesteps_\"+time_suffix+\".txt\"\n",
    "wavenumber_text_file=\"KE_hor_wavenums_WRF_west_to_east_1D_\"+domain+\"_model_lev_\"+str(model_lev)+\"_multiple_timesteps_\"+time_suffix+\".txt\"\n",
    "times_text_file=\"KE_hor_times_WRF_west_to_east_1D_\"+domain+\"_model_lev_\"+str(model_lev)+\"_multiple_timesteps_\"+time_suffix+\".csv\"\n",
    "\n",
    "\n",
    "print('saving PSD values to file : ',spectral_density_text_filename)\n",
    "np.savetxt(spectral_density_text_filename, total_ke_density_all_timesteps)\n",
    "print('saving wavenumber values to file : ',wavenumber_text_file)\n",
    "np.savetxt(wavenumber_text_file, wavenums)\n",
    "print('saving time values to file : ',times_text_file)\n",
    "#np.savetxt(times_text_file, np.array(times_list))\n",
    "with open(times_text_file, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(times_list)\n",
    "    \n",
    "print(\"\\n\\n FILENAMES \\n \")\n",
    "print('spectral_density_text_filename: ',spectral_density_text_filename)\n",
    "print('wavenumber_text_file: ',wavenumber_text_file)\n",
    "print('times_text_file: ',times_text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7372150",
   "metadata": {},
   "source": [
    "### RAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xarray as xr\n",
    "import glob\n",
    "import hdf5plugin\n",
    "import h5py\n",
    "import numpy as np\n",
    "import datetime \n",
    "import pandas as pd\n",
    "import csv\n",
    "#import wrf\n",
    "#from netCDF4 import Dataset, num2date\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "from scipy.fft import fft, fft2, fftfreq, fftshift\n",
    "from scipy import signal\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def get_spectral_density(one_d_u_velocity,one_d_v_velocity=None,DX=100.0):\n",
    "\n",
    "    N=len(one_d_u_velocity)\n",
    "    w = signal.windows.hann(N)\n",
    "    Cw=np.mean(w*w)\n",
    "    \n",
    "    xf = fftfreq(N, DX)[0:N//2] # get positive incl 0 frequencies; \n",
    "    #zero frequency (or the DC signal) was removed by detrending, TO DO: make sure the value of fourier coeff for k=0 are small\n",
    "    if isinstance(one_d_v_velocity,np.ndarray):\n",
    "        #print('two 1D var provided')\n",
    "        detrended_u = signal.detrend(one_d_u_velocity,type='linear')\n",
    "        detrended_v = signal.detrend(one_d_v_velocity,type='linear')\n",
    "        udwf = fft(detrended_u*w)\n",
    "        vdwf = fft(detrended_v*w)\n",
    "        dale_durran_formula_2017 = (DX/(2.0*np.pi*N))*((np.abs(udwf[0:N//2]))**2 + (np.abs(vdwf[0:N//2]))**2)*(1.0/Cw)\n",
    "    else:\n",
    "        #print('only one 1D var provided')\n",
    "        detrended_u = signal.detrend(one_d_u_velocity,type='linear')\n",
    "        udwf = fft(detrended_u*w)\n",
    "        dale_durran_formula_2017 = (DX/(2.0*np.pi*N))*((np.abs(udwf[0:N//2]))**2)*(1.0/Cw)\n",
    "    #dale_durran_formula_2017[-1]=dale_durran_formula_2017[-1]/2.0\n",
    "    return xf, dale_durran_formula_2017\n",
    "\n",
    "\n",
    "def get_KE_spectral_density_wes_to_east_1D_from_xr(FILENAME,MODEL_LEVEL,DX=100.0):\n",
    "\n",
    "    spectral_density_all_rows=[]\n",
    "    \n",
    "    print('working on file: ',FILENAME)\n",
    "    ds_les = xr.open_dataset(FILENAME,engine=\"h5netcdf\",phony_dims='sort')\n",
    "\n",
    "    nlevs,nrows,ncols=np.shape(ds_les.UP)\n",
    "    \n",
    "    for row in range(nrows):\n",
    "        print('row: ',row)\n",
    "\n",
    "        # vertical KE\n",
    "        #one_d_u_velocity=ds_les.WP[MODEL_LEVEL,row,:].values\n",
    "        #one_d_v_velocity=ds_les.WP[MODEL_LEVEL,row,:].values\n",
    "        \n",
    "        # horizontal KE\n",
    "        one_d_u_velocity=ds_les.UP[MODEL_LEVEL,row,:].values\n",
    "        one_d_v_velocity=ds_les.VP[MODEL_LEVEL,row,:].values\n",
    "        \n",
    "        N=len(one_d_u_velocity)\n",
    "        w = signal.windows.hann(N)\n",
    "        Cw=np.mean(w*w)\n",
    "        \n",
    "        #xf = fftfreq(N, DX)[0:N//2]*2.0*np.pi # get positive incl 0 frequencies; \n",
    "        #zero frequency (or the DC signal) was removed by detrending, TO DO: make sure the value of fourier coeff for k=0 are small\n",
    "        detrended_u = signal.detrend(one_d_u_velocity,type='linear')\n",
    "        detrended_v = signal.detrend(one_d_v_velocity,type='linear')\n",
    "        udwf = fft(detrended_u*w)\n",
    "        vdwf = fft(detrended_v*w)\n",
    "        dale_durran_formula_2017 = (DX/(2.0*np.pi*N))*((np.abs(udwf[0:N//2]))**2 + (np.abs(vdwf[0:N//2]))**2)*(1.0/Cw)\n",
    "        #dale_durran_formula_2017[-1]=dale_durran_formula_2017[-1]/2.0\n",
    "        spectral_density_all_rows.append(dale_durran_formula_2017)\n",
    "    \n",
    "    \n",
    "    return np.array(spectral_density_all_rows).mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "domain='AUS1.1-R'\n",
    "dx=1600.0\n",
    "model_lev=60\n",
    "les_files = sorted(glob.glob('/nobackup/pmarines/PROD/'+domain+'/G12/out/a-A-2006-01-23-103000-g1.h5'))  #g1\n",
    "#les_files = sorted(glob.glob('/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/a-L-2016-12-30-11[0-4]*-g3.h5'))[::2]  #g2\n",
    "#les_files = sorted(glob.glob('/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/a-L-2016-12-30-11[0-4]*-g3.h5'))[::2]  #g3\n",
    "print('# files/timesteps to be processed: ',len(les_files))\n",
    "print('first file is : ',les_files[0])\n",
    "ds_les_0 = xr.open_dataset(les_files[0],engine=\"h5netcdf\",phony_dims='sort')\n",
    "uu = ds_les_0.UP[model_lev,10,:]\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('getting times from all the files ...')\n",
    "start_time=get_time_from_RAMS_file(les_files[0])[0]\n",
    "end_time=get_time_from_RAMS_file(les_files[-1])[0]\n",
    "print('times vary from ',start_time,' to ',end_time)\n",
    "time_suffix=get_time_from_RAMS_file(les_files[0])[1]+'-'+get_time_from_RAMS_file(les_files[-1])[1]\n",
    "times_list=[get_time_from_RAMS_file(filio)[0] for filio in les_files]\n",
    "print(times_list)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('getting wavenumbers from the first file...')\n",
    "len_uu = len(uu)\n",
    "print('length of 1D u velocity array: ',len(uu))\n",
    "wavenums = fftfreq(len_uu, dx)[0:len_uu//2]*2.0*np.pi # get positive incl 0 frequencies; \n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "print('creating arguments for passing on to multiprocessing.starmap ...')\n",
    "total_ke_density_all_timesteps=[]\n",
    "cpu_count1 = cpu_count()\n",
    "\n",
    "argument=[]\n",
    "for fil in les_files:\n",
    "    argument = argument + [(fil,model_lev,dx)]\n",
    "    #print(argument)\n",
    "\n",
    "print('# arguments: ',len(argument))\n",
    "print('first argument is ',argument[0])\n",
    "\n",
    "\n",
    "spec_density_list=[]\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('number of processors = ',cpu_count1)\n",
    "print('starting the multiprocessing part ...')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(cpu_count1) as p:\n",
    "        for result in p.starmap(get_KE_spectral_density_wes_to_east_1D_from_xr,argument):\n",
    "            #wavenumber_list    = result[0]\n",
    "            total_ke_density_all_timesteps += [result]\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "\n",
    "# plotting\n",
    "\n",
    "total_ke_density_all_timesteps = np.array(total_ke_density_all_timesteps)\n",
    "print('length of KE PSD array for all timesteps is : ',np.shape(total_ke_density_all_timesteps))\n",
    "\n",
    "\n",
    "spectral_density_text_filename=\"KE_hor_psd_RAMS_west_to_east_1D_\"+domain+\"_model_lev_\"+str(model_lev)+\"_multiple_timesteps_\"+time_suffix+\".txt\"\n",
    "wavenumber_text_file=\"KE_hor_wavenums_RAMS_west_to_east_1D_\"+domain+\"_model_lev_\"+str(model_lev)+\"_multiple_timesteps_\"+time_suffix+\".txt\"\n",
    "times_text_file=\"KE_hor_times_RAMS_west_to_east_1D_\"+domain+\"_model_lev_\"+str(model_lev)+\"multiple_timesteps_\"+time_suffix+\".csv\"\n",
    "\n",
    "\n",
    "print('saving PSD values to file : ',spectral_density_text_filename)\n",
    "np.savetxt(spectral_density_text_filename, total_ke_density_all_timesteps)\n",
    "print('saving wavenumber values to file : ',wavenumber_text_file)\n",
    "np.savetxt(wavenumber_text_file, wavenums)\n",
    "print('saving time values to file : ',times_text_file)\n",
    "#np.savetxt(times_text_file, np.array(times_list))\n",
    "with open(times_text_file, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(times_list)\n",
    "    \n",
    "print(\"\\n\\n FILENAMES \\n \")\n",
    "print('spectral_density_text_filename: ',spectral_density_text_filename)\n",
    "print('wavenumber_text_file: ',wavenumber_text_file)\n",
    "print('times_text_file: ',times_text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d59e4",
   "metadata": {},
   "source": [
    "### plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 18\n",
    "matplotlib.rcParams['axes.titlesize'] = 18\n",
    "matplotlib.rcParams['xtick.labelsize'] = 17\n",
    "matplotlib.rcParams['ytick.labelsize'] = 17\n",
    "matplotlib.rcParams['legend.fontsize'] = 18\n",
    "matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "matplotlib.rcParams['hatch.linewidth'] = 0.25\n",
    "matplotlib.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "lev_num = 60\n",
    "\n",
    "## RAMS\n",
    "wavenums_rams=np.loadtxt('KE_hor_wavenums_RAMS_west_to_east_1D_AUS1.1-R_model_lev_'+str(lev_num)+'_multiple_timesteps_20060123103000-20060123103000.txt')\n",
    "mean_spec_density_rams=np.loadtxt('KE_hor_psd_RAMS_west_to_east_1D_AUS1.1-R_model_lev_'+str(lev_num)+'_multiple_timesteps_20060123103000-20060123103000.txt')\n",
    "\n",
    "## WRF\n",
    "wavenums_wrf=np.loadtxt('KE_hor_wavenums_WRF_west_to_east_1D_AUS1.1-R_model_lev_'+str(lev_num)+'_multiple_timesteps_20060102103000-20060102103000.txt')\n",
    "mean_spec_density_wrf=np.loadtxt('KE_hor_psd_WRF_west_to_east_1D_AUS1.1-R_model_lev_'+str(lev_num)+'_multiple_timesteps_20060102103000-20060102103000.txt')\n",
    "\n",
    "ax1.semilogy(wavenums_rams[1:], mean_spec_density_rams[1:]  , '-k', label='RAMS')\n",
    "ax1.semilogy(wavenums_wrf[1:], mean_spec_density_wrf[1:]  , '-g', label = 'WRF')\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.plot(wavenums_rams[25:-120],(25*wavenums_rams[25:-120])**(-5/3) , linestyle='--',label=r'$E(k) \\propto k^{-5/3}$', linewidth=1.5, color='r')\n",
    "ax1.set_ylabel(r'KE spectral density $(m^{3} s^{-2})$')\n",
    "ax1.set_xlabel(r'Wavenumber (radian/m)')\n",
    "ax1.axvline(2*np.pi/11200., color='g', alpha=0.5,label='')\n",
    "secax = ax1.secondary_xaxis('top', functions=(lambda x: 2*np.pi/x, lambda x: 2*np.pi/x))\n",
    "secax.set_xlabel(r\"Wavelength (m)\")\n",
    "\n",
    "plt.legend(loc=('upper left'))\n",
    "plt.title('KE (w) spectrum (1D, west to east) for WRF and RAMS at  model level '+str(lev_num))\n",
    "plt.savefig('KE_uv_lev_'+str(lev_num)+'_west_to_east_1D_spectral_density_linear_detrend_then_window_WRF_RAMS.png',dpi=200)\n",
    "#plt.savefig('KE_uv_lev_'+str(lev_num)+'_west_to_east_1D_spectral_density_linear_detrend_then_window.png')\n",
    "print('-----\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46284da",
   "metadata": {},
   "source": [
    "### Functions for power spectrum comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a4967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import glob\n",
    "import hdf5plugin\n",
    "import h5py\n",
    "import numpy as np\n",
    "import datetime \n",
    "import pandas as pd\n",
    "import csv\n",
    "#import wrf\n",
    "#from netCDF4 import Dataset, num2date\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.fft import fft, fft2, fftfreq, fftshift\n",
    "from scipy import signal\n",
    "\n",
    "from wrf import (\n",
    "    CoordPair,\n",
    "    GeoBounds,\n",
    "    cartopy_xlim,\n",
    "    cartopy_ylim,\n",
    "    get_cartopy,\n",
    "    getvar,\n",
    "    interplevel,\n",
    "    interpline,\n",
    "    latlon_coords,\n",
    "    ll_to_xy,\n",
    "    smooth2d,\n",
    "    to_np,\n",
    "    vertcross,\n",
    "    xy_to_ll,\n",
    "    ll_to_xy_proj\n",
    ")\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import time\n",
    "\n",
    "def get_time_from_WRF_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][11:30] # Grab time string from WRF file\n",
    "    pd_time = pd.to_datetime(cur_time[0:9]+' '+cur_time[11:18])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def get_spectral_density(one_d_u_velocity,one_d_v_velocity=None,DX=100.0):\n",
    "    N=len(one_d_u_velocity)\n",
    "    w = signal.windows.hann(N)\n",
    "    Cw=np.mean(w*w)\n",
    "    \n",
    "    xf = fftfreq(N, DX)[0:N//2] # get positive incl 0 frequencies; \n",
    "    #zero frequency (or the DC signal) was removed by detrending, TO DO: make sure the value of fourier coeff for k=0 are small\n",
    "    if isinstance(one_d_v_velocity,np.ndarray):\n",
    "        #print('two 1D var provided')\n",
    "        detrended_u = signal.detrend(one_d_u_velocity,type='linear')\n",
    "        detrended_v = signal.detrend(one_d_v_velocity,type='linear')\n",
    "        udwf = fft(detrended_u*w)\n",
    "        vdwf = fft(detrended_v*w)\n",
    "        dale_durran_formula_2017 = (DX/(2.0*np.pi*N))*((np.abs(udwf[0:N//2]))**2 + (np.abs(vdwf[0:N//2]))**2)*(1.0/Cw)\n",
    "    else:\n",
    "        #print('only one 1D var provided')\n",
    "        detrended_u = signal.detrend(one_d_u_velocity,type='linear')\n",
    "        udwf = fft(detrended_u*w)\n",
    "        dale_durran_formula_2017 = (DX/(2.0*np.pi*N))*((np.abs(udwf[0:N//2]))**2)*(1.0/Cw)\n",
    "    #dale_durran_formula_2017[-1]=dale_durran_formula_2017[-1]/2.0\n",
    "    return xf, dale_durran_formula_2017\n",
    "\n",
    "def get_KE_spectral_density_wes_to_east_1D_from_xr(FILENAME,MODEL_LEVEL,DX=100.0):\n",
    "    spectral_density_all_rows=[]\n",
    "    print('working on file: ',FILENAME)\n",
    "    ncfile = Dataset(FILENAME)\n",
    "    UP = getvar(ncfile,'ua')\n",
    "    VP = getvar(ncfile,'va')\n",
    "\n",
    "    nlevs,nrows,ncols=np.shape(UP)\n",
    "    \n",
    "    for row in range(nrows):\n",
    "        print('row: ',row)\n",
    "        one_d_u_velocity=UP[MODEL_LEVEL,row,:].values\n",
    "        one_d_v_velocity=VP[MODEL_LEVEL,row,:].values\n",
    "        \n",
    "        N=len(one_d_u_velocity)\n",
    "        w = signal.windows.hann(N)\n",
    "        Cw=np.mean(w*w)\n",
    "        \n",
    "        #xf = fftfreq(N, DX)[0:N//2]*2.0*np.pi # get positive incl 0 frequencies; \n",
    "        #zero frequency (or the DC signal) was removed by detrending, TO DO: make sure the value of fourier coeff for k=0 are small\n",
    "        detrended_u = signal.detrend(one_d_u_velocity,type='linear')\n",
    "        detrended_v = signal.detrend(one_d_v_velocity,type='linear')\n",
    "        udwf = fft(detrended_u*w)\n",
    "        vdwf = fft(detrended_v*w)\n",
    "        dale_durran_formula_2017 = (DX/(2.0*np.pi*N))*((np.abs(udwf[0:N//2]))**2 + (np.abs(vdwf[0:N//2]))**2)*(1.0/Cw)\n",
    "        #dale_durran_formula_2017[-1]=dale_durran_formula_2017[-1]/2.0\n",
    "        spectral_density_all_rows.append(dale_durran_formula_2017)\n",
    "    \n",
    "    \n",
    "    return np.array(spectral_density_all_rows).mean(axis=0)\n",
    "\n",
    "\n",
    "def compare_KE_spectrum(FILENAME,MODEL_FLAG):\n",
    "domain='AUS1.1-R'\n",
    "dx=1600.0\n",
    "model_lev=60\n",
    "les_files = sorted(glob.glob('/nobackupp11/isingh2/WRF_final_for_testing/WRF/run/FIRST_RUN/wrfout_d01_2006-01-23_10:30:00'))  #g1\n",
    "print('# files/timesteps to be processed: ',len(les_files))\n",
    "print('first file is : ',les_files[0])\n",
    "ds_les_0 = Dataset(les_files[0])\n",
    "uu = getvar(ds_les_0,'ua')[model_lev,10,:]\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('getting times from all the files ...')\n",
    "start_time=get_time_from_WRF_file(les_files[0])[0]\n",
    "end_time=get_time_from_WRF_file(les_files[-1])[0]\n",
    "print('times vary from ',start_time,' to ',end_time)\n",
    "time_suffix=get_time_from_WRF_file(les_files[0])[1]+'-'+get_time_from_WRF_file(les_files[-1])[1]\n",
    "times_list=[get_time_from_WRF_file(filio)[0] for filio in les_files]\n",
    "print(times_list)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('getting wavenumbers from the first file...')\n",
    "len_uu = len(uu)\n",
    "print('length of 1D u velocity array: ',len(uu))\n",
    "wavenums = fftfreq(len_uu, dx)[0:len_uu//2]*2.0*np.pi # get positive incl 0 frequencies; \n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "print('creating arguments for passing on to multiprocessing.starmap ...')\n",
    "total_ke_density_all_timesteps=[]\n",
    "cpu_count1 = cpu_count()\n",
    "\n",
    "argument=[]\n",
    "for fil in les_files:\n",
    "    argument = argument + [(fil,model_lev,dx)]\n",
    "    #print(argument)\n",
    "\n",
    "print('# arguments: ',len(argument))\n",
    "print('first argument is ',argument[0])\n",
    "\n",
    "\n",
    "spec_density_list=[]\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('number of processors = ',cpu_count1)\n",
    "print('starting the multiprocessing part ...')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(cpu_count1) as p:\n",
    "        for result in p.starmap(get_KE_spectral_density_wes_to_east_1D_from_xr,argument):\n",
    "            #wavenumber_list    = result[0]\n",
    "            total_ke_density_all_timesteps += [result]\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "\n",
    "# plotting\n",
    "\n",
    "total_ke_density_all_timesteps = np.array(total_ke_density_all_timesteps)\n",
    "print('length of KE PSD array for all timesteps is : ',np.shape(total_ke_density_all_timesteps))\n",
    "\n",
    "\n",
    "spectral_density_text_filename=\"KE_hor_psd_WRF_west_to_east_1D_\"+domain+\"_model_lev_\"+str(model_lev)+\"_multiple_timesteps_\"+time_suffix+\".txt\"\n",
    "wavenumber_text_file=\"KE_hor_wavenums_WRF_west_to_east_1D_\"+domain+\"_model_lev_\"+str(model_lev)+\"_multiple_timesteps_\"+time_suffix+\".txt\"\n",
    "times_text_file=\"KE_hor_times_WRF_west_to_east_1D_\"+domain+\"_model_lev_\"+str(model_lev)+\"_multiple_timesteps_\"+time_suffix+\".csv\"\n",
    "\n",
    "\n",
    "print('saving PSD values to file : ',spectral_density_text_filename)\n",
    "np.savetxt(spectral_density_text_filename, total_ke_density_all_timesteps)\n",
    "print('saving wavenumber values to file : ',wavenumber_text_file)\n",
    "np.savetxt(wavenumber_text_file, wavenums)\n",
    "print('saving time values to file : ',times_text_file)\n",
    "#np.savetxt(times_text_file, np.array(times_list))\n",
    "with open(times_text_file, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(times_list)\n",
    "    \n",
    "print(\"\\n\\n FILENAMES \\n \")\n",
    "print('spectral_density_text_filename: ',spectral_density_text_filename)\n",
    "print('wavenumber_text_file: ',wavenumber_text_file)\n",
    "print('times_text_file: ',times_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat  ../get_KE_psd_WRF_parallel.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24022247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "def plotly_colorbar2(ZMIN,ZMAX,n = 8):\n",
    "  \n",
    "    print('tickvals = ',np.linspace(np.log10(ZMIN), np.log10(ZMAX), n))\n",
    "    print('ticktext = ',10 ** np.linspace(np.log10(ZMIN), np.log10(ZMAX), n))\n",
    "    return dict(\n",
    "        title = \"KE spectral density\",\n",
    "        tickmode = \"array\",\n",
    "        dtick=1,\n",
    "        tickvals = np.linspace(np.log10(ZMIN), np.log10(ZMAX), n),\n",
    "        ticktext = 10 ** np.linspace(np.log10(ZMIN),np.log10(ZMAX), n))#,\n",
    "        #tickformat= '4e')\n",
    "\n",
    "\n",
    "#color list here : https://plotly.com/python/builtin-colorscales/\n",
    "zvals=np.array(total_ke_density_all_levs[1:])\n",
    "# k_5_3_line=(10*wavenums[25:-125])**(-5/3)\n",
    "# k_5_3_plane_vals=    np.repeat(k_5_3_line[np.newaxis,:], len(lev_nums), axis=0)\n",
    "# print(np.shape(k_5_3_plane_vals))\n",
    "# print(np.shape(wavenums[25:-125]))\n",
    "# print('min of 5/3 plane = ',np.min(k_5_3_plane_vals))\n",
    "# print('max of 5/3 plane = ',np.max(k_5_3_plane_vals))\n",
    "print(np.shape(zvals))\n",
    "zmin=np.min(zvals)\n",
    "zmax=np.max(zvals)\n",
    "print('zmin = ',zmin)\n",
    "print('zmax = ',zmax)\n",
    "    \n",
    "    \n",
    "fig = go.Figure(data=[go.Surface(z=zvals, y=lev_nums, x=wavenums[1:], \\\n",
    "                                 colorscale= 'Viridis', cmin=np.log10(zmin),cmax=np.log10(zmax),\\\n",
    "                                 colorbar=plotly_colorbar2(zmin,zmax,n=8))])\n",
    "#                       go.Surface(z=k_5_3_plane_vals,  y=lev_nums, x=wavenums[25:-125],opacity=0.5, colorscale='Viridis', showscale=False)]) \n",
    "#fig = go.Figure(data=[go.Surface(z=k_5_3_plane_vals,  y=lev_nums, x=wavenums[25:-125],opacity=0.5, colorscale='Viridis', showscale=False)]) \n",
    "\n",
    "fig.update_layout(title='KE spectral density with height', autosize=True,\\\n",
    "                  width=1000, height=1000,\\\n",
    "                  scene = {\n",
    "                      \"xaxis\": {\"nticks\": 5, \"title\" : \"Wavenumber (radians/m)\"},\n",
    "                      \"yaxis\": {\"nticks\": 20, \"title\" : \"Model level\"},\n",
    "                      \"zaxis\": {\"nticks\": 5,  \"title\" : \"KE spectral density (m<sup>2</sup> s<sup>-3</sup>)\"},\n",
    "                      'camera_eye': {\"x\": 0, \"y\": -1, \"z\": 0.5},\n",
    "                      \"aspectratio\": {\"x\": 1, \"y\": 1, \"z\":1}\n",
    "                    }\n",
    "                  )\n",
    "fig.update_coloraxes(colorbar_exponentformat = 'power')\n",
    "\n",
    "fig.update_layout(scene=dict(zaxis=dict(dtick=1, type='log'),xaxis=dict(dtick=1, type='log')))\n",
    "fig.update_scenes(zaxis_exponentformat='power',xaxis_exponentformat='power')\n",
    "#show contour on top\n",
    "fig.update_traces(contours_z=dict(show=True, usecolormap=True,\n",
    "                                  highlightcolor=\"limegreen\", project_z=True))\n",
    "fig.show()\n",
    "fig.write_html(\"plotly_interactive_store_test.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3bab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Z = np.meshgrid(aas.phony_dim_1.values, aas.phony_dim_2.values,aas.phony_dim_3.values)\n",
    "\n",
    "# ellipsoid\n",
    "values = aas.WP.values\n",
    "\n",
    "fig = go.Figure(data=go.Isosurface(\n",
    "    x=X.flatten(),\n",
    "    y=Y.flatten(),\n",
    "    z=Z.flatten(),\n",
    "    value=values.flatten(),\n",
    "    isomin=20,\n",
    "    isomax=21,\n",
    "    caps=dict(x_show=False, y_show=False)\n",
    "    ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b6d4f",
   "metadata": {},
   "source": [
    "## Alternate plot of the KE spectrum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b36b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.rcParams[\"font.family\"] = \"helvetica\"\n",
    "\n",
    "mean_spec_density=spec_den_np.mean(axis=0)\n",
    "\n",
    "\n",
    "def wavenumber_to_lambda(k):\n",
    "    return 2*np.pi/k\n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "wavenums=non_zero_pos_freq*2*np.pi\n",
    "\n",
    "ax1.semilogy(wavenums, mean_spec_density*(wavenums[2]-wavenums[1])  , '-k')\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.plot(non_zero_pos_freq[25:-120]*2*np.pi,(10*non_zero_pos_freq[25:-120]*2*np.pi)**(-5/3) , \\\n",
    "         linestyle='--',label=r'$E(k) \\propto k^{-5/3}$', linewidth=1.5, color='r')\n",
    "ax1.set_ylabel(r'E(k)Δk $(m^{2} s^{-2})$')\n",
    "ax1.set_xlabel(r'Wavenumber (radian/m)')\n",
    "ax1.axvline(2*np.pi/700., color='g', alpha=0.5,label='')\n",
    "secax = ax1.secondary_xaxis('top', functions=(lambda x: 2*np.pi/x, lambda x: 2*np.pi/x))\n",
    "secax.set_xlabel(r\"Wavelength (m)\")\n",
    "\n",
    "plt.legend(loc=('upper left'))\n",
    "#plt.savefig('KE_uv_lev_50_spectral_density_linear_detrend_then_window.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220ea10",
   "metadata": {},
   "source": [
    "## 2D FFT : Dale Durran formulation (testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b17de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft, fft2, fftfreq, fftshift\n",
    "from scipy import signal\n",
    "#import cmath\n",
    "#import numba as nb\n",
    "# Number of sample points\n",
    "#image=0.5*(aas['UP'][50,1300,:].values**2) + (aas['VP'][50,1300,:].values**2)\n",
    "\n",
    "\n",
    "def get_spectral_density(twoD_field,DX=100.0,DY=100.0):\n",
    "\n",
    "    Nx=np.shape(twoD_field)[1]\n",
    "    Ny=np.shape(twoD_field)[0]\n",
    "    #w = signal.windows.hann(N)\n",
    "    \n",
    "    \n",
    "    wavenum_x = fftfreq(Nx, DX)[0:Nx//2]*2.0*np.pi # get non-negative frequencies; \n",
    "    wavenum_y = fftfreq(Ny, DY)[0:Ny//2]*2.0*np.pi # get non-negative frequencies; \n",
    "    \n",
    "    delkx = 2.0*np.pi/Nx*DX\n",
    "    delky = 2.0*np.pi/Ny*DY\n",
    "    delkh = max([delkx, delky]) \n",
    "    Nmax = int(np.sqrt(2)*max([Nx/2,Ny/2]))\n",
    "    p=np.arange(0,Nmax+1,1)\n",
    "    kp    = p*delkh\n",
    "    #zero frequency (or the DC signal) was removed by detrending, TO DO: make sure the value of fourier coeff for k=0 are small\n",
    "\n",
    "    #print('two 1D var provided')\n",
    "    detrended_u = signal.detrend(one_d_u_velocity,type='constant')\n",
    "    detrended_v = signal.detrend(one_d_v_velocity,type='constant')\n",
    "    #udwf = fft(detrended_u*w)\n",
    "    #vdwf = fft(detrended_v*w)\n",
    "    dale_durran_formula_2017 = (DX*DY*min([delkx,delky]))/(8*Nx*Ny*np.pi**2)*((np.abs(udwf[0:N//2]))**2+(np.abs(vdwf[0:N//2]))**2)\n",
    "\n",
    "    #dale_durran_formula_2017[-1]=dale_durran_formula_2017[-1]/2.0\n",
    "    return xf, dale_durran_formula_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17190e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft, fft2, fftfreq, fftshift\n",
    "print(np.shape(u_2d))\n",
    "print(np.shape(fft2(u_2d)))\n",
    "print(np.min((np.abs(fft2(u_2d))**2)))\n",
    "print(np.max((np.abs(fft2(u_2d))**2)))\n",
    "plt.imshow(np.abs(fftshift(fft2(u_2d-np.mean(u_2d))))**2)\n",
    "plt.colorbar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "twoD_field=u_2d\n",
    "\n",
    "Nx=np.shape(twoD_field)[1]\n",
    "Ny=np.shape(twoD_field)[0]\n",
    "#w = signal.windows.hann(N)\n",
    "print('Nx = ',Nx)\n",
    "print('Ny = ',Ny)\n",
    "\n",
    "wavenum_x = fftfreq(Nx, 100.0)[0:Nx//2]*2.0*np.pi # get non-negative frequencies; \n",
    "wavenum_y = fftfreq(Ny, 100.0)[0:Ny//2]*2.0*np.pi # get non-negative frequencies; \n",
    "\n",
    "delkx = 2.0*np.pi/Nx*100.0\n",
    "delky = 2.0*np.pi/Ny*100.0\n",
    "delkh = max([delkx, delky]) \n",
    "print('delkh ',delkh)\n",
    "Nmax = int(np.sqrt(2)*max([Nx/2,Ny/2]))\n",
    "print('Nmax: ',Nmax)\n",
    "p=np.arange(0,Nmax+1,1)\n",
    "kp    = p*delkh\n",
    "print('number of kp ',len(kp))\n",
    "print(kp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423171df-7463-4af8-9453-ca4329490bff",
   "metadata": {},
   "source": [
    "## 2D PSD: Bert Vandenbroucke's Code (Complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed2f62-c194-42c7-a1cc-dd343a8226e4",
   "metadata": {},
   "source": [
    "The following code is based on the script provided at https://bertvandenbroucke.netlify.app/2019/05/24/computing-a-power-spectrum-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c52bcd-7410-4d3c-ad91-1469fb65b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile make_PSD_RAMS.py\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "plt.style.use('ggplot')\n",
    "import read_vars_WRF_RAMS\n",
    "\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "   \n",
    "def read_head(headfile,h5file):\n",
    "        # Function that reads header files from RAMS\n",
    "\n",
    "        # Inputs:\n",
    "        #   headfile: header file including full path in str format\n",
    "        #   h5file: h5 datafile including full path in str format\n",
    "\n",
    "        # Returns:\n",
    "        #   zmn: height levels for momentum values (i.e., grid box upper and lower levels)\n",
    "        #   ztn: height levels for thermodynaic values (i.e., grid box centers)\n",
    "        #   nx:: the number of x points for the domain associated with the h5file\n",
    "        #   ny: the number of y points for the domain associated with the h5file\n",
    "        #   npa: the number of surface patches\n",
    "\n",
    "\n",
    "        dom_num = h5file[h5file.index('.h5')-1] # Find index of .h5 to determine position showing which nest domain to use\n",
    "\n",
    "        with open(headfile) as f:\n",
    "            contents = f.readlines()\n",
    "\n",
    "        idx_zmn = contents.index('__zmn0'+dom_num+'\\n')\n",
    "        nz_m = int(contents[idx_zmn+1])\n",
    "        zmn = np.zeros(nz_m)\n",
    "        for i in np.arange(0,nz_m):\n",
    "            zmn[i] =  float(contents[idx_zmn+2+i])\n",
    "\n",
    "        idx_ztn = contents.index('__ztn0'+dom_num+'\\n')\n",
    "        nz_t = int(contents[idx_ztn+1])\n",
    "        ztn = np.zeros(nz_t)\n",
    "        for i in np.arange(0,nz_t):\n",
    "            ztn[i] =  float(contents[idx_ztn+2+i])\n",
    "\n",
    "        ztop = np.max(ztn) # Model domain top (m)\n",
    "\n",
    "        # Grad the size of the horizontal grid spacing\n",
    "        idx_dxy = contents.index('__deltaxn\\n')\n",
    "        dxy = float(contents[idx_dxy+1+int(dom_num)].strip())\n",
    "\n",
    "        idx_npatch = contents.index('__npatch\\n')\n",
    "        npa = int(contents[idx_npatch+2])\n",
    "\n",
    "        idx_ny = contents.index('__nnyp\\n')\n",
    "        idx_nx = contents.index('__nnxp\\n')\n",
    "        ny = np.ones(int(contents[idx_ny+1]))\n",
    "        nx = np.ones(int(contents[idx_ny+1]))\n",
    "        for i in np.arange(0,len(ny)):\n",
    "            nx[i] = int(contents[idx_nx+2+i])\n",
    "            ny[i] = int(contents[idx_ny+2+i])\n",
    "\n",
    "        ny_out = ny[int(dom_num)-1]\n",
    "        nx_out = nx[int(dom_num)-1]\n",
    "\n",
    "        return zmn, ztn, nx_out, ny_out, dxy, npa \n",
    "\n",
    "simulations=['PHI1.1-R','PHI2.1-R','WPO1.1-R','BRA1.1-R','DRC1.1-R','AUS1.1-R','USA1.1-R']\n",
    "domain='3'\n",
    "variables = [['Tk', 0, 'model', '$K$']             , ['THETA', 0, 'model', '$K$'],\\\n",
    "             ['QV', 0, 'model', '$kg kg^{-1}$'], ['RH', 0, 'model', '$RH_{sfc}^{2} (percent^{2})$'],\\\n",
    "             ['U', 0, 'model', '$m s^{-1}$']        , ['V', 0, 'model', '$m s^{-1}$'],\\\n",
    "             ['WSPD', 0, 'model', '$m s^{-1}$']  , ['W', 0, 'model', '$m s^{-1}$'],\\\n",
    "             ['MCAPE', -999, None, '$J^{2}kg^{-2})$']     , ['MCIN', -999, None, '$MCIN^{2} (J^{2}kg^{-2})$'], \\\n",
    "             ['Tk', 750, 'pressure', '$K$']        , ['THETA', 750, 'pressure', '$K$'],\\\n",
    "             ['QV', 750, 'pressure', '$kg kg^{-1}$'], ['RH', 750, 'pressure', '$RH_{750}^{2} (percent^{2})$'],\\\n",
    "             ['U', 750, 'pressure', '$m^{2}s^{-2})$']   , ['V', 750, 'pressure', '$m s^{-1}$'],\\\n",
    "             ['WSPD', 750, 'pressure', '$m s^{-1}$'], ['W', 750, 'pressure', '$m s^{-1}$'],\\\n",
    "             ['Tk', 500, 'pressure', '$K$']        , ['THETA', 500, 'pressure', '$K$'],\\\n",
    "             ['QV', 500, 'pressure', '$kg kg^{-1}$'], ['RH', 500, 'pressure', '$RH_{500}^{2} (percent^{2})$'],\\\n",
    "             ['U', 500, 'pressure', '$m s^{-1}$']   , ['V', 500, 'pressure', '$m s^{-1}$'],\\\n",
    "             ['WSPD', 500, 'pressure', '$m s^{-1}$'], ['W', 500, 'pressure', '$m s^{-1}$'],\\\n",
    "             ['Tk', 200, 'pressure', '$K$']        , ['THETA', 200, 'pressure', '$K$'],\\\n",
    "             ['QV', 200, 'pressure', '$kg kg^{-1}$'], ['RH', 200, 'pressure', '$percent$'],\\\n",
    "             ['U', 200, 'pressure', '$m s^{-1}$']   , ['V', 200, 'pressure', '$m s^{-1}$'], \\\n",
    "             ['WSPD', 200, 'pressure', '$m s^{-1}$'], ['W', 200, 'pressure', '$m s^{-1}$']]\n",
    "\n",
    "units_dict = {'Tk':'$K$','QV':'$kg kg^{-1}$','RH':'percent','WSPD':'$m s^{-1}$','U':'$m s^{-1}$',\\\n",
    "              'V':'$m s^{-1}$','W':'$m s^{-1}$','MCAPE':'$J kg^{-1}$','MCIN':'$J kg^{-1}$','THETA':'$K$'}\n",
    "\n",
    "colors    =  ['#000000','#E69F00','#56B4E9','#009E73','#F0E442','#0072B2','#D55E00','#CC79A7']\n",
    "\n",
    "color_dict = {'ARG1.1-R_old':'#000000',\\\n",
    "              'PHI1.1-R':'#E69F00',\\\n",
    "              'PHI2.1-R':'#56B4E9',\\\n",
    "              'WPO1.1-R':'#009E73',\\\n",
    "              'BRA1.1-R':'#7F7F7F',\\\n",
    "              'USA1.1-R':'#0072B2',\\\n",
    "              'DRC1.1-R':'#D55E00',\\\n",
    "              'AUS1.1-R':'#CC79A7'}\n",
    "\n",
    "def make_PSD_RAMS(TYPE, WHICH_TIME, VARIABLE, SIMULATIONS, DOMAIN):\n",
    "\n",
    "    print('working on ',VARIABLE,'\\n')\n",
    "    fig = plt.figure(figsize=(9,9))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    for ii,simulation in enumerate(SIMULATIONS): \n",
    "        print('    working on simulation: ',simulation)\n",
    "        if DOMAIN=='1' or DOMAIN =='2':\n",
    "            rams_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+DOMAIN+'/out/'+'a-A-*g'+DOMAIN+'.h5'))# CSU machine\n",
    "        if DOMAIN=='3':\n",
    "            rams_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+DOMAIN+'/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "        print('        total # files = ',len(rams_files))\n",
    "        print('        first file is ',rams_files[0])\n",
    "        print('        last file is ',rams_files[-1])\n",
    "        if WHICH_TIME=='start':\n",
    "            rams_fil    = rams_files[0]\n",
    "        if WHICH_TIME=='middle':\n",
    "            rams_fil    = rams_files[int(len(rams_files)/2)]\n",
    "        if WHICH_TIME=='end':\n",
    "            rams_fil    = rams_files[-1]\n",
    "        print('        choosing the '+WHICH_TIME+' file: ',rams_fil)\n",
    "      \n",
    "        z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(rams_fil,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "        y_dim,x_dim = np.shape(z)\n",
    "        if DOMAIN=='1':\n",
    "            dx=1.6\n",
    "        if DOMAIN=='2':\n",
    "            dx=0.4\n",
    "        if DOMAIN=='3':\n",
    "            dx=0.1\n",
    "            \n",
    "        print('        cropping the array')\n",
    "        # make array square\n",
    "        # Get the dimensions of the image\n",
    "        height, width = z.shape\n",
    "        print('        original height of image: ',height)\n",
    "        print('        original width of image: ',width)\n",
    "        # Calculate the size of the square\n",
    "        size = min(width, height)\n",
    "\n",
    "        # Calculate the coordinates for cropping\n",
    "        left = (width - size) // 2\n",
    "        top = (height - size) // 2\n",
    "        right = (width + size) // 2\n",
    "        bottom = (height + size) // 2\n",
    "\n",
    "        # Crop the image using NumPy slicing\n",
    "        cropped_img_array = z[top:bottom, left:right]\n",
    "\n",
    "        # fourier spectrum part\n",
    "        image = cropped_img_array\n",
    "        print('        shape of input image: ',np.shape(image))\n",
    "        npix = image.shape[0]\n",
    "\n",
    "        fourier_image = np.fft.fftn(image)\n",
    "        fourier_amplitudes = np.abs(fourier_image)**2\n",
    "\n",
    "        kfreq = np.fft.fftfreq(npix) * npix\n",
    "        print('size of wavenumbers from np.fft.fftfreq: ',len(np.fft.fftfreq(npix)))\n",
    "        #print('wavenumbers from np.fft.fftfreq: ',np.fft.fftfreq(npix))\n",
    "        print('min/max of wavenumbers from np.fft.fftfreq: ',min(np.fft.fftfreq(npix)),'/',max(np.fft.fftfreq(npix)))\n",
    "        print('size of wavenumbers from np.fft.fftfreq*npix: ',len(kfreq))\n",
    "        #print('wavenumbers from np.fft.fftfreq*npix: ',kfreq)\n",
    "        print('min/max of wavenumbers from np.fft.fftfreq*npix: ',min(kfreq),'/',max(kfreq))\n",
    "        kfreq2D = np.meshgrid(kfreq, kfreq)\n",
    "        knrm = np.sqrt(kfreq2D[0]**2 + kfreq2D[1]**2)\n",
    "        #print('shape of knrm: ',np.shape(knrm))\n",
    "        #print('shape of fourier_amplitudes: ',np.shape(fourier_amplitudes))\n",
    "        knrm = knrm.flatten()\n",
    "        fourier_amplitudes = fourier_amplitudes.flatten()\n",
    "        #print('shape of knrm: ',np.shape(knrm))\n",
    "        #print('shape of fourier_amplitudes: ',np.shape(fourier_amplitudes))\n",
    "        kbins = np.arange(0.5, npix//2+1, 1.)\n",
    "        #print(kbins)\n",
    "        kvals = 0.5 * (kbins[1:] + kbins[:-1])\n",
    "        Abins, _, _ = stats.binned_statistic(knrm, fourier_amplitudes,\n",
    "                                             statistic = \"mean\",\n",
    "                                             bins = kbins)\n",
    "        Abins *= np.pi * (kbins[1:]**2 - kbins[:-1]**2)\n",
    "        wavelengths = npix*dx/kvals\n",
    "        #print('wavenumbers: ',kvals)\n",
    "        #print('wavelengths: ',wavelengths)\n",
    "        # plotting part\n",
    "        #ax1.semilogy(kvals, Abins  , color=color_dict[simulation], label=simulation)\n",
    "        if TYPE=='standard':\n",
    "            ax1.semilogy(wavelengths, Abins  , color=color_dict[simulation], label=simulation)\n",
    "        if TYPE=='premultiplied':\n",
    "            ax1.plot(kvals, Abins*kvals, color=color_dict[simulation], label=simulation)\n",
    "        #ax1.plot(kvals, Abins  , color=color_dict[simulation], label=simulation)\n",
    "        #ax1.plot(kvals, Abins*kvals, color=color_dict[simulation], label=simulation)\n",
    "        #ax1.semilogy(kvals, Abins*kvals, color=color_dict[simulation], label=simulation)\n",
    "\n",
    "    ax1.set_xscale(\"log\")\n",
    "    ax1.invert_xaxis()  # Invert the x-axis\n",
    "    #ax1.plot(kvals[25:-120],(25*kvals[25:-120])**(-5/3) , linestyle='--',label=r'$E(k) \\propto k^{-5/3}$', linewidth=1.5, color='r')\n",
    "    if TYPE=='standard': \n",
    "        ax1.set_ylabel(r'power spectral density (arbitrary units)')# $(m^{3} s^{-2})$')\n",
    "    if TYPE=='premultiplied':\n",
    "        ax1.set_ylabel(r'power spectral density*k (arbitrary units)')# $(m^{3} s^{-2})$')\n",
    "    #ax1.set_xlabel(r'# waves/length of domain')\n",
    "    ax1.set_xlabel(r'wavelength (km)')\n",
    "    #ax1.axvline(2*np.pi/11200., color='g', alpha=0.5,label='')\n",
    "    #secax = ax1.secondary_xaxis('top', functions=(lambda x: 2*np.pi/x, lambda x: 2*np.pi/x))\n",
    "    #secax = ax1.secondary_xaxis('top', functions=(lambda x: npix*dx/x, lambda x: npix*dx/x))\n",
    "    #secax.set_xlabel(r\"Wavelength (km)\")\n",
    "    plt.legend(loc=('upper right'))\n",
    "    if VARIABLE[2]:\n",
    "        title_string = 'Power spectrum of '+VARIABLE[0]+' at '+VARIABLE[2]+' level '+str(int(VARIABLE[1]))+' for d0'+DOMAIN+': '+WHICH_TIME+' of the simulation'\n",
    "    else:\n",
    "        title_string = 'Power spectrum of '+VARIABLE[0]+' for d0'+DOMAIN+': '+WHICH_TIME+' of the simulation'\n",
    "    plt.title(title_string)\n",
    "    #plt.title('KE (w) spectrum (1D, west to east) for WRF and RAMS at  model level '+str(lev_num))\n",
    "    #plt.savefig('KE_uv_lev_'+str(lev_num)+'_west_to_east_1D_spectral_density_linear_detrend_then_window_WRF_RAMS.png',dpi=200)\n",
    "    #plt.savefig('KE_uv_lev_'+str(lev_num)+'_west_to_east_1D_spectral_density_linear_detrend_then_window.png')\n",
    "    print('-----\\n')\n",
    "    plt.tight_layout()\n",
    "    if VARIABLE[2]:\n",
    "        filename = 'power_spectrum_'+TYPE+'_RAMS_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_d0'+DOMAIN+'_'+WHICH_TIME+'.png'\n",
    "    else:\n",
    "        filename = 'power_spectrum_'+TYPE+'_RAMS_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_d0'+DOMAIN+'_'+WHICH_TIME+'.png'\n",
    "    print('saving to png file: ',filename)\n",
    "    plt.savefig(filename, dpi = 150, bbox_inches = \"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "for var in variables:\n",
    "    for psd_type in ['standard','premultiplied']:\n",
    "        make_PSD_RAMS(psd_type,'middle', var, simulations, domain)\n",
    "        \n",
    "        \n",
    "# argument = []\n",
    "# for var in variables:\n",
    "#     for psd_type in ['standard','premultiplied']:\n",
    "#         argument = argument + [(psd_type,'middle', var, simulations, domain)]\n",
    "\n",
    "# print('length of argument is: ',len(argument))\n",
    "# # # ############################### FIRST OF ALL ################################\n",
    "# cpu_count1 = 37 #cpu_count()\n",
    "# print('number of cpus: ',cpu_count1)\n",
    "# # # #############################################################################\n",
    "\n",
    "# def main(FUNCTION, ARGUMENT):\n",
    "#     start_time = time.perf_counter()\n",
    "#     with Pool(processes = (cpu_count1-1)) as pool:\n",
    "#         data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "#     finish_time = time.perf_counter()\n",
    "#     print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "#     #df_all = pd.concat(data, ignore_index=True)\n",
    "#     #thermo_indices_data_csv_file = csv_folder+'thermodynamic_indices_' + DOMAIN + '_comb_track_filt_01_02_50_02_sr5017_setpos.csv'\n",
    "#     #print('saving thermodynamic indices to the file: ',thermo_indices_data_csv_file)\n",
    "#     #df_all.to_csv(thermo_indices_data_csv_file)  # sounding data\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main(make_PSD_RAMS, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a6999",
   "metadata": {},
   "source": [
    "# Multiprocessing 3-panel vertical cross-sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e114e6-478b-4bd7-a67b-a5c19497dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile plot_tobac_crosssections_parallel_plot_cell_dim_box.py\n",
    "\n",
    "## plot features (besides the cell being tracked) in the plan view\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import numpy.ma as ma\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.patches import Rectangle\n",
    "import datetime\n",
    "import sys\n",
    "import glob\n",
    "import rams_tools\n",
    "import os\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from RAMS_Post_Process import fx_postproc_RAMS as RAMS_fx\n",
    "import cartopy.crs as crs\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib import ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "def radar_colormap():\n",
    "    nws_reflectivity_colors = [\n",
    "    \"#646464\", # ND\n",
    "    \"#ccffff\", # -30\n",
    "    \"#cc99cc\", # -25\n",
    "    \"#996699\", # -20\n",
    "    \"#663366\", # -15\n",
    "    \"#cccc99\", # -10\n",
    "    \"#999966\", # -5\n",
    "    \"#646464\", # 0\n",
    "    \"#04e9e7\", # 5\n",
    "    \"#019ff4\", # 10\n",
    "    \"#0300f4\", # 15\n",
    "    \"#02fd02\", # 20\n",
    "    \"#01c501\", # 25\n",
    "    \"#008e00\", # 30\n",
    "    \"#fdf802\", # 35\n",
    "    \"#e5bc00\", # 40\n",
    "    \"#fd9500\", # 45\n",
    "    \"#fd0000\", # 50\n",
    "    \"#d40000\", # 55\n",
    "    \"#bc0000\", # 60\n",
    "    \"#f800fd\", # 65\n",
    "    \"#9854c6\", # 70\n",
    "    \"#fdfdfd\" # 75\n",
    "    ]\n",
    "\n",
    "    return mpl.colors.ListedColormap(nws_reflectivity_colors)\n",
    "\n",
    "\n",
    "cma1=plt.get_cmap('bwr')\n",
    "cma2=radar_colormap()\n",
    "cma3=plt.get_cmap('tab20c')\n",
    "#cma4=ncm.cmap(\"WhiteBlueGreenYellowRed\")\n",
    "cma5=plt.get_cmap('gray_r')\n",
    "cma6=plt.get_cmap('rainbow')\n",
    "cma7=plt.get_cmap('Oranges')\n",
    "cma8=plt.get_cmap('coolwarm')\n",
    "#cma9=cma4.reversed()\n",
    "cma10=plt.get_cmap('gist_yarg')\n",
    "\n",
    "Cp=1004.\n",
    "Rd=287.0\n",
    "p00 = 100000.0\n",
    "\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def fig_process_vert(AX, TER, CONTOUR, XY_1D, XY1, XY2, Y_CROSS, PLOT_COLORBAR, CBAR_EXP, FONTSIZE, TITLESTRING, TIMESTRING, FILENAMESTRING, PROD, UNITS, VERT_CROSS, HEIGHT, YLABEL, IS_PANEL_PLOT):\n",
    "    #F = plt.gcf()  # Gets the current figure\n",
    "    #ax = plt.gca()  # Gets the current axes\n",
    "\n",
    "    if IS_PANEL_PLOT == False:\n",
    "        AX.set_title('%s (%s) \\n %s' % (TITLESTRING, UNITS, TIMESTRING),\n",
    "                  fontsize=FONTSIZE, stretch='normal')\n",
    "\n",
    "    if VERT_CROSS == \"zonal\":\n",
    "        AX.fill_between(XY_1D[XY1:XY2], 0, TER[int(Y_CROSS),\n",
    "                        XY1:XY2]/1000.0, facecolor='wheat')\n",
    "        AX.set_xlabel('x-distance (km)', fontsize=FONTSIZE)\n",
    "\n",
    "    else:\n",
    "        AX.fill_between(XY_1D[XY1:XY2], 0, TER[XY1:XY2,\n",
    "                        int(Y_CROSS)]/1000.0, facecolor='wheat')\n",
    "        AX.set_xlabel('y-distance (km)', fontsize=FONTSIZE)\n",
    "\n",
    "    AX.patch.set_color(\"white\")\n",
    "\n",
    "    if YLABEL:\n",
    "        AX.set_ylabel('Height (km)', fontsize=FONTSIZE)\n",
    "    AX.set_ylim([0, HEIGHT])\n",
    "    AX.set_xlim([XY1*100.0/1000.0, XY2*100.0/1000.0])\n",
    "\n",
    "    class OOMFormatter(matplotlib.ticker.ScalarFormatter):\n",
    "        def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n",
    "            self.oom = order\n",
    "            self.fformat = fformat\n",
    "            matplotlib.ticker.ScalarFormatter.__init__(\n",
    "                self, useOffset=offset, useMathText=mathText)\n",
    "\n",
    "        def _set_orderOfMagnitude(self, nothing):\n",
    "            self.orderOfMagnitude = self.oom\n",
    "\n",
    "        def _set_format(self, vmin, vmax):\n",
    "            self.format = self.fformat\n",
    "            if self._useMathText:\n",
    "                self.format = '$%s$' % matplotlib.ticker._mathdefault(\n",
    "                    self.format)\n",
    "\n",
    "    if PLOT_COLORBAR:\n",
    "        if IS_PANEL_PLOT == False:\n",
    "            if abs(CBAR_EXP):\n",
    "                divider = make_axes_locatable(AX)\n",
    "                cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.6)\n",
    "                bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",\n",
    "                                   format=OOMFormatter(CBAR_EXP, mathText=False),extend='both')\n",
    "                bar.ax.tick_params(labelsize=FONTSIZE-1)\n",
    "                #file_id = '%s_%s' % (PROD, FILENAMESTRING)\n",
    "                #filename = '%s.png' % (file_id)\n",
    "                #print(filename)\n",
    "                # Saves the figure with small margins\n",
    "                #plt.savefig(filename, dpi=my_dpi, bbox_inches='tight')\n",
    "            else:\n",
    "                divider = make_axes_locatable(AX)\n",
    "                cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.66)\n",
    "                bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",extend='both')\n",
    "                bar.ax.tick_params(labelsize=FONTSIZE-1)\n",
    "                #file_id = '%s_%s' % (PROD, FILENAMESTRING)\n",
    "                #filename = '%s.png' % (file_id)\n",
    "                #print(filename)\n",
    "                # Saves the figure with small margins\n",
    "                #plt.savefig(filename, dpi=my_dpi, bbox_inches='tight')\n",
    "\n",
    "        else:\n",
    "            if abs(CBAR_EXP):\n",
    "                divider = make_axes_locatable(AX)\n",
    "                cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.4)\n",
    "                bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",\n",
    "                                   format=OOMFormatter(CBAR_EXP, mathText=False),extend='both')\n",
    "                bar.ax.tick_params(labelsize=FONTSIZE-1)\n",
    "            else:\n",
    "                divider = make_axes_locatable(AX)\n",
    "                cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.4)\n",
    "                bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",extend='both')\n",
    "                bar.ax.tick_params(labelsize=FONTSIZE-1)\n",
    "        # plt.close() This should remain commented. plt should be closed in the panel plot function\n",
    "        # if export_flag == 1:\n",
    "        # Convert the figure to a gif file\n",
    "        #os.system('convert -render -flatten %s %s.gif' % (filename, file_id))\n",
    "        #os.system('rm -f %s' % filename)\n",
    "\n",
    "def plot_zonal_vertcross(DATA, TERR, X_1D, Z_3D, Z_1D, DXY, VAR1, DESTAGGER, STAGGER_DIM, LEVELS_VAR1, CMAP_VAR1,\n",
    "                       VAR2_XR, LEVELS_VAR2, VAR2_COLOR,\n",
    "                       VAR3, LEVELS_VAR3, VAR3_COLOR,\n",
    "                       VAR4, LEVELS_VAR4, VAR4_COLOR,\n",
    "                       yy, x1, x2,\n",
    "                       PLOT_WINDS, PANEL_PLOT, HEIGHT, AX, PLOT_CBAR, EXP_LABEL, PANEL_LABEL, XPOS, YPOS, ZPOS, ZPOS_GRID, CELL_DIM, NUM_MODEL_LEVS, TITLETIME, FILENAMETIME):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "\n",
    "       \n",
    "    XV1, __ = np.meshgrid(X_1D[x1:x2]/1000.0, Z_1D/1000.0)\n",
    "    z_3D_2D_slice    =  (TERR[yy,x1:x2] + Z_1D[:,np.newaxis])/1000.0\n",
    "\n",
    "    #var1 = DATA[VAR1]\n",
    "\n",
    "\n",
    "    if DESTAGGER:\n",
    "        print(var1)\n",
    "        var1 = destagger(var1, STAGGER_DIM, meta=True)\n",
    "    else:\n",
    "        print(' ')\n",
    "\n",
    "    \n",
    "    if (isinstance(LEVELS_VAR1, np.ndarray)):\n",
    "#         C1 = AX.contourf(XV1, zh[:, yy, x1:x2]/1000.0, VAR2_XR[:, yy, x1:x2], levels=LEVELS_VAR1,\n",
    "#                            cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "        C1 = AX.contourf(XV1,z_3D_2D_slice, VAR2_XR[:, yy, x1:x2], levels=LEVELS_VAR1,\n",
    "                           cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "    else:\n",
    "        C1 = AX.contourf(XV1, z_3D_2D_slice, VAR2_XR[:, yy, x1:x2],\n",
    "                           cmap=CMAP_VAR1, extend='both')\n",
    "\n",
    "        \n",
    "    #levels_th = np.arange(290.0, 690.0, 2.0)\n",
    "    #C4 = plt.contour(XV1, zh[:, yy, x1:x2]/1000.0, DATA.THETA.values[:, yy, x1:x2], colors='k', levels=levels_th, axis=AX, linewidth=0.6)\n",
    "    #plt.clabel(C4, inline=1, fontsize=14, fmt='%3.0f')\n",
    "\n",
    "        \n",
    "    if (isinstance(LEVELS_VAR2, np.ndarray)):\n",
    "        C2 = AX.contour(XV1, z_3D_2D_slice, VAR2_XR[:, yy, x1:x2],\n",
    "                             levels=LEVELS_VAR2, colors=VAR2_COLOR, linewidths=1., linestyles=\"-\")\n",
    "    else:\n",
    "        C2 = AX.contour(XV1, z_3D_2D_slice, VAR2_XR[:, yy, x1:x2],\n",
    "                             colors='k', linewidths=1., linestyles=\"--\")\n",
    "    AX.clabel(C2, inline=1, fontsize=10, fmt='%3.0f')\n",
    "\n",
    "    if VAR3=='RTP-RV_g/kg':\n",
    "        total_condensate_zonal = DATA[\"RTP\"][:, yy, x1:x2]*1000.0 - DATA[\"RV\"][:, yy, x1:x2]*1000.0\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(XV1,z_3D_2D_slice, np.absolute(total_condensate_zonal),\n",
    "                                 levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\") \n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "            \n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    elif VAR3=='RTP-RV_g/m3':\n",
    "        th = DATA['THETA'][:, yy, x1:x2]\n",
    "        pi = DATA['PI'][:, yy, x1:x2]\n",
    "        rv = DATA['RV'][:, yy, x1:x2]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        total_condensate_zonal = (DATA[\"RTP\"][:, yy, x1:x2]*1000.0 - DATA[\"RV\"] [:, yy, x1:x2]*1000.0)*dens\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(XV1,z_3D_2D_slice, np.absolute(total_condensate_zonal),\n",
    "                                 levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\") \n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "            \n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    elif VAR3=='precipitating_condensate_g/m3':\n",
    "        th = DATA['THETA'][:, yy, x1:x2]\n",
    "        pi = DATA['PI'][:, yy, x1:x2]\n",
    "        rv = DATA['RV'][:, yy, x1:x2]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        total_condensate_zonal = (DATA[\"RTP\"][:, yy, x1:x2]*1000.0 - DATA[\"RV\"] [:, yy, x1:x2]*1000.0 \\\n",
    "                                                                        - DATA[\"RCP\"][:, yy, x1:x2]*1000.0 \\\n",
    "                                                                        - DATA[\"RPP\"][:, yy, x1:x2]*1000.0)*dens\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(XV1,z_3D_2D_slice, np.absolute(total_condensate_zonal),\n",
    "                                 levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")\n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "        #del total_condensate_zonal\n",
    "        \n",
    "    else:\n",
    "        print('please provide correct value of VAR3')\n",
    "\n",
    "    if VAR4:\n",
    "        print('something')\n",
    "#         var4 = DATA.variables[VAR4]\n",
    "#         if (isinstance(LEVELS_VAR4, np.ndarray)):\n",
    "#             C4 = pAXlt.contour(XV1, z_3D_2D_slice/1000.0, var4[:, yy, x1:x2]*1000.,\n",
    "#                              levels=LEVELS_VAR4, colors=VAR4_COLOR, linewidths=2.0, linestyles=\":\")  # , linewidths=0.85,linestyles=\"--\")#hatches=[None,None,'.','/']\n",
    "#         else:\n",
    "#             print('please provide levels!')\n",
    "            \n",
    "#     if PLOT_WINDS:\n",
    "#         winds_thin_x = 4\n",
    "#         winds_thin_z = 4\n",
    "#         XVwind, ZVwind = np.meshgrid(xh[x1:x2:winds_thin_x], z[::winds_thin_z])\n",
    "#         u1 = DATA.variables[\"UP\"][::winds_thin_z,\n",
    "#                                        yy, x1:x2:winds_thin_x]*1.94384\n",
    "#         w1 = DATA.variables[\"WP\"][::winds_thin_z,\n",
    "#                                        yy, x1:x2:winds_thin_x]*1.94384\n",
    "#         QV1 = AX.barbs(XVwind, zh[::winds_thin_z, yy, x1:x2:winds_thin_x]/1000.0,\n",
    "#                        u1, w1, length=7.2, pivot='middle', linewidth=0.60, flip_barb=True)\n",
    "          \n",
    "    C4zs = AX.plot(X_1D[x1:x2], TERR[yy, x1:x2]/1000., color='sienna', linewidth=3.6)\n",
    "    # if DATA.variables['zs']:\n",
    "    #    C5 = plot(xh[x1:x2],zs[0,yy,x1:x2]/1000.0, color = \"black\") # Plot topography\n",
    "\n",
    "    title = 'Vertical cross-section (zonal) of $w$'\n",
    "    prodid = 'tests'+'_'+VAR1+'_th_vcross_zonal_'+'y' + \\\n",
    "        str(yy)+'_x1_'+str(x1)+'_x2_'+str(x2)+'.png'\n",
    "    units = 'm/s'#var1.attrs['units']  # '$ x 10^{-5} $'+\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    # the x coords of this transformation are axes, and the\n",
    "    # y coord are data\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "    #        verticalalignment='top', bbox=props)\n",
    "    AX.text(0.55, 0.94, EXP_LABEL, fontsize=12,\n",
    "            verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "    \n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "        #        verticalalignment='top', bbox=props)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "        \n",
    "    # text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n",
    "    #  verticalalignment='center', transform=ax.transAxes\n",
    "    \n",
    "    AX.scatter(XPOS*DXY/1000.0,TERR[YPOS,XPOS]/1000.+ZPOS/1000.,marker='+',color='k',s=130.5)\n",
    "    \n",
    "     # Draw box that is used for averaging of hydrometeor conc. for filtering:\n",
    "    if CELL_DIM is not None:\n",
    "        #print('making zonal cross-section: xpos = ',XPOS)\n",
    "        #print('making zonal cross-section: cell_dim = ',CELL_DIM)\n",
    "        print('cell dimension is ',CELL_DIM,' grid points')\n",
    "        low_x_km  = np.max([(XPOS - (CELL_DIM/2.)),0.0])*DXY/1000.0\n",
    "        box_width_km = CELL_DIM*DXY/1000.0\n",
    "        print('width of the box is ',box_width_km,' km')\n",
    "        low_z_grid  = int(np.max([ZPOS_GRID - (CELL_DIM / 2.),0]))\n",
    "        high_z_grid = int(np.min([ZPOS_GRID + (CELL_DIM / 2.),NUM_MODEL_LEVS]))\n",
    "        box_height_km = (Z_1D[high_z_grid] - Z_1D[low_z_grid])/1000.0\n",
    "        print('width of the box is ',box_height_km,' km')\n",
    "        low_z_km   = (TERR[YPOS,XPOS] + Z_1D[low_z_grid])/1000.0\n",
    "        print('bottom side of the box is ',low_z_km,' km high')\n",
    "        print('lower left corner of the box is x= ',low_x_km, ' z=',low_z_km)\n",
    "        rect1  = AX.add_patch(Rectangle((low_x_km,low_z_km), box_width_km, box_height_km , color='green', fc = 'none',lw = 1.8))\n",
    "\n",
    "    \n",
    "    #AX.scatter(xh[x1:x2], zh[:, yy, x1:x2]/1000.0[var1[:, yy, x1:x2].idmax(dim='phony_dim_3')],marker='o',color='k',s=10.5)\n",
    "    \n",
    "    # plot \n",
    "    fig_process_vert(AX, TERR, C1, X_1D, x1, x2, yy, PLOT_CBAR, 0, 15, title, TITLETIME, FILENAMETIME, prodid, units, \"zonal\", HEIGHT, True, PANEL_PLOT)\n",
    "    # fig_process_vert(CONTOUR,Y_CROSS,CBAR_EXP,TITLESTRING,TIMESTRING,FILENAMESTRING,PROD,UNITS,VERT_CROSS,HEIGHT,IS_PANEL_PLOT):\n",
    "    ###\n",
    "\n",
    "def plot_meridional_vertcross(DATA, TERR, Y_1D, Z_3D, Z_1D, DXY, VAR1, DESTAGGER, STAGGER_DIM, LEVELS_VAR1, CMAP_VAR1,\n",
    "                       VAR2_XR, LEVELS_VAR2, VAR2_COLOR,\n",
    "                       VAR3, LEVELS_VAR3, VAR3_COLOR,\n",
    "                       VAR4, LEVELS_VAR4, VAR4_COLOR,\n",
    "                       xx, y1, y2,\n",
    "                       PLOT_WINDS, PANEL_PLOT, HEIGHT, AX, PLOT_CBAR, EXP_LABEL, PANEL_LABEL, XPOS, YPOS, ZPOS, ZPOS_GRID, CELL_DIM, NUM_MODEL_LEVS, TITLETIME, FILENAMETIME):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "    \n",
    "    \n",
    "    YV1, __ = np.meshgrid(Y_1D[y1:y2]/1000.0, Z_1D/1000.0)\n",
    "    z_3D_2D_slice    =  (TERR[y1:y2, xx] + Z_1D[:,np.newaxis])/1000.0\n",
    "    #zh=Z_3D\n",
    "\n",
    "    #var1 = DATA[VAR1]\n",
    "\n",
    "    if DESTAGGER:\n",
    "        print(var1)\n",
    "        var1 = destagger(var1, STAGGER_DIM, meta=True)\n",
    "    else:\n",
    "        print(' ')\n",
    "\n",
    "    \n",
    "    if (isinstance(LEVELS_VAR1, np.ndarray)):\n",
    "        #C1 = AX.contourf(YV1, zh[:, y1:y2, xx]/1000.0, VAR2_XR[:, y1:y2, xx], levels=LEVELS_VAR1,\n",
    "        #                  cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "        C1 = AX.contourf(YV1, z_3D_2D_slice, VAR2_XR[:, y1:y2, xx], levels=LEVELS_VAR1,\n",
    "                          cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "    else:\n",
    "        C1 = AX.contourf(YV1, z_3D_2D_slice, VAR2_XR[:, y1:y2, xx],\n",
    "                          cmap=CMAP_VAR1, extend='both')\n",
    "\n",
    "        \n",
    "    #levels_th = np.arange(290.0, 690.0, 2.0)\n",
    "    #C4 = plt.contour(XV1, zh[:, yy, x1:x2]/1000.0, DATA.THETA.values[:, yy, x1:x2], colors='k', levels=levels_th, axis=AX, linewidth=0.6)\n",
    "    #plt.clabel(C4, inline=1, fontsize=14, fmt='%3.0f')\n",
    "\n",
    "        \n",
    "    if (isinstance(LEVELS_VAR2, np.ndarray)):\n",
    "        C2 = AX.contour(YV1, z_3D_2D_slice, VAR2_XR[:, y1:y2, xx],\n",
    "                             levels=LEVELS_VAR2, colors=VAR2_COLOR, linewidths=1., linestyles=\"-\")\n",
    "    else:\n",
    "        C2 = AX.contour(YV1,z_3D_2D_slice, VAR2_XR[:, y1:y2, xx],\n",
    "                             colors='k', linewidths=1., linestyles=\"--\")\n",
    "    AX.clabel(C2, inline=1, fontsize=10, fmt='%3.0f')\n",
    "\n",
    "\n",
    "    if VAR3=='RTP-RV_g/kg':\n",
    "        total_condensate_meridional = DATA[\"RTP\"][:, y1:y2, xx]*1000.0 - DATA[\"RV\"][:, y1:y2, xx]*1000.0\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(YV1, z_3D_2D_slice, np.absolute(total_condensate_meridional),\\\n",
    "                                levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    elif VAR3=='RTP-RV_g/m3':\n",
    "        th = DATA['THETA'][:, y1:y2, xx]\n",
    "        pi = DATA['PI'][:, y1:y2, xx]\n",
    "        rv = DATA['RV'][:, y1:y2, xx]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        total_condensate_meridional = (DATA[\"RTP\"][:, y1:y2, xx]*1000.0 - DATA[\"RV\"][:, y1:y2, xx]*1000.0)*dens\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(YV1, z_3D_2D_slice, np.absolute(total_condensate_meridional),\\\n",
    "                                levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")\n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    elif VAR3=='precipitating_condensate_g/m3':\n",
    "        th = DATA['THETA'][:, y1:y2, xx]\n",
    "        pi = DATA['PI'][:, y1:y2, xx]\n",
    "        rv = DATA['RV'][:, y1:y2, xx]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        total_condensate_meridional = (DATA[\"RTP\"][:, y1:y2, xx]*1000.0 - DATA[\"RV\"] [:, y1:y2, xx]*1000.0 \\\n",
    "                                                                        - DATA[\"RCP\"][:, y1:y2, xx]*1000.0 \\\n",
    "                                                                        - DATA[\"RPP\"][:, y1:y2, xx]*1000.0)*dens\n",
    "        \n",
    "        \n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(YV1, z_3D_2D_slice, np.absolute(total_condensate_meridional),\\\n",
    "                                levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")\n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print('please provide correct value of VAR3')\n",
    " \n",
    "        #del total_condensate_meridional\n",
    "           \n",
    "#   if VAR4:\n",
    "#         var4 = DATA.variables[VAR4]\n",
    "#         if (isinstance(LEVELS_VAR4, np.ndarray)):\n",
    "#             C4 = AX.contour(YV1, z_3D_2D_slice/1000.0, var4[:, y1:y2, xx]*1000.,\n",
    "#                             levels=LEVELS_VAR4, colors=VAR4_COLOR, linewidths=2.0, linestyles=\":\")  # , linewidths=0.85,linestyles=\"--\")#hatches=[None,None,'.','/']\n",
    "#         else:\n",
    "#             print('please provide levels!')\n",
    "            \n",
    "#     if PLOT_WINDS:\n",
    "#         winds_thin_x = 4\n",
    "#         winds_thin_z = 4\n",
    "#         YVwind, ZVwind = np.meshgrid(yh[y1:y2:winds_thin_x], z[::winds_thin_z])\n",
    "#         v1 = DATA.variables[\"VP\"][::winds_thin_z,y1:y2:winds_thin_x, xx]*1.94384\n",
    "#         w1 = DATA.variables[\"WP\"][::winds_thin_z,y1:y2:winds_thin_x, xx]*1.94384\n",
    "#         #QV1 = AX.barbs(YVwind, zh[::winds_thin_z,y1:y2:winds_thin_x, xx]/1000.0,\n",
    "#         #               v1, w1, length=7.2, pivot='middle', linewidth=0.60, flip_barb=True)\n",
    "          \n",
    "\n",
    "    C4zs = AX.plot(Y_1D[y1:y2], TERR[y1:y2, xx]/1000., color='sienna', linewidth=3.6)\n",
    "    # if DATA.variables['zs']:\n",
    "    #    C5 = plot(xh[x1:x2],zs[0,yy,x1:x2]/1000.0, color = \"black\") # Plot topography\n",
    "\n",
    "    title = 'Vertical cross-section (meridional) of $w$'\n",
    "    prodid = 'tests'+'_'+VAR1+'_th_vcross_zonal_'+'xx' + \\\n",
    "        str(xx)+'_y1_'+str(y1)+'_y2_'+str(y2)+'.png'\n",
    "    units = 'm/s'#var1.attrs['units']  # '$ x 10^{-5} $'+\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    # the x coords of this transformation are axes, and the\n",
    "    # y coord are data\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "    #        verticalalignment='top', bbox=props)\n",
    "    AX.text(0.55, 0.94, EXP_LABEL, fontsize=12,\n",
    "            verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "    \n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "        #        verticalalignment='top', bbox=props)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "        \n",
    "    # text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n",
    "    #  verticalalignment='center', transform=ax.transAxes\n",
    "    \n",
    "    AX.scatter(YPOS*DXY/1000.0,TERR[YPOS,XPOS]/1000.+ZPOS/1000.,marker='+',color='k',s=130.5)\n",
    "    \n",
    "    \n",
    "    # Draw box that is used for averaging of hydrometeor conc. for filtering:\n",
    "    if CELL_DIM is not None:\n",
    "        low_y_km  = np.max([(YPOS - (CELL_DIM/2.)),0.0])*DXY/1000.0\n",
    "        box_width_km = CELL_DIM*DXY/1000.0\n",
    "        low_z_grid  = int(np.max([ZPOS_GRID - (CELL_DIM / 2.),0]))\n",
    "        high_z_grid = int(np.min([ZPOS_GRID + (CELL_DIM / 2.),NUM_MODEL_LEVS]))\n",
    "        box_height_km = (Z_1D[high_z_grid] - Z_1D[low_z_grid])/1000.0\n",
    "        low_z_km   = (TERR[YPOS,XPOS] + Z_1D[low_z_grid])/1000.0\n",
    "        rect1  =  AX.add_patch(Rectangle((low_y_km,low_z_km), box_width_km, box_height_km , color='green', fc = 'none',lw = 1.8))\n",
    "\n",
    "    #AX.scatter(xh[x1:x2], zh[:, yy, x1:x2]/1000.0[var1[:, yy, x1:x2].idmax(dim='phony_dim_3')],marker='o',color='k',s=10.5)\n",
    "\n",
    "    fig_process_vert(AX, TERR, C1, Y_1D, y1, y2, xx, PLOT_CBAR, 0, 15, title, TITLETIME, FILENAMETIME, prodid, units, \"meridional\", HEIGHT, False, PANEL_PLOT)\n",
    " \n",
    "def plot_plan_view_cell(DATA, TERR, X_1D, Y_1D, ZM, XPOS, YPOS, ZPOS, DXY, VAR1, LEVELS_VAR1, SINGLE_LEVEL_VAR1, CMAP_VAR1,x1,x2, y1, y2,\n",
    "                    AX, PLOT_CBAR, EXP_LABEL, PANEL_LABEL,  TITLETIME, FILENAMETIME, XPOS_FEATURES=None,YPOS_FEATURES=None):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "    XH1, YH1 = np.meshgrid(X_1D[x1:x2]/1000.0,Y_1D[y1:y2]/1000.0)\n",
    "    vert_lev = np.argmin(np.abs(ZM-ZPOS))\n",
    "    var_to_plotted=DATA.variables[VAR1][vert_lev,y1:y2,x1:x2].values\n",
    "    C111   = AX.contourf(XH1 ,YH1,var_to_plotted ,levels=LEVELS_VAR1,cmap= CMAP_VAR1,extend='both')#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "    C112   = AX.contour(XH1 ,YH1,var_to_plotted ,levels=SINGLE_LEVEL_VAR1,colors='k')#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "    AX.clabel(C112, inline=1, fontsize=10, fmt='%3.0f')\n",
    "    #C_terr = AX.contour (XH1, YH1, TERR[y1:y2,x1:x2], np.array([500.]),linewidths=1.4,colors=\"saddlebrown\")\n",
    "\n",
    "    if XPOS_FEATURES is not None:\n",
    "        tobac_features_scatter1 = AX.scatter(XPOS_FEATURES*DXY/1000.0,YPOS_FEATURES*DXY/1000.0,marker='^',s=100.5,c='green')#facecolors='none', edgecolors='green')\n",
    "        \n",
    "        \n",
    "    tobac_features_scatter = AX.scatter(XPOS*DXY/1000.0,YPOS*DXY/1000.0,marker='+',s=130.5,c='k')\n",
    "    \n",
    "    #plt.colorbar(C111,shrink=0.7, pad=0.02,fraction=0.11)\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    # the x coords of this transformation are axes, and the\n",
    "    # y coord are data\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "    #        verticalalignment='top', bbox=props)\n",
    "    if EXP_LABEL:\n",
    "        AX.text(0.55, 0.94, EXP_LABEL, fontsize=12,\n",
    "                verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "\n",
    "    AX.set_title('Vertical velocity at model level '+str(vert_lev)+'\\n'+TITLETIME,fontsize=16)\n",
    "    AX.set_xlabel('x-distance (km)',fontsize=16)\n",
    "    AX.set_ylabel('y-distance (km)',fontsize=16)\n",
    "    \n",
    "    if PLOT_CBAR:\n",
    "        print('not plotting colorbar for the plan view')\n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        # AX.text(0.85, 13, EXP_LABEL, transform=trans,fontsize=26,\n",
    "        #        verticalalignment='top', bbox=props)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "        \n",
    "    return C111\n",
    "\n",
    "def plot_vert_zonal_meridional_crosssection_tobac(DOMAIN,TOBAC_DF,CELL_NO,XH,YH,ZZ_M,ZM,TERR,DXY,CMAP,OUTPUT_DIR):\n",
    "        tdata_neu=TOBAC_DF[TOBAC_DF['cell']==CELL_NO]\n",
    "        #print('tracking cell# ',cell_no, '(',str(jj+1), 'cell out of 20)')\n",
    "        print('this cell has '+str(len(tdata_neu))+' time steps')\n",
    "        xpos=tdata_neu.X.values.astype(int)\n",
    "        ypos=tdata_neu.Y.values.astype(int)\n",
    "        zpos=tdata_neu.zmn.values.astype(int)\n",
    "        zpos_grid=tdata_neu.vdim.values.astype(int)\n",
    "        #num=tdata_neu.num.values.astype(int)\n",
    "        times_tracked=tdata_neu.timestr.values\n",
    "        thresholds=tdata_neu.threshold_value.values\n",
    "        print(times_tracked)\n",
    "\n",
    "        ii = 0 \n",
    "\n",
    "        for tim in times_tracked:\n",
    "            print('timestep '+str(ii)+': '+tim)\n",
    "            tim_pd = pd.to_datetime(tim)\n",
    "            rams_fil='/nobackup/pmarines/DATA_FM/'+DOMAIN+'/LES_data/a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5'\n",
    "            print('RAMS date file: ',rams_fil)\n",
    "            rams_fil_da=xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "            titletime= get_time_from_RAMS_file(rams_fil)[0]\n",
    "            filenametime= get_time_from_RAMS_file(rams_fil)[1]\n",
    "            wpp = rams_fil_da[\"WP\"]\n",
    "            #print('size of total condensate is ',sys.getsizeof(total_condensate)/1000000)\n",
    "            fig = plt.figure(figsize=(11, 11), frameon=False)  # (16,11)\n",
    "            #ax1=plt.gca()\n",
    "            ax1 = plt.subplot(1, 2, 1)\n",
    "            ax2 = plt.subplot(1, 2, 2)\n",
    "            print('threshold for identification of updraft for this cell is : ',thresholds[0])\n",
    "            plot_zonal_vertcross     (rams_fil_da, TERR, XH, ZZ_M, ZM, DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                       wpp,np.array([thresholds[0]]), 'k',\n",
    "                                       'precipitating_condensate_g/m3', np.array([0.05]), 'purple',\n",
    "                                       None, np.array([5]), 'green',\n",
    "                                       ypos[ii], min(xpos)-50, max(xpos)+50 ,\n",
    "                                       False, False, 16.0, ax1,'cell#'+str(CELL_NO)+'\\nxpos:'+str(xpos[ii])+' gr pt'+'\\nypos:'+str(ypos[ii])+' gr pt'+'\\nzpos:'+str(zpos[ii])+' m','(a)',xpos[ii],zpos[ii],titletime, filenametime)\n",
    "            \n",
    "            \n",
    "            plot_meridional_vertcross(rams_fil_da, TERR, YH, ZZ_M, ZM,DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                      wpp,np.array([thresholds[0]]), 'k',\n",
    "                                      'precipitating_condensate_g/m3', np.array([0.05]), 'purple',\n",
    "                                      None, np.array([5]), 'green',\n",
    "                                      xpos[ii], min(ypos)-50, max(ypos)+50 ,\n",
    "                                      False, False, 16.0, ax2,'cell#'+str(CELL_NO)+'\\nxpos:'+str(xpos[ii])+' gr pt'+'\\nypos:'+str(ypos[ii])+' gr pt'+'\\nzpos:'+str(zpos[ii])+' m','(b)',ypos[ii],zpos[ii],titletime,filenametime)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            #png_file='vert_cross_cell_num'+str(cell_no)+'_xpos'+str(xpos[ii])+'_ypos'+str(ypos[ii])+'_zpos'+str(zpos[ii])+'_timestep'+tim_pd.strftime(\"%Y%m%d%H%M%S\")+'.png'\n",
    "            png_file=OUTPUT_DIR+'vert_cross_'+DOMAIN+'_eq_UD_thres-1-2-5-10-20'+'_cellno'+str(CELL_NO)+'_timestep'+tim_pd.strftime(\"%Y%m%d%H%M%S\")+'.png'\n",
    "            print(png_file)\n",
    "            plt.savefig(png_file,dpi=150)\n",
    "            #plt.close()\n",
    "            ii = ii + 1\n",
    "        print('============================================\\n\\n')  \n",
    "    \n",
    "def plot_vert_zonal_meridional_crosssection_plan_view_tobac(DOMAIN,TOBAC_DF,CELL_NO,XH,YH,ZZ_M,ZM,TERR,DXY,CMAP,OUTPUT_DIR,EXPERIMENT_MARKER,TOBAC_FEATURES_DF=None):\n",
    "        tdata_neu=TOBAC_DF[TOBAC_DF['cell']==CELL_NO]\n",
    "        #print('tracking cell# ',cell_no, '(',str(jj+1), 'cell out of 20)')\n",
    "        print('this cell has '+str(len(tdata_neu))+' time steps')\n",
    "        xpos=list(tdata_neu.X.values.astype(int))\n",
    "        ypos=list(tdata_neu.Y.values.astype(int))\n",
    "        zpos=list(tdata_neu.zmn.values.astype(int))    \n",
    "        zpos_grid=list(tdata_neu.vdim.values.astype(int))\n",
    "        cell_dim =list(np.array(tdata_neu.num.values.astype(int))**(1/3))\n",
    "        times_tracked=tdata_neu.timestr.values\n",
    "        times_tracked_pd = pd.to_datetime(times_tracked)\n",
    "        thresholds=tdata_neu.threshold_value.values\n",
    "        cell_labels=['cell#'+str(CELL_NO)+'\\nxpos:'+str(xpos[kk])+' gr pt'+'\\nypos:'+str(ypos[kk])+' gr pt'+'\\nzpos:'+str(zpos[kk])+' m' for kk in range(len(xpos))]\n",
    "\n",
    "#         print('original xpos: ',xpos)\n",
    "#         print('type of xpos: ',type(xpos))\n",
    "#         print('original times_tracked: ',times_tracked_pd)\n",
    "#         print('earliest time tracked for all cells = ',min(pd.to_datetime(TOBAC_DF.timestr.values)))\n",
    "#         print('last time tracked for all cells = ',max(pd.to_datetime(TOBAC_DF.timestr.values)))\n",
    "#         print('original cell labels: ',cell_labels)\n",
    "        \n",
    "        if ((min(times_tracked_pd)-min(pd.to_datetime(TOBAC_DF.timestr.values))).total_seconds() >= 120.0):\n",
    "            preinit_times=pd.date_range(end=min(times_tracked_pd), periods=5, freq='30S')[:-1]\n",
    "            times_tracked_pd=preinit_times.union(times_tracked_pd)\n",
    "            xpos=list([xpos[0],xpos[0],xpos[0],xpos[0]])+xpos\n",
    "            ypos=list([ypos[0],ypos[0],ypos[0],ypos[0]])+ypos\n",
    "            zpos=list([zpos[0],zpos[0],zpos[0],zpos[0]])+zpos\n",
    "            zpos_grid=list([zpos_grid[0],zpos_grid[0],zpos_grid[0],zpos_grid[0]])+zpos_grid\n",
    "            cell_dim=list([cell_dim[0],cell_dim[0],cell_dim[0],cell_dim[0]])+cell_dim\n",
    "            #print(xpos)\n",
    "            cell_labels=['Cell not yet detected' for kk in range(4)] + cell_labels\n",
    "                                                \n",
    "        if ((max(pd.to_datetime(TOBAC_DF.timestr.values))-max(times_tracked_pd)).total_seconds() >= 120.0):\n",
    "            post_times=pd.date_range(start=max(times_tracked_pd), periods=5, freq='30S')[1:]\n",
    "            times_tracked_pd=times_tracked_pd.append(post_times)\n",
    "            xpos = xpos + list([xpos[-1],xpos[-1],xpos[-1],xpos[-1]])\n",
    "            ypos = ypos + list([ypos[-1],ypos[-1],ypos[-1],ypos[-1]])\n",
    "            zpos = zpos + list([zpos[-1],zpos[-1],zpos[-1],zpos[-1]])\n",
    "            zpos_grid=zpos_grid + list([zpos_grid[-1],zpos_grid[-1],zpos_grid[-1],zpos_grid[-1]])\n",
    "            cell_dim= cell_dim + list([cell_dim[-1],cell_dim[-1],cell_dim[-1],cell_dim[-1]])\n",
    "            cell_labels= cell_labels + ['Cell tracking over' for kk in range(4)]\n",
    "            \n",
    "        #print('After times are added: ',times_tracked_pd)\n",
    "        #print('processed xpos: ',xpos)\n",
    "        #print('processed cell labels: ',cell_labels)\n",
    "        \n",
    "        original_cell_labels=['cell#'+str(CELL_NO)+'\\nxpos:'+str(xpos[kk])+' gr pt'+'\\nypos:'+str(ypos[kk])+' gr pt'+'\\nzpos:'+str(zpos[kk])+' m' for kk in range(len(xpos))]\n",
    "        \n",
    "\n",
    "        ii = 0 \n",
    "\n",
    "        for tim_pd in times_tracked_pd:\n",
    "            print('timestep '+str(ii)+': '+tim_pd.strftime(\"%Y-%m-%d-%H:%M:%S\"))\n",
    "            #tim_pd = pd.to_datetime(tim)\n",
    "            rams_fil='/nobackup/pmarines/DATA_FM/'+DOMAIN+'/LES_data/a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5'\n",
    "            print('RAMS date file: ',rams_fil)\n",
    "            rams_fil_da=xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "            titletime= get_time_from_RAMS_file(rams_fil)[0]\n",
    "            filenametime= get_time_from_RAMS_file(rams_fil)[1]\n",
    "            wpp = rams_fil_da[\"WP\"]\n",
    "            \n",
    "            \n",
    "            if TOBAC_FEATURES_DF is not None:\n",
    "                # plot features that are wiithin 1 km of the vertical level of the cell that is being plotted\n",
    "                tdata_feat=TOBAC_FEATURES_DF[(TOBAC_FEATURES_DF['time']==tim_pd)   & (abs(TOBAC_FEATURES_DF['zmn']-zpos[ii])<=1000.) & \\\n",
    "                                             (TOBAC_FEATURES_DF['X']>=xpos[ii]-50) & (TOBAC_FEATURES_DF['X']<=xpos[ii]+50)          & \\\n",
    "                                             (TOBAC_FEATURES_DF['Y']>=ypos[ii]-50) & (TOBAC_FEATURES_DF['Y']<=ypos[ii]+50)]\n",
    "                                            \n",
    "                xpos_feat=np.array((tdata_feat.X.values.astype(int)))\n",
    "                ypos_feat=np.array((tdata_feat.Y.values.astype(int)))\n",
    "            else:\n",
    "                xpos_feat=None\n",
    "                ypos_feat=None\n",
    "            \n",
    "            \n",
    "            #print('size of total condensate is ',sys.getsizeof(total_condensate)/1000000)\n",
    "            fig = plt.figure(figsize=(15, 9), frameon=False)  # (16,11)\n",
    "            #ax1=plt.gca()\n",
    "            ax1 = plt.subplot(1, 3, 1)\n",
    "            ax1.set_aspect('equal', adjustable='box')\n",
    "            ax2 = plt.subplot(1, 3, 2)\n",
    "            #ax2.set_aspect('equal', adjustable='box')\n",
    "            ax3 = plt.subplot(1, 3, 3)\n",
    "            #ax3.set_aspect('equal', adjustable='box')\n",
    "            print('threshold for identification of updraft for this cell is : ',thresholds[0])\n",
    "            \n",
    "            cbar_conts = plot_plan_view_cell      (rams_fil_da, TERR, XH, YH, ZM, xpos[ii], ypos[ii], zpos[ii], DXY, 'WP', np.arange(-20.,20.1,.1),[thresholds[0]], CMAP, min(xpos)-50, max(xpos)+50,  min(ypos)-50,  max(ypos)+50,\n",
    "                                      ax1, False,cell_labels[ii], '(a)'  ,titletime, filenametime,xpos_feat,ypos_feat)\n",
    "            print('plan view done')\n",
    "            plot_zonal_vertcross     (rams_fil_da, TERR, XH, ZZ_M, ZM, DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                       wpp,np.array([thresholds[0]]), 'k',\n",
    "                                       'precipitating_condensate_g/m3', np.array([0.05]), 'purple',\n",
    "                                       None, np.array([5]), 'green',\n",
    "                                       ypos[ii], min(xpos)-50, max(xpos)+50 ,\n",
    "                                       False, False, 15.0, ax2,False,cell_labels[ii],'(b)',xpos[ii],ypos[ii],zpos[ii],zpos_grid[ii],cell_dim[ii], 231, titletime,filenametime)\n",
    "            print('vertical zonal cross-section done')\n",
    "         \n",
    "            \n",
    "            plot_meridional_vertcross(rams_fil_da, TERR, YH, ZZ_M, ZM,DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                      wpp,np.array([thresholds[0]]), 'k',\n",
    "                                      'precipitating_condensate_g/m3', np.array([0.05]), 'purple',\n",
    "                                      None, np.array([5]), 'green',\n",
    "                                      xpos[ii], min(ypos)-50, max(ypos)+50 ,\n",
    "                                      False, False, 15.0, ax3,False,cell_labels[ii],'(c)',xpos[ii],ypos[ii],zpos[ii],zpos_grid[ii],cell_dim[ii], 231,titletime,filenametime)\n",
    "            print('vertical meridional cross-section done')\n",
    "            \n",
    "            cb_ax = fig.add_axes([0.2, 0.0001, 0.6, 0.02])  # two panels\n",
    "            #[left, bottom, width, height]\n",
    "            cbar  = fig.colorbar(cbar_conts, cax=cb_ax, orientation = 'horizontal')\n",
    "            #cb = fig.colorbar(cbar_conts, ax=(ax1, ax2,ax3), orientation='horizontal')\n",
    "            plt.tight_layout()\n",
    "            #png_file='vert_cross_cell_num'+str(cell_no)+'_xpos'+str(xpos[ii])+'_ypos'+str(ypos[ii])+'_zpos'+str(zpos[ii])+'_timestep'+tim_pd.strftime(\"%Y%m%d%H%M%S\")+'.png'\n",
    "            png_file=OUTPUT_DIR+'three_panel_'+DOMAIN+'_'+EXPERIMENT_MARKER+'_UD_thres-1-2-5-10-20'+'_cellno'+str(CELL_NO)+'_timestep'+tim_pd.strftime(\"%Y%m%d%H%M%S\")+'.png'\n",
    "            print(png_file)\n",
    "            plt.savefig(png_file,dpi=150)\n",
    "            #plt.close()\n",
    "            ii = ii + 1\n",
    "        print('============================================\\n\\n')\n",
    "\n",
    "        \n",
    "############################### FIRST OF ALL ################################\n",
    "cpu_count1 = cpu_count()\n",
    "print('number of cpus: ',cpu_count1)\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "# Paths to model data and where to save data\n",
    "\n",
    "domain='DRC1.1-R'\n",
    "#domain='PHI1.1-R'\n",
    "#domain='PHI2.1-R'\n",
    "#domain='DRC1.1-R'\n",
    "\n",
    "\n",
    "\n",
    "#tobac_tracking_dirpath = '/nobackupp11/isingh2/tobac_tracking-main/' # Pleiades\n",
    "tobac_tracking_dirpath='/Users/isingh/SVH/INCUS/jupyter_nbks/tobac_thermals/peter_tobac_output/'+domain+'/'# personal macbook\n",
    "#tobac_features_dirpath = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/OLD/' # Pleiades\n",
    "\n",
    "#les_path = '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/'\n",
    "les_path = '/Users/isingh/SVH/INCUS/sample_LES_data/'+domain+'/' # personal macbook\n",
    "\n",
    "tobac_tracking_filename  = 'comb_filt_track_01_02_05_10_20_ARG1.1-R.p'\n",
    "#tobac_features_filename  = 'combined_df_1_2_5_10_20_nmin64.p'\n",
    "\n",
    "tobac_tracking_filepath  = tobac_tracking_dirpath+tobac_tracking_filename\n",
    "#tobac_features_filepath  = tobac_features_dirpath+tobac_features_filename\n",
    "\n",
    "# Grab all the rams files\n",
    "h5filepath = les_path+'a-L*g3.h5'\n",
    "h5files1 = sorted(glob.glob(h5filepath))\n",
    "hefilepath = les_path+'a-L*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "#print(h5files1)\n",
    "start_time=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "end_time=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "print('starting time in simulations: ',start_time)\n",
    "print('ending time in simulations: ',end_time)\n",
    "\n",
    "\n",
    "#### read in RAMS data file to get parameters for plotting ####\n",
    "rams_terr=xr.open_dataset(h5files1[0],engine='h5netcdf', phony_dims='sort').TOPT.values\n",
    "#rams_lats=ds.GLAT.values #\n",
    "#rams_lons=ds.GLON.values\n",
    "\n",
    "\n",
    "zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],h5files1[0])\n",
    "\n",
    "xh=np.arange(dxy/2,nx*dxy,dxy)\n",
    "yh=np.arange(dxy/2,ny*dxy,dxy)\n",
    "\n",
    "#zt_msl_3D    =  rams_terr[:,:] + zt[:,np.newaxis,np.newaxis]\n",
    "#zm_msl_3D    =  rams_terr[:,:] + zm[:,np.newaxis,np.newaxis]\n",
    "\n",
    "#thres_list=[1.0,2.0,5.0,10.0,20.0]#,30.0,40.0,50.0]\n",
    "\n",
    "##### read in tobac data #####\n",
    "print('reading tracking file: ',tobac_tracking_filepath)\n",
    "tdata =          pd.read_pickle(tobac_tracking_filepath)\n",
    "\n",
    "print('reading features file',tobac_features_filepath)\n",
    "tdata_features = pd.read_pickle(tobac_features_filepath)\n",
    "\n",
    "print('number of unique cells identified: ',len(tdata.cell.unique()))\n",
    "all_cells=np.array(tdata.cell.unique())\n",
    "\n",
    "\n",
    "cl = random.choice(all_cells)\n",
    "print('cell number = ',cl)\n",
    "plot_vert_zonal_meridional_crosssection_plan_view_tobac(domain,tdata,cl,xh,yh,None,zm,rams_terr,dxy,plt.get_cmap('bwr'),'/Users/isingh/SVH/SVH_paper1/scratch/','precip_condensate_localized_makebox_other_features_incl_trackpy_changes',False)\n",
    "# argument = []\n",
    "# for __ in range(100):\n",
    "#     cl = random.choice(all_cells)\n",
    "#     #for cl in all_cells:\n",
    "#     argument = argument + [(domain,tdata,cl,xh,yh,None,zm,rams_terr,dxy,plt.get_cmap('bwr'),'/home3/isingh2/nobackup/tobac_plots/','precip_condensate_localized_makebox_other_features_incl_trackpy_changes',tdata_features)]\n",
    "\n",
    "# #print(argument)\n",
    "# print('will make plots for ',len(argument),' cells')\n",
    "# #print('first argument is ',argument[0])\n",
    "\n",
    "\n",
    "# def main(FUNCTION, ARGUMENT):\n",
    "#     pool = Pool(cpu_count1-1)\n",
    "#     #pool = Pool(1)\n",
    "#     start_time = time.perf_counter()\n",
    "#     results = pool.starmap(FUNCTION, ARGUMENT)\n",
    "#     finish_time = time.perf_counter()\n",
    "#     print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(plot_vert_zonal_meridional_crosssection_plan_view_tobac, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d37cf",
   "metadata": {},
   "source": [
    "# WRF Python cross-section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(xpos)-12):\n",
    "    print('-----')\n",
    "    print('time : ',times_tracked[ii])\n",
    "    print('xpos : ',xpos[ii])\n",
    "    print('ypos : ',ypos[ii])\n",
    "    print('zpos : ',zpos[ii])\n",
    "    print('-----')\n",
    "    cross_start = CoordPair(x=int(xpos[ii])-100, y=int(ypos[ii]))\n",
    "    cross_end = CoordPair(x=int(xpos[ii])+100, y=int(ypos[ii]))\n",
    "    #z_cross = zz_cross.copy()\n",
    "    #z_cross_values=z_cross_filled.data\n",
    "\n",
    "    zz_cross = vertcross(var1,vert=ZZ_T,levels=None,\\\n",
    "                        wrfin=None,timeidx=None,stagger=None,\\\n",
    "                        projection=None,ll_point=None,pivot_point=None,\\\n",
    "                        angle=None,start_point=cross_start,end_point=cross_end,\\\n",
    "                        latlon=False, meta=True)\n",
    "\n",
    "    #zz_cross_th = vertcross(theta,vert=ZZ_M,levels=None,\\\n",
    "    #                    wrfin=None,timeidx=None,stagger=None,\\\n",
    "    #                    projection=None,ll_point=None,pivot_point=piv,\\\n",
    "    #                    angle=None,start_point=cross_start,end_point=cross_end,\\\n",
    "    #                    latlon=False, meta=True)\n",
    "\n",
    "    ter_line = interpline(ter, wrfin=None, start_point=cross_start,end_point=cross_end)#,pivot_point=piv, angle=90)#)\n",
    "\n",
    "    fig = plt.figure(figsize=(15,11))\n",
    "    ax_cross = plt.axes()\n",
    "    xs = np.arange(0, zz_cross.shape[-1], 1)\n",
    "    ys = to_np(zz_cross.coords[\"vertical\"])\n",
    "    contours = ax_cross.contourf(xs,ys,to_np(zz_cross),cmap=cma1,extend=\"both\",levels=np.arange(-30,31,1))\n",
    "    #contours_th = ax_cross.contour(xs,ys,to_np(zz_cross_th),extend=\"both\",levels=np.arange(290,600,5),colors='k')\n",
    "    #ax_cross.clabel(contours_th, inline=1, fontsize=10, fmt=\"%i\")\n",
    "    # Add the color bar\n",
    "    cb = fig.colorbar(contours, ax=ax_cross)\n",
    "    cb.ax.tick_params(labelsize=8)\n",
    "\n",
    "    # Fill in the mountain area\n",
    "    ax_cross.fill_between(xs, 0, ter_line.values[:],facecolor='wheat')\n",
    "    ax_cross.plot(xs,ter_line.values[:], color = \"black\",linewidth=1.0) # Plot topography\n",
    "    #ht_fill = ax_cross.fill_between(xs, 0, to_np(ter_line),facecolor=\"saddlebrown\")\n",
    "    \n",
    "    ax_cross.scatter(xpos[ii],zpos[ii],marker=\"+\",color='red')\n",
    "\n",
    "    # Set the x-ticks to use latitude and longitude labels\n",
    "    coord_pairs = to_np(zz_cross.coords[\"xy_loc\"])\n",
    "    x_ticks = np.arange(coord_pairs.shape[0])\n",
    "    x_labels = [pair.xy_str() for pair in to_np(coord_pairs)]\n",
    "\n",
    "    # Set the desired number of x ticks below\n",
    "    num_ticks = 5\n",
    "    thin = int((len(x_ticks) / num_ticks) + .5)\n",
    "    ax_cross.set_title('Vertical cross-section of vertical velocity (m/s)\\n'+times_tracked[ii])\n",
    "    \n",
    "    ax_cross.text(0.89, 0.89, 'Cell#'+str(cell_no)+'\\nX='+str(xpos[ii])+'\\nY='+str(ypos[ii])+'\\nZ='+str(zpos[ii]), horizontalalignment='center',\n",
    "     verticalalignment='center', transform=ax_cross.transAxes,bbox=dict(edgecolor='k', alpha=0.5))\n",
    "    \n",
    "    \n",
    "    ax_cross.set_xticks(x_ticks[::thin])\n",
    "    ax_cross.set_xticklabels(x_labels[::thin], rotation=45, fontsize=8)\n",
    "    ax_cross.set_ylim([0,19000])\n",
    "    # Set the x-axis and  y-axis labels\n",
    "    ax_cross.set_xlabel(\"x, y\", fontsize=12)\n",
    "    ax_cross.set_ylabel(\"Height (m)\", fontsize=12)\n",
    "    filename='vert_cross_tracking_ARG_test_cell'+str(cell_no)+'_xpos'+str(xpos[ii])+'_ypos'+str(ypos[ii])+'.png'\n",
    "    print(filename)\n",
    "    plt.savefig(filename,dpi=150)\n",
    "    print('############################\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes, mark_inset, InsetPosition\n",
    "\n",
    "\n",
    "# Get the WRF variables\n",
    "def plot_vert_cross(FILE,ZZ,PT1,PT2,CMAP,\\\n",
    "                    VARNAME1,LEVELS_VAR1,\\\n",
    "                    VARNAME2,LEVELS_VAR2,\\\n",
    "                    VARNAME3,LEVELS_VAR3,\\\n",
    "                    VARNAME4,LEVELS_VAR4,\\\n",
    "                    VERT_VORTICITY,Q_HYDROMETEORS,LEVELS_HYD_G_KG,QCLOUD,Q_COLOR,\\\n",
    "                    SHADE_TERRAIN,AX,\\\n",
    "                    CREATE_XLABELS, CREATE_YLABELS, CREATE_COLORBAR, CREATE_TITLE,\\\n",
    "                    SUPTITLE,PANEL_TEXT,\\\n",
    "                    INSET_AXIS):\n",
    "    \n",
    "     # Open the NetCDF file\n",
    "    nc_file = xr.open_dataset(FILE)\n",
    "    cur_time = os.path.split(h5files1[i])[1][4:21] # Grab time string from RAMS file\n",
    "    print(cur_time)\n",
    "    rams_times = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    print(rams_times)\n",
    "    \n",
    "    tiempo = np.datetime_as_string(rams_times.values, timezone='UTC',unit='m')\n",
    "\n",
    "    ter     =  nc_file['TOPT']\n",
    "    ZZ_T    =  ter + zt[:,np.newaxis,np.newaxis]\n",
    "    ZZ_M    =  ter + zm[:,np.newaxis,np.newaxis]\n",
    "    \n",
    "    if VARNAME1:\n",
    "        var1 =  nc_file[VARNAME1]\n",
    "    if VARNAME2:\n",
    "        var2 =  nc_file[VARNAME2]\n",
    "    if VARNAME3:\n",
    "        var3 =  nc_file[VARNAME3]\n",
    "    if VARNAME4:\n",
    "        var4 =  nc_file[VARNAME4]\n",
    "    if VERT_VORTICITY:\n",
    "        print('vert vorticity not available')\n",
    "        #vort =  (getvar(nc_file, \"avo\")*(10**-5) - getvar(nc_file, \"F\"))\n",
    "    if Q_HYDROMETEORS:\n",
    "        qhyd =  (nc_file['QCLOUD']+nc_file['QCLOUD']+nc_file['QCLOUD']+nc_file['QCLOUD'])*1000.\n",
    "    if QCLOUD:\n",
    "        qclo =  nc_file['QCLOUD']*1000.0\n",
    "    #if Q_NONPRECIP_HYD:\n",
    "    #    q_nonprec =  getvar(nc_file, \"QCLOUD\")+getvar(nc_file,\"QSNOW\")+getvar(nc_file,\"QICE\")+getvar(nc_file,\"QGRAUP\")\n",
    "\n",
    "    #print(var1)\n",
    "\n",
    "    # Set the start point and end point for the cross section\n",
    "    start_point = CoordPair(lat=PT1[0], lon=PT1[1])\n",
    "    end_point = CoordPair(lat=PT2[0], lon=PT2[1])\n",
    "\n",
    "    # Compute the vertical cross-section interpolation.  Also, include the\n",
    "    # lat/lon points along the cross-section in the metadata by setting latlon\n",
    "    # to True.\n",
    "\n",
    "    # Get the lat/lon points\n",
    "    lats, lons = latlon_coords(z)\n",
    "\n",
    "    # Make the contour plot for wind speed\n",
    "    if VARNAME1:\n",
    "        var1_cross = vertcross(var1, ZZ_T, wrfin=None, start_point=start_point,\n",
    "                           end_point=end_point, latlon=True, meta=True)\n",
    "        print('min '+VARNAME1+' ',np.min(var1_cross))\n",
    "        print('max '+VARNAME1+' ',np.max(var1_cross))\n",
    "        #print(var1_cross)\n",
    "        #print(np.shape(var1_cross))\n",
    "        xs = np.arange(0, var1_cross.shape[-1], 1)\n",
    "        ys = to_np(var1_cross.coords[\"vertical\"])\n",
    "        if LEVELS_VAR1 is None:\n",
    "            var1_contours = AX.contourf(xs,ys,to_np(var1_cross),cmap=CMAP,extend='both')\n",
    "        else:\n",
    "            var1_contours = AX.contourf(xs,ys,to_np(var1_cross),levels=LEVELS_VAR1,cmap=CMAP,extend='both')\n",
    "    if VARNAME2:\n",
    "        var2_cross = vertcross(var2, ZZ_T, wrfin=nc_file, start_point=start_point,\n",
    "                           end_point=end_point, latlon=True, meta=True)\n",
    "        var2_contours = AX.contour(xs,ys,to_np(var2_cross),levels=LEVELS_VAR2,linestyles='-',colors='k')\n",
    "        AX.clabel(var2_contours, inline=1, fontsize=10,fmt='%.0f')\n",
    "    if VARNAME3:\n",
    "        var3_cross = vertcross(var3, ZZ_T, wrfin=nc_file, start_point=start_point,\n",
    "                           end_point=end_point, latlon=True, meta=True)\n",
    "        var3_contours = AX.contour(xs,ys,to_np(var3_cross), levels=LEVELS_VAR3,linestyles='-',colors='blue')\n",
    "        AX.clabel(var3_contours, inline=1, fontsize=10,fmt='%.3f')\n",
    "    if VARNAME4:\n",
    "        var4_cross = vertcross(var4, ZZ_T, wrfin=nc_file, start_point=start_point,\n",
    "                           end_point=end_point, latlon=True, meta=True)\n",
    "        var4_contours = AX.contour(xs,ys,to_np(var4_cross), levels=LEVELS_VAR4,linestyles='-',colors='magenta')\n",
    "        AX.clabel(var4_contours, inline=1, fontsize=10,fmt='%.0f')\n",
    "        \n",
    "    if VERT_VORTICITY:\n",
    "        vort_cross = vertcross(vort, z, wrfin=nc_file, start_point=start_point,\n",
    "                           end_point=end_point, latlon=True, meta=True)\n",
    "        vort_contours = AX.contour(xs,ys,to_np(vort_cross), levels=np.arange(-0.05,-0.01,0.005),colors='forestgreen',\\\n",
    "                    linestyles='--')\n",
    "        vort_contours = AX.contour(xs,ys,to_np(vort_cross), levels=np.arange(0.01,0.055,0.005),colors='forestgreen',\\\n",
    "                    linestyles='-')   \n",
    "    if Q_HYDROMETEORS:\n",
    "        qhyd_cross = vertcross(qhyd, z, wrfin=nc_file, start_point=start_point,\n",
    "                           end_point=end_point, latlon=True, meta=True)\n",
    "        if LEVELS_HYD_G_KG is None:\n",
    "            qhyd_contours = AX.contour(xs,ys,to_np(qhyd_cross),linestyles='-',colors=Q_COLOR,linewidths=2.5)\n",
    "        else:\n",
    "            qhyd_contours = AX.contour(xs,ys,to_np(qhyd_cross),levels=[LEVELS_HYD_G_KG],linestyles='-',colors=Q_COLOR,linewidths=2.5)\n",
    "        #AX.clabel(qhyd_contours, inline=1, fontsize=10,fmt='%.0f')\n",
    "    if QCLOUD:\n",
    "        qclo_cross = vertcross(qclo, z, wrfin=nc_file, start_point=start_point,\n",
    "                           end_point=end_point, latlon=True, meta=True)\n",
    "        qclo_contours = AX.contour(xs,ys,to_np(qclo_cross)*1000.,linestyles='-',colors=Q_COLOR)\n",
    "    \n",
    "    thin_x=7\n",
    "    thin_y=7\n",
    "    if PT1[1]==PT2[1]:\n",
    "        print('NS')\n",
    "        va   =  getvar(nc_file, \"va\")\n",
    "        wa   =  getvar(nc_file, \"wa\")\n",
    "        va_cross   = vertcross(va, z, wrfin=nc_file, start_point=start_point,\n",
    "                           end_point=end_point, latlon=True, meta=True)\n",
    "        wa_cross   = vertcross(wa, z, wrfin=nc_file, start_point=start_point,\n",
    "                           end_point=end_point, latlon=True, meta=True)\n",
    "        barbs = AX.barbs(xs[::thin_x],ys[::thin_y],va_cross.values[::thin_x,::thin_y],wa_cross.values[::thin_x,::thin_y],\\\n",
    "                           length=6,pivot='middle',flip_barb=True,linewidth=0.9)\n",
    "    elif PT1[0]==PT2[0]:\n",
    "        print('WE')\n",
    "        ua   =  getvar(nc_file, \"ua\")\n",
    "        wa   =  getvar(nc_file, \"wa\")\n",
    "        ua_cross   = vertcross(ua, z, wrfin=nc_file, start_point=start_point,\n",
    "                           end_point=end_point, latlon=True, meta=True)\n",
    "        wa_cross   = vertcross(wa, z, wrfin=nc_file, start_point=start_point,\n",
    "                           end_point=end_point, latlon=True, meta=True)\n",
    "        barbs = AX.barbs(xs[::thin_x],ys[::thin_y],ua_cross[::thin_x,::thin_y],wa_cross[::thin_x,::thin_y],\\\n",
    "                           length=6,pivot='middle',flip_barb=True,linewidth=0.9)\n",
    "    else:\n",
    "        print('SLANTED CROSS-SEC')\n",
    "        print('no barbs')\n",
    "\n",
    "\n",
    "    #AX.clabel(var4_contours, inline=1, fontsize=10,fmt='%.3f')\n",
    "    #AX.clabel(var4_contours, inline=1, fontsize=10,fmt='%.0f')\n",
    "    # Add the color bar\n",
    "    if CREATE_COLORBAR:\n",
    "        cb = plt.colorbar(var1_contours,ax=AX,orientation='horizontal')\n",
    "        cb.ax.tick_params(labelsize=16) \n",
    "    \n",
    "    ##############\n",
    "    if SHADE_TERRAIN:\n",
    "        # Make a copy of the z cross data. Let's use regular numpy arrays for this.\n",
    "        var_cross_filled = np.ma.copy(to_np(var1_cross))\n",
    "\n",
    "        # For each cross section column, find the first index with non-missing\n",
    "        # values and copy these to the missing elements below.\n",
    "        for i in range(var_cross_filled.shape[-1]):\n",
    "            column_vals = var_cross_filled[:,i]\n",
    "            # Let's find the lowest index that isn't filled. The nonzero function\n",
    "            # finds all unmasked values greater than 0. Since 0 is a valid value\n",
    "            # for dBZ, let's change that threshold to be -200 dBZ instead.\n",
    "            first_idx = int(np.transpose((column_vals > -200).nonzero())[0])\n",
    "            var_cross_filled[0:first_idx, i] = var_cross_filled[first_idx, i]\n",
    "\n",
    "        # Get the terrain heights along the cross section line\n",
    "        ter_line = interpline(ter, wrfin=nc_file, start_point=start_point,\n",
    "                              end_point=end_point)\n",
    "        #print(ter_line)\n",
    "        ht_fill = AX.fill_between(xs, 0, to_np(ter_line),facecolor=\"burlywood\")\n",
    "    ####################\n",
    "\n",
    "\n",
    "    # Set the x-ticks to use latitude and longitude labels\n",
    "    if CREATE_XLABELS:\n",
    "        coord_pairs = to_np(var1_cross.coords[\"xy_loc\"])\n",
    "        x_ticks = np.arange(coord_pairs.shape[0])\n",
    "        #x_labels = [pair.latlon_str() for pair in to_np(coord_pairs)]\n",
    "        x_labels = [pair.latlon_str(fmt=\"{:.2f}, {:.2f}\")\n",
    "                for pair in to_np(coord_pairs)]\n",
    "        #print(x_labels)\n",
    "        AX.set_xticks(x_ticks[::10])\n",
    "        AX.set_xticklabels(x_labels[::10], rotation=45,fontsize=9)\n",
    "        AX.set_xlabel(\"Latitude, Longitude\", fontsize=16)\n",
    "    else:\n",
    "        AX.set(xticklabels=[]) \n",
    "        AX.tick_params(bottom=False)  # remove the ticks\n",
    "\n",
    "    # Set the y-ticks to be height\n",
    "    #vert_vals = np.round(to_np(var1_cross.coords[\"vertical\"]),decimals=0)\n",
    "    #print(vert_vals)\n",
    "    #v_ticks = np.arange(vert_vals.shape[0])\n",
    "    #AX.set_yticks(v_ticks[::10])\n",
    "    #AX.set_yticklabels(vert_vals[::10], fontsize=16)\n",
    "    \n",
    "    AX.set_ylim([0,18000])\n",
    "    \n",
    "    if CREATE_YLABELS:\n",
    "        AX.set_ylabel(\"Height (m)\", fontsize=16)\n",
    "    else:\n",
    "        AX.set(xticklabels=[]) \n",
    "        AX.tick_params(left=False)\n",
    "\n",
    "    \n",
    "    title_string = \"vertical cross-section of \"+var1.description.lower()+' ('+var1.units+')\\n'+ tiempo\n",
    "    \n",
    "    if SUPTITLE:\n",
    "        title_string = title_string + ' (' + SUPTITLE + ')'\n",
    "        #plt.suptitle(SUPTITLE, x=0.67,y=0.84, fontsize=8)\n",
    "    # Add a title\n",
    "    if CREATE_TITLE:\n",
    "        AX.set_title(title_string, {\"fontsize\" : 14})\n",
    "\n",
    "    if PANEL_TEXT:\n",
    "        AX.annotate(PANEL_TEXT, xy=(0.04, 0.96), fontsize=12,\n",
    "            xycoords='axes fraction', textcoords='offset points',\n",
    "            bbox=dict(facecolor='white', alpha=0.9, boxstyle='round'),\n",
    "            horizontalalignment='left', verticalalignment='top') \n",
    "    \n",
    "    if INSET_AXIS:\n",
    "        rect = [0.1, 0.15, 0.4, 0.4]\n",
    "        # inset location relative to main plot (ax) in normalized units\n",
    "        inset_x = 1\n",
    "        inset_y = 1\n",
    "        inset_size = 0.2\n",
    "        AX_INSET = plt.axes([0, 0, 1, 1], projection=crs.PlateCarree())\n",
    "        #ax2.add_feature(cfeature.LAND)\n",
    "        #ax2.add_feature(cfeature.OCEAN)\n",
    "        #ax2.add_feature(cfeature.COASTLINE)\n",
    "        #ip = InsetPosition(AX, [inset_x - inset_size,\n",
    "        #                        inset_y - inset_size,\n",
    "        #                        inset_size,\n",
    "        #                        inset_size])\n",
    "        ip = InsetPosition(AX, [0.01,inset_y - inset_size,inset_size,inset_size])\n",
    "        AX_INSET.set_axes_locator(ip)\n",
    "        refl = getvar(nc_file,'dbz')[0,:,:]\n",
    "        p_inset = getvar(nc_file, \"pressure\")\n",
    "        wspd_inset = getvar(nc_file, \"wspd\")\n",
    "        z_inset = getvar(nc_file, \"z\", units=\"m\")\n",
    "        z_prs_lvl_inset = interplevel(z_inset, p_inset,300)\n",
    "        wspd_prs_lvl_inset = interplevel(wspd_inset, p_inset,300)\n",
    "        lats_inset, lons_inset = latlon_coords(p_inset)\n",
    "        #refl_values = refl[0,:,:].values\n",
    "        #refl_values[refl_values < -5.0] = 0.0\n",
    "        #print(np.max(refl))\n",
    "        \n",
    "        C_terr_inset = AX_INSET.contour(lons_inset,lats_inset,ter,transform=crs.PlateCarree(),levels=[1000.0],colors='saddlebrown')#np.arange(1000,7000,2000))\n",
    "        C_refl_inset = AX_INSET.contour(lons_inset,lats_inset,refl.values,transform=crs.PlateCarree(),levels=[35],colors='magenta')\n",
    "        C_hgts_inset = AX_INSET.contour(lons_inset,lats_inset,z_prs_lvl_inset.values,transform=crs.PlateCarree(),levels=np.arange(8000,12000,60),colors='k',linewidths=0.7)#,cmap=cma4)\n",
    "        C_wspd_inset = AX_INSET.contourf(lons_inset,lats_inset,wspd_prs_lvl_inset.values,transform=crs.PlateCarree(),levels=np.arange(40,91,1),cmap=cma4)\n",
    "        #AX_INSET.set_extent([-75, -55, -40, -25], crs=crs.PlateCarree())\n",
    "        AX_INSET.plot([PT1[1],PT2[1]],[PT1[0],PT2[0]],transform=crs.PlateCarree(),color='k')\n",
    "        AX_INSET.coastlines(resolution='110m')\n",
    "        \n",
    "    nc_file.close()\n",
    "    #plt.show()\n",
    "    return var1_contours,tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f7969",
   "metadata": {},
   "source": [
    "# WRF-RAMS comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2722b61",
   "metadata": {},
   "source": [
    "## Integrated total condensate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23130700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel version\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import metpy.calc as metcalc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.ticker import FormatStrFormatter, LinearLocator\n",
    "import nclcmaps as ncm\n",
    "import cartopy.crs as crs\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "from cartopy.mpl.gridliner import LATITUDE_FORMATTER, LONGITUDE_FORMATTER\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "\n",
    "def radar_colormap():\n",
    "    nws_reflectivity_colors = [\n",
    "    \"#646464\", # ND\n",
    "    \"#ccffff\", # -30\n",
    "    \"#cc99cc\", # -25\n",
    "    \"#996699\", # -20\n",
    "    \"#663366\", # -15\n",
    "    \"#cccc99\", # -10\n",
    "    \"#999966\", # -5\n",
    "    \"#646464\", # 0\n",
    "    \"#04e9e7\", # 5\n",
    "    \"#019ff4\", # 10\n",
    "    \"#0300f4\", # 15\n",
    "    \"#02fd02\", # 20\n",
    "    \"#01c501\", # 25\n",
    "    \"#008e00\", # 30\n",
    "    \"#fdf802\", # 35\n",
    "    \"#e5bc00\", # 40\n",
    "    \"#fd9500\", # 45\n",
    "    \"#fd0000\", # 50\n",
    "    \"#d40000\", # 55\n",
    "    \"#bc0000\", # 60\n",
    "    \"#f800fd\", # 65\n",
    "    \"#9854c6\", # 70\n",
    "    \"#fdfdfd\" # 75\n",
    "    ]\n",
    "\n",
    "    return mpl.colors.ListedColormap(nws_reflectivity_colors)\n",
    "\n",
    "\n",
    "cma1=plt.get_cmap('bwr')\n",
    "cma2=radar_colormap()\n",
    "cma3=plt.get_cmap('tab20c')\n",
    "cma4=ncm.cmap(\"WhiteBlueGreenYellowRed\")\n",
    "cma5=plt.get_cmap('gray_r')\n",
    "cma6=plt.get_cmap('rainbow')\n",
    "cma7=plt.get_cmap('Oranges')\n",
    "cma8=plt.get_cmap('coolwarm')\n",
    "cma9=cma4.reversed()\n",
    "cma10=plt.get_cmap('gist_yarg')\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colorlist=[\"darkblue\", \"lightsteelblue\", \"white\"]\n",
    "newcmp = LinearSegmentedColormap.from_list('testCmap', colors=colorlist, N=256)\n",
    "                \n",
    "import rams_tools\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "from RAMS_Post_Process import fx_postproc_RAMS as RAMS_fx\n",
    "from memory_profiler import profile\n",
    "\n",
    "from wrf import (\n",
    "    CoordPair,\n",
    "    GeoBounds,\n",
    "    cartopy_xlim,\n",
    "    cartopy_ylim,\n",
    "    get_cartopy,\n",
    "    getvar,\n",
    "    interplevel,\n",
    "    interpline,\n",
    "    latlon_coords,\n",
    "    ll_to_xy,\n",
    "    smooth2d,\n",
    "    to_np,\n",
    "    vertcross,\n",
    "    xy_to_ll,\n",
    "    ll_to_xy_proj\n",
    ")\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import random\n",
    "\n",
    "#rc('mathtext', default='regular')\n",
    "#matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 18\n",
    "matplotlib.rcParams['axes.titlesize'] = 18\n",
    "matplotlib.rcParams['xtick.labelsize'] = 17\n",
    "matplotlib.rcParams['ytick.labelsize'] = 17\n",
    "matplotlib.rcParams['legend.fontsize'] = 18\n",
    "matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "matplotlib.rcParams['hatch.linewidth'] = 0.25\n",
    "matplotlib.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def read_var(filename,varname):\n",
    "    with h5py.File(filename,\"r\") as f:\n",
    "        data_out = f[varname][:]\n",
    "    return data_out\n",
    "\n",
    "def read_3dvar_subset(filename,varname,X1,X2,Y1,Y2):\n",
    "    with h5py.File(filename,\"r\") as f:\n",
    "        data_out = f[varname][:,Y1:Y2,X1:X2]\n",
    "    return data_out\n",
    "\n",
    "def read_2dvar_subset(filename,varname,X1,X2,Y1,Y2):\n",
    "    with h5py.File(filename,\"r\") as f:\n",
    "        data_out = f[varname][Y1:Y2,X1:X2]\n",
    "    return data_out\n",
    "\n",
    "#@profile\n",
    "def plot_w_itc_WRF_RAMS_comparison(WRF_FILENAME,RAMS_FILENAME,ZT,savepath):\n",
    "    # Constants for calculating total integrated condensate\n",
    "    cp = 1004; # J/kg/K\n",
    "    rd = 287; # J/kg/K\n",
    "    p00 = 100000; # Reference Pressure\n",
    "    ########### WRF PORTION ###########\n",
    "    print('calculating WRF ITC for file: ',WRF_FILENAME)\n",
    "    #NC_FILE = Dataset(WRF_FILENAME)\n",
    "    ncfile = Dataset(WRF_FILENAME)\n",
    "    wrf_terr = getvar(ncfile,'ter')\n",
    "    wrf_lats, wrf_lons = latlon_coords(wrf_terr)\n",
    "    da_wrf = xr.open_dataset(WRF_FILENAME)\n",
    "    qq =  da_wrf['QCLOUD'].squeeze() + da_wrf['QGRAUP'].squeeze() + da_wrf['QICE'].squeeze() + da_wrf['QRAIN'].squeeze() + da_wrf['QSNOW'].squeeze()\n",
    "    moist_pot_temp_pert = da_wrf['THM'] .squeeze()\n",
    "    base_temp = da_wrf['T00'].squeeze() \n",
    "    moist_pot_temp = moist_pot_temp_pert + base_temp\n",
    "    qv   = da_wrf['QVAPOR'].squeeze()\n",
    "    theta = moist_pot_temp/(1+1.61*qv)\n",
    "    p = da_wrf['P'].squeeze() + da_wrf['PB'].squeeze()\n",
    "    tk = theta/((100000.0/p)**0.286)\n",
    "    heights = (da_wrf['PH'].squeeze()+da_wrf['PHB'].squeeze())/9.81\n",
    "    rho = p/(281.0 * tk)\n",
    "    del(theta,p,tk,qv,moist_pot_temp,moist_pot_temp_pert)\n",
    "    dz =  heights[1:,:,:] - heights[:-1,:,:]\n",
    "    itc_wrf = np.nansum(qq.values*rho.values*dz.values,axis=0) # integrated total condensate in kg\n",
    "    del(rho,qq,dz)\n",
    "    itc_mm_wrf = itc_wrf/997.0*1000.0 # integrated total condensate in mm\n",
    "    #print(itc_mm_wrf)\n",
    "    print('shape of ITC WRF is ',np.shape(itc_mm_wrf))\n",
    "    print('done calculating ITC (mm) WRF')\n",
    "    wrf_times = da_wrf['XTIME']\n",
    "    cur_time_wrf = np.datetime_as_string(wrf_times.values, timezone='UTC',unit='m')\n",
    "    #---------------#---------------#---------------#---------------#---------------#---------------\n",
    "    \n",
    "    ########### RAMS PORTION ###########\n",
    "    print('calculating RAMS ITC for file: ',RAMS_FILENAME)\n",
    "    cur_time = os.path.split(RAMS_FILENAME)[1][9:21]\n",
    "    ds_rams=xr.open_dataset(RAMS_FILENAME,engine='h5netcdf', phony_dims='sort')\n",
    "    rams_lats=ds_rams.GLAT.values\n",
    "    rams_lons=ds_rams.GLON.values\n",
    "    rams_terr=ds_rams.TOPT.values\n",
    "    #zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],h5files1[0])\n",
    "    rams_time, rams_time_savestr = get_time_from_RAMS_file(RAMS_FILENAME)\n",
    "    print('Time in this file ',rams_time)\n",
    "\n",
    "    wp = ds_rams['WP']\n",
    "    nx = np.shape(wp)[2]\n",
    "    ny = np.shape(wp)[1]\n",
    "    rtp = ds_rams['RTP'] - ds_rams['RV']\n",
    "    th = ds_rams['THETA']\n",
    "    pi = ds_rams['PI']\n",
    "    rv = ds_rams['RV']\n",
    "    # Convert RAMS native variables to temperature and pressure\n",
    "    pres = np.power((pi/cp),cp/rd)*p00\n",
    "    temp = th*(pi/cp)\n",
    "    del(th,pi)\n",
    "    # Calculate atmospheric density\n",
    "    dens = pres/(rd*temp*(1+0.61*rv))\n",
    "    del(pres,temp,rv)\n",
    "    # Difference in heights (dz)    \n",
    "    diff_zt_3D = np.tile(np.diff(ZT),(int(ny),int(nx),1))\n",
    "    diff_zt_3D = np.moveaxis(diff_zt_3D,2,0)\n",
    "    # Calculate integrated condensate\n",
    "    itc = np.nansum(rtp[1:,:,:]*dens[1:,:,:]*diff_zt_3D,axis=0) # integrated total condensate in kg\n",
    "    itc_mm_rams = itc/997*1000 # integrated total condensate in mm\n",
    "    itc_mm_rams[itc_mm_rams<=0] = 0.001\n",
    "    print('shape of ITC RAMS is ',np.shape(itc_mm_rams))\n",
    "    print('done calculating ITC (mm) RAMS')\n",
    "    ######################## PLOTTING ##########################################\n",
    "    itc_lvls = np.arange(0.01,10.01,0.01) # Adjusted these levels, such that figure shows regions with at least 1 grid box with 0.1 g/kg of condensate\n",
    "    itc_cbar_ticks = np.log10(np.array([1,5,10]))\n",
    "    itc_cbar_ticklbls = np.array([1,5,10])\n",
    "    \n",
    "    # Scale size of figure based on dimensions of domain\n",
    "    max_dim = np.max([nx,ny])\n",
    "    fs_scale = 9\n",
    "    lw = 1.0\n",
    "   \n",
    "    # Plot Figure\n",
    "    fig, axs = plt.subplots(nrows=2,ncols=1,subplot_kw={'projection': crs.PlateCarree()},figsize=(11,11))\n",
    "    # axs is a 2 dimensional array of `GeoAxes`.  We will flatten it into a 1-D array\n",
    "    axs=axs.flatten()\n",
    "    \n",
    "    itc_wrf_cont  = axs[0].contourf(wrf_lons ,wrf_lats ,np.log10(itc_mm_wrf), levels=np.log10(itc_lvls),transform=crs.PlateCarree(),cmap=newcmp,extend='both')\n",
    "    wrf_terr      = axs[0].contour (wrf_lons ,wrf_lats ,wrf_terr,levels=[500.],transform=crs.PlateCarree(),linewidths=1.4,colors=\"saddlebrown\")\n",
    "    itc_rams_cont = axs[1].contourf(rams_lons,rams_lats,np.log10(itc_mm_rams),levels=np.log10(itc_lvls),transform=crs.PlateCarree(),cmap=newcmp,extend='both')\n",
    "    rams_terr     = axs[1].contour (rams_lons,rams_lats,rams_terr,levels=[500.],transform=crs.PlateCarree(),linewidths=1.4,colors=\"saddlebrown\")\n",
    "\n",
    "    axs[0].set_title ('WRF: Int. Total Condensate (mm; shaded) at '+cur_time, size=12)\n",
    "    axs[1].set_title('RAMS: Int. Total Condensate (mm; shaded) at '+cur_time, size=12)\n",
    "    #----------\n",
    "    gl1 = axs[0].gridlines()#color=\"gray\",alpha=0.5, linestyle='--',draw_labels=True,linewidth=2)\n",
    "    axs[0].coastlines(resolution='110m')\n",
    "    gl1.xlines = True\n",
    "    gl1.ylines = True\n",
    "    LATLON_LABELS=True\n",
    "    if LATLON_LABELS:\n",
    "        print('LATLON labels are on')\n",
    "        gl1.xlabels_top = True\n",
    "        gl1.ylabels_right = False\n",
    "        gl1.ylabels_left = True\n",
    "        gl1.ylabels_bottom = True\n",
    "    else:\n",
    "        gl1.xlabels_top = False\n",
    "        gl1.ylabels_right = False\n",
    "        gl1.ylabels_left = False\n",
    "        gl1.ylabels_bottom = True\n",
    "    gl1.xlabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    gl1.ylabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    #----------\n",
    "    gl2 = axs[1].gridlines()#color=\"gray\",alpha=0.5, linestyle='--',draw_labels=True,linewidth=2)\n",
    "    axs[1].coastlines(resolution='110m')\n",
    "    gl2.xlines = True\n",
    "    gl2.ylines = True\n",
    "    LATLON_LABELS=True\n",
    "    if LATLON_LABELS:\n",
    "        print('LATLON labels are on')\n",
    "        gl2.xlabels_top = True\n",
    "        gl2.ylabels_right = False\n",
    "        gl2.ylabels_left = True\n",
    "        gl2.ylabels_bottom = True\n",
    "    else:\n",
    "        gl2.xlabels_top = False\n",
    "        gl2.ylabels_right = False\n",
    "        gl2.ylabels_left = False\n",
    "        gl2.ylabels_bottom = True\n",
    "    gl2.xlabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    gl2.ylabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    \n",
    "    # COLORBAR\n",
    "    # Add a colorbar axis at the bottom of the graph\n",
    "    cbar_ax = fig.add_axes([0.2, -0.017, 0.6, 0.015])\n",
    "    cbar=fig.colorbar(rams_terr, cax=cbar_ax,orientation='horizontal',ticks=itc_cbar_ticks)\n",
    "    cbar.ax.set_yticklabels(itc_cbar_ticklbls)\n",
    "    cbar.ax.set_ylabel('Integrated Total Condensate (mm)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    png_name = 'WRF_RAMS_D01_ITC_mm_comparison_cbar_'+cur_time+'.png'\n",
    "    print('saving to file: ',png_name)\n",
    "    fig.savefig(savepath+png_name,dpi=200)\n",
    "    plt.close()\n",
    "    return\n",
    "###############################################################\n",
    "domain_wrf = \"d01\"  #2019-04-21 13\n",
    "domain_rams = \"g1\"\n",
    "\n",
    "if domain_wrf=='d01':\n",
    "    grid_spacing = 1600.\n",
    "    winds_thin=25\n",
    "elif domain_wrf=='d02':\n",
    "    grid_spacing=400.\n",
    "    winds_thin=30\n",
    "else:\n",
    "    print('provide correct value of grid spacing!!!')\n",
    "    \n",
    "directory_wrf=\"/nobackupp11/isingh2/WRF_final_for_testing/WRF/run/FIRST_RUN/\"\n",
    "directory_rams=\"/nobackup/pmarines/PROD/AUS1.1-R/G12/out/\"\n",
    "\n",
    "start_time = '2006-01-23 03:00:00'\n",
    "end_time = '2006-01-23 04:00:00'\n",
    "date_range = pd.date_range(start_time,end_time,freq='30min')\n",
    "\n",
    "fi_list_wrf = []\n",
    "fi_list_rams = []\n",
    "\n",
    "\n",
    "for times in date_range:\n",
    "    file_finding_string_wrf = \"wrfout_\"+domain_wrf+\"_\"+times.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "    fi_list_wrf.append(sorted(glob.glob(directory_wrf+file_finding_string_wrf))[0])\n",
    "    \n",
    "    file_finding_string_rams = \"a-A-\"+times.strftime('%Y-%m-%d-%H%M%S')+\"-\"+domain_rams+\".h5\"\n",
    "    fi_list_rams.append(sorted(glob.glob(directory_rams+file_finding_string_rams))[0])#.pop()\n",
    "    \n",
    "print('number of WRF files found: ',len(fi_list_wrf))\n",
    "print('WRF files: ',fi_list_wrf)\n",
    "    \n",
    "print('number of RAMS files found: ',len(fi_list_rams))\n",
    "print('RAMS files: ',fi_list_rams)\n",
    "print('------\\n\\n')\n",
    "    \n",
    "hefilepath = directory_rams+'a-A*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "\n",
    "zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],fi_list_rams[0]) \n",
    "\n",
    "argument = []\n",
    "for ii in range(len(fi_list_wrf)):\n",
    "    argument = argument + [(fi_list_wrf[ii],fi_list_rams[ii],zt,'./')]\n",
    "\n",
    "#print(argument)\n",
    "print('will make plots for ',len(argument),' filesx2','\\n\\n\\n')\n",
    "print('first argument is ',argument[0])\n",
    "\n",
    "# single processor\n",
    "plot_w_itc_WRF_RAMS_comparison(*random.choice(argument))\n",
    "\n",
    "# multiple processors\n",
    "# cpu_count1 = cpu_count()\n",
    "\n",
    "# def main(FUNCTION, ARGUMENT):\n",
    "#     pool = Pool(int(cpu_count1/8))\n",
    "#     #pool = Pool(1)\n",
    "#     start_time = time.perf_counter()\n",
    "#     results = pool.starmap(FUNCTION, ARGUMENT)\n",
    "#     finish_time = time.perf_counter()\n",
    "#     print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(plot_w_itc_WRF_RAMS_comparison, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f0c1f",
   "metadata": {},
   "source": [
    "## Max Column w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f81409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel version\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import metpy.calc as metcalc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.ticker import FormatStrFormatter, LinearLocator\n",
    "import nclcmaps as ncm\n",
    "import cartopy.crs as crs\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "from cartopy.mpl.gridliner import LATITUDE_FORMATTER, LONGITUDE_FORMATTER\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "def radar_colormap():\n",
    "    nws_reflectivity_colors = [\n",
    "    \"#646464\", # ND\n",
    "    \"#ccffff\", # -30\n",
    "    \"#cc99cc\", # -25\n",
    "    \"#996699\", # -20\n",
    "    \"#663366\", # -15\n",
    "    \"#cccc99\", # -10\n",
    "    \"#999966\", # -5\n",
    "    \"#646464\", # 0\n",
    "    \"#04e9e7\", # 5\n",
    "    \"#019ff4\", # 10\n",
    "    \"#0300f4\", # 15\n",
    "    \"#02fd02\", # 20\n",
    "    \"#01c501\", # 25\n",
    "    \"#008e00\", # 30\n",
    "    \"#fdf802\", # 35\n",
    "    \"#e5bc00\", # 40\n",
    "    \"#fd9500\", # 45\n",
    "    \"#fd0000\", # 50\n",
    "    \"#d40000\", # 55\n",
    "    \"#bc0000\", # 60\n",
    "    \"#f800fd\", # 65\n",
    "    \"#9854c6\", # 70\n",
    "    \"#fdfdfd\" # 75\n",
    "    ]\n",
    "\n",
    "    return mpl.colors.ListedColormap(nws_reflectivity_colors)\n",
    "\n",
    "\n",
    "cma1=plt.get_cmap('bwr')\n",
    "cma2=radar_colormap()\n",
    "cma3=plt.get_cmap('tab20c')\n",
    "cma4=ncm.cmap(\"WhiteBlueGreenYellowRed\")\n",
    "cma5=plt.get_cmap('gray_r')\n",
    "cma6=plt.get_cmap('rainbow')\n",
    "cma7=plt.get_cmap('Oranges')\n",
    "cma8=plt.get_cmap('coolwarm')\n",
    "cma9=cma4.reversed()\n",
    "cma10=plt.get_cmap('gist_yarg')\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colorlist=[\"darkblue\", \"lightsteelblue\", \"white\"]\n",
    "newcmp = LinearSegmentedColormap.from_list('testCmap', colors=colorlist, N=256)\n",
    "                \n",
    "import rams_tools\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "from RAMS_Post_Process import fx_postproc_RAMS as RAMS_fx\n",
    "from memory_profiler import profile\n",
    "\n",
    "from wrf import (\n",
    "    CoordPair,\n",
    "    GeoBounds,\n",
    "    cartopy_xlim,\n",
    "    cartopy_ylim,\n",
    "    get_cartopy,\n",
    "    getvar,\n",
    "    interplevel,\n",
    "    interpline,\n",
    "    latlon_coords,\n",
    "    ll_to_xy,\n",
    "    smooth2d,\n",
    "    to_np,\n",
    "    vertcross,\n",
    "    xy_to_ll,\n",
    "    ll_to_xy_proj\n",
    ")\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import random\n",
    "\n",
    "#rc('mathtext', default='regular')\n",
    "#matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 18\n",
    "matplotlib.rcParams['axes.titlesize'] = 18\n",
    "matplotlib.rcParams['xtick.labelsize'] = 17\n",
    "matplotlib.rcParams['ytick.labelsize'] = 17\n",
    "matplotlib.rcParams['legend.fontsize'] = 18\n",
    "matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "matplotlib.rcParams['hatch.linewidth'] = 0.25\n",
    "matplotlib.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def read_var(filename,varname):\n",
    "    with h5py.File(filename,\"r\") as f:\n",
    "        data_out = f[varname][:]\n",
    "    return data_out\n",
    "\n",
    "def read_3dvar_subset(filename,varname,X1,X2,Y1,Y2):\n",
    "    with h5py.File(filename,\"r\") as f:\n",
    "        data_out = f[varname][:,Y1:Y2,X1:X2]\n",
    "    return data_out\n",
    "\n",
    "def read_2dvar_subset(filename,varname,X1,X2,Y1,Y2):\n",
    "    with h5py.File(filename,\"r\") as f:\n",
    "        data_out = f[varname][Y1:Y2,X1:X2]\n",
    "    return data_out\n",
    "\n",
    "#@profile\n",
    "def plot_maxcol_w_WRF_RAMS_comparison(WRF_FILENAME,RAMS_FILENAME,ZT,savepath):\n",
    "    ########### WRF PORTION ###########\n",
    "    print('calculating WRF w for file: ',WRF_FILENAME)\n",
    "    #NC_FILE = Dataset(WRF_FILENAME)\n",
    "    ncfile   = Dataset(WRF_FILENAME)\n",
    "    wrf_terr = getvar(ncfile,'ter')\n",
    "    wrf_w    = getvar(ncfile,'wa')\n",
    "    print(wrf_w)\n",
    "    wrf_lats, wrf_lons = latlon_coords(wrf_terr)\n",
    "    da_wrf   = xr.open_dataset(WRF_FILENAME)\n",
    "    print(da_wrf)\n",
    "    w_col_max_wrf =  wrf_w.max(axis=0)\n",
    "    #print(itc_mm_wrf)\n",
    "    print('shape of WRF max column w is ',np.shape(w_col_max_wrf))\n",
    "    wrf_times = da_wrf['XTIME']\n",
    "    cur_time_wrf = np.datetime_as_string(wrf_times.values, timezone='UTC',unit='m')\n",
    "    #---------------#---------------#---------------#---------------#---------------#---------------\n",
    "    ########### RAMS PORTION ###########\n",
    "    print('calculating RAMS w for file: ',RAMS_FILENAME)\n",
    "    cur_time = os.path.split(RAMS_FILENAME)[1][9:21]\n",
    "    ds_rams=xr.open_dataset(RAMS_FILENAME,engine='h5netcdf', phony_dims='sort')\n",
    "    rams_lats=ds_rams.GLAT.values\n",
    "    rams_lons=ds_rams.GLON.values\n",
    "    rams_terr=ds_rams.TOPT.values\n",
    "    #zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],h5files1[0])\n",
    "    rams_time, rams_time_savestr = get_time_from_RAMS_file(RAMS_FILENAME)\n",
    "    print('Time in this file ',rams_time)\n",
    "\n",
    "    w_col_max_rams = ds_rams['WP'].max(axis=0)\n",
    "    print('shape of RAMS max column w is ',np.shape(w_col_max_rams))\n",
    "    ######################## PLOTTING ##########################################\n",
    "    # Plot Figure\n",
    "    fig, axs = plt.subplots(nrows=2,ncols=1,subplot_kw={'projection': crs.PlateCarree()},figsize=(11,11))\n",
    "    # axs is a 2 dimensional array of `GeoAxes`.  We will flatten it into a 1-D array\n",
    "    axs=axs.flatten()\n",
    "    \n",
    "    w_wrf_cont  = axs[0].contourf(wrf_lons ,wrf_lats ,w_col_max_wrf, levels=np.arange(-40,41,1),transform=crs.PlateCarree(),cmap=cma1,extend='both')\n",
    "    wrf_terr    = axs[0].contour (wrf_lons ,wrf_lats ,wrf_terr,levels=[500.],transform=crs.PlateCarree(),linewidths=1.4,colors=\"saddlebrown\")\n",
    "    w_rams_cont = axs[1].contourf(rams_lons,rams_lats,w_col_max_rams,levels=np.arange(-40,41,1),transform=crs.PlateCarree(),cmap=cma1,extend='both')\n",
    "    rams_terr   = axs[1].contour (rams_lons,rams_lats,rams_terr,levels=[500.],transform=crs.PlateCarree(),linewidths=1.4,colors=\"saddlebrown\")\n",
    "\n",
    "    axs[0].set_title ('WRF: Max col. w (m/s; shaded) at '+cur_time, size=12)\n",
    "    axs[1].set_title('RAMS: Max col w (m/s; shaded) at '+cur_time, size=12)\n",
    "    #----------\n",
    "    gl1 = axs[0].gridlines()#color=\"gray\",alpha=0.5, linestyle='--',draw_labels=True,linewidth=2)\n",
    "    axs[0].coastlines(resolution='110m')\n",
    "    gl1.xlines = True\n",
    "    gl1.ylines = True\n",
    "    LATLON_LABELS=True\n",
    "    if LATLON_LABELS:\n",
    "        print('LATLON labels are on')\n",
    "        gl1.xlabels_top = True\n",
    "        gl1.ylabels_right = False\n",
    "        gl1.ylabels_left = True\n",
    "        gl1.ylabels_bottom = True\n",
    "    else:\n",
    "        gl1.xlabels_top = False\n",
    "        gl1.ylabels_right = False\n",
    "        gl1.ylabels_left = False\n",
    "        gl1.ylabels_bottom = True\n",
    "    gl1.xlabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    gl1.ylabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    #----------\n",
    "    gl2 = axs[1].gridlines()#color=\"gray\",alpha=0.5, linestyle='--',draw_labels=True,linewidth=2)\n",
    "    axs[1].coastlines(resolution='110m')\n",
    "    gl2.xlines = True\n",
    "    gl2.ylines = True\n",
    "    LATLON_LABELS=True\n",
    "    if LATLON_LABELS:\n",
    "        print('LATLON labels are on')\n",
    "        gl2.xlabels_top = True\n",
    "        gl2.ylabels_right = False\n",
    "        gl2.ylabels_left = True\n",
    "        gl2.ylabels_bottom = True\n",
    "    else:\n",
    "        gl2.xlabels_top = False\n",
    "        gl2.ylabels_right = False\n",
    "        gl2.ylabels_left = False\n",
    "        gl2.ylabels_bottom = True\n",
    "    gl2.xlabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    gl2.ylabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    \n",
    "    # COLORBAR\n",
    "    # Add a colorbar axis at the bottom of the graph\n",
    "    cbar_ax = fig.add_axes([0.2, -0.017, 0.6, 0.015])\n",
    "    cbar=fig.colorbar(w_wrf_cont, cax=cbar_ax,orientation='horizontal')#,ticks=itc_cbar_ticks)\n",
    "    #cbar.ax.set_yticklabels(itc_cbar_ticklbls)\n",
    "    cbar.ax.set_ylabel('max col w (m/s)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    png_name = 'WRF_RAMS_D02_colmax_w_mps_comparison_cbar_'+cur_time+'.png'\n",
    "    print('saving to file: ',png_name)\n",
    "    fig.savefig(savepath+png_name,dpi=200)\n",
    "    plt.close()\n",
    "    return\n",
    "###############################################################\n",
    "domain_wrf = \"d02\"  #2019-04-21 13\n",
    "domain_rams = \"g2\"\n",
    "\n",
    "if domain_wrf=='d01':\n",
    "    grid_spacing = 1600.\n",
    "    winds_thin=25\n",
    "elif domain_wrf=='d02':\n",
    "    grid_spacing=400.\n",
    "    winds_thin=30\n",
    "else:\n",
    "    print('provide correct value of grid spacing!!!')\n",
    "    \n",
    "directory_wrf=\"/nobackupp11/isingh2/WRF_final_for_testing/WRF/run/FIRST_RUN/\"\n",
    "directory_rams=\"/nobackup/pmarines/PROD/AUS1.1-R/G12/out/\"\n",
    "\n",
    "start_time = '2006-01-23 03:00:00'\n",
    "end_time = '2006-01-23 16:00:00'\n",
    "date_range = pd.date_range(start_time,end_time,freq='30min')\n",
    "\n",
    "fi_list_wrf = []\n",
    "fi_list_rams = []\n",
    "\n",
    "\n",
    "for times in date_range:\n",
    "    file_finding_string_wrf = \"wrfout_\"+domain_wrf+\"_\"+times.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "    fi_list_wrf.append(sorted(glob.glob(directory_wrf+file_finding_string_wrf))[0])\n",
    "    \n",
    "    file_finding_string_rams = \"a-A-\"+times.strftime('%Y-%m-%d-%H%M%S')+\"-\"+domain_rams+\".h5\"\n",
    "    fi_list_rams.append(sorted(glob.glob(directory_rams+file_finding_string_rams))[0])#.pop()\n",
    "    \n",
    "print('number of WRF files found: ',len(fi_list_wrf))\n",
    "print('WRF files: ',fi_list_wrf)\n",
    "    \n",
    "print('number of RAMS files found: ',len(fi_list_rams))\n",
    "print('RAMS files: ',fi_list_rams)\n",
    "print('------\\n\\n')\n",
    "    \n",
    "hefilepath = directory_rams+'a-A*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "\n",
    "zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],fi_list_rams[0]) \n",
    "\n",
    "argument = []\n",
    "for ii in range(len(fi_list_wrf)):\n",
    "    argument = argument + [(fi_list_wrf[ii],fi_list_rams[ii],zt,'./')]\n",
    "\n",
    "#print(argument)\n",
    "print('will make plots for ',len(argument),' filesx2','\\n\\n\\n')\n",
    "print('first argument is ',argument[0])\n",
    "\n",
    "# single processor\n",
    "#plot_maxcol_w_WRF_RAMS_comparison(*random.choice(argument))\n",
    "\n",
    "\n",
    "# for argu in argument:\n",
    "#     plot_maxcol_w_WRF_RAMS_comparison(*argu)\n",
    "# multiple processors\n",
    "cpu_count1 = cpu_count()\n",
    "\n",
    "def main(FUNCTION, ARGUMENT):\n",
    "    pool = Pool(int(cpu_count1/2))\n",
    "    #pool = Pool(1)\n",
    "    start_time = time.perf_counter()\n",
    "    results = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(plot_maxcol_w_WRF_RAMS_comparison, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d48f9f",
   "metadata": {},
   "source": [
    "## PDFs of various quanitities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062023d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from netCDF4 import Dataset\n",
    "from scipy import stats\n",
    "\n",
    "domain_wrf = \"d01\"  #2019-04-21 13\n",
    "domain_rams = \"g1\"\n",
    "\n",
    "directory_wrf=\"/nobackupp11/isingh2/WRF_final_for_testing/WRF/run/FIRST_RUN/\"\n",
    "directory_rams=\"/nobackup/pmarines/PROD/AUS1.1-R/G12/out/\"\n",
    "\n",
    "start_time = '2006-01-23 09:00:00'\n",
    "end_time = '2006-01-23 10:00:00'\n",
    "date_range = pd.date_range(start_time,end_time,freq='30min')\n",
    "print(date_range)\n",
    "\n",
    "fi_list_wrf = []\n",
    "fi_list_rams = []\n",
    "\n",
    "\n",
    "for times in date_range:\n",
    "    file_finding_string_wrf = \"wrfout_\"+domain_wrf+\"_\"+times.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "    fi_list_wrf.append(sorted(glob.glob(directory_wrf+file_finding_string_wrf))[0])\n",
    "    \n",
    "    file_finding_string_rams = \"a-A-\"+times.strftime('%Y-%m-%d-%H%M%S')+\"-\"+domain_rams+\".h5\"\n",
    "    fi_list_rams.append(sorted(glob.glob(directory_rams+file_finding_string_rams))[0])#.pop()\n",
    "    \n",
    "print('number of WRF files found: ',len(fi_list_wrf))\n",
    "print('WRF files: ',fi_list_wrf)\n",
    "    \n",
    "print('number of RAMS files found: ',len(fi_list_rams))\n",
    "print('RAMS files: ',fi_list_rams)\n",
    "print('------\\n\\n')\n",
    "    \n",
    "hefilepath = directory_rams+'a-A*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "\n",
    "zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],fi_list_rams[0]) \n",
    "\n",
    "def get_time_from_WRF_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][11:30] # Grab time string from WRF file\n",
    "    pd_time = pd.to_datetime(cur_time[0:9]+' '+cur_time[11:18])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def plot_pdf(WRF_VARIABLE_NAME='wa',RAMS_VARIABLE_NAME='WP',WRF_FILENAME=None,RAMS_FILENAME=None,WRF_USE_DATASET_XARRAY='XARRAY'):\n",
    "    if WRF_FILENAME:\n",
    "        print('reading <<',WRF_VARIABLE_NAME,'>> in the WRF file for time ',get_time_from_WRF_file(WRF_FILENAME)[0])\n",
    "        \n",
    "        if WRF_USE_DATASET_XARRAY=='DATASET':\n",
    "            ncfile = Dataset(WRF_FILENAME)\n",
    "            wrf_var =  getvar(ncfile,WRF_VARIABLE_NAME)\n",
    "        elif WRF_USE_DATASET_XARRAY:\n",
    "            wrf_var = xr.open_dataset(WRF_FILENAME)[WRF_VARIABLE_NAME]\n",
    "        else:\n",
    "            print('DATASET or XARRAY')\n",
    "        \n",
    "        print('shape of WRF variable is ',np.shape(wrf_var))\n",
    "        \n",
    "        #sns.kdeplot(wrf_var.to_series(),label='WRF')\n",
    "        print('plotting the data')\n",
    "        stacked_wrf=wrf_var.stack(stacked=[...])\n",
    "        sns.kdeplot(stacked_wrf,label='WRF')\n",
    "        stats.gaussian_kde(dataset\n",
    "        \n",
    "    if RAMS_FILENAME:\n",
    "        print('reading <<',RAMS_VARIABLE_NAME,'>> in the RAMS file for time ',get_time_from_RAMS_file(RAMS_FILENAME)[0])\n",
    "        rams_var=xr.open_dataset(RAMS_FILENAME,engine='h5netcdf', phony_dims='sort')[RAMS_VARIABLE_NAME]\n",
    "        print('shape of RAMS variable is ',np.shape(rams_var))\n",
    "        print('plotting the data')\n",
    "        #sns.kdeplot(rams_var.to_series(),label='RAMS')\n",
    "        #sns.kdeplot(rams_var.stack(stacked=[...]),label='RAMS')\n",
    "        \n",
    "    plt.legend()\n",
    "    #plt.savefig()\n",
    "    \n",
    "def plot_CFAD(WRF_VARIABLE_NAME='wa',WRF_FILENAME=None,WRF_USE_DATASET_XARRAY='XARRAY'):\n",
    "    print('reading <<',WRF_VARIABLE_NAME,'>> in the WRF file for time ',get_time_from_WRF_file(WRF_FILENAME)[0])\n",
    "\n",
    "plot_pdf('wa','WP',fi_list_wrf[1],fi_list_rams[1],'DATASET') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2114fc26",
   "metadata": {},
   "source": [
    "## Calculate CMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090886cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_wrf = \"d01\"  #2019-04-21 13\n",
    "domain_rams = \"g1\"\n",
    "\n",
    "directory_wrf=\"/nobackupp11/isingh2/WRF_final_for_testing/WRF/run/FIRST_RUN/\"\n",
    "directory_rams=\"/nobackup/pmarines/PROD/AUS1.1-R/G12/out/\"\n",
    "\n",
    "start_time = '2006-01-23 09:00:00'\n",
    "end_time = '2006-01-23 10:00:00'\n",
    "date_range = pd.date_range(start_time,end_time,freq='30min')\n",
    "print(date_range)\n",
    "\n",
    "fi_list_wrf = []\n",
    "fi_list_rams = []\n",
    "\n",
    "\n",
    "for times in date_range:\n",
    "    file_finding_string_wrf = \"wrfout_\"+domain_wrf+\"_\"+times.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "    fi_list_wrf.append(sorted(glob.glob(directory_wrf+file_finding_string_wrf))[0])\n",
    "    \n",
    "    file_finding_string_rams = \"a-A-\"+times.strftime('%Y-%m-%d-%H%M%S')+\"-\"+domain_rams+\".h5\"\n",
    "    fi_list_rams.append(sorted(glob.glob(directory_rams+file_finding_string_rams))[0])#.pop()\n",
    "    \n",
    "print('number of WRF files found: ',len(fi_list_wrf))\n",
    "print('WRF files: ',fi_list_wrf)\n",
    "    \n",
    "print('number of RAMS files found: ',len(fi_list_rams))\n",
    "print('RAMS files: ',fi_list_rams)\n",
    "print('------\\n\\n')\n",
    "\n",
    "def calculate_CMF(WRF_FILENAME,LEVEL):\n",
    "    #------\n",
    "    #ncfile = Dataset(WRF_FILENAME)\n",
    "    #wa = getvar(ncfile,'wa')#.squeeze()\n",
    "    #print(wa)\n",
    "    #wrf_terr = getvar(ncfile,'ter')\n",
    "    #wrf_lats, wrf_lons = latlon_coords(wrf_terr)\n",
    "    #-------\n",
    "    da_wrf = xr.open_dataset(WRF_FILENAME)\n",
    "    #qq =  da_wrf['QCLOUD'].squeeze() + da_wrf['QGRAUP'].squeeze() + da_wrf['QICE'].squeeze() + da_wrf['QRAIN'].squeeze() + da_wrf['QSNOW'].squeeze()\n",
    "    moist_pot_temp_pert = da_wrf['THM'] .squeeze()\n",
    "    base_temp = da_wrf['T00'].squeeze() \n",
    "    moist_pot_temp = moist_pot_temp_pert + base_temp\n",
    "    qv   = da_wrf['QVAPOR'].squeeze()\n",
    "    theta = moist_pot_temp/(1+1.61*qv)\n",
    "    p = da_wrf['P'].squeeze() + da_wrf['PB'].squeeze()\n",
    "    tk = theta/((100000.0/p)**0.286)\n",
    "    heights = (da_wrf['PH'].squeeze()+da_wrf['PHB'].squeeze())/9.81\n",
    "    rho = p/(281.0 * tk)\n",
    "    del(tk,p,theta,qv,moist_pot_temp,moist_pot_temp_pert,base_temp)\n",
    "    print('rho calculation done')\n",
    "    print(da_wrf['w'])\n",
    "    wa_level = (da_wrf['w'][:,LEVEL,:,:].squeeze() + da_wrf['w'][:,LEVEL+1,:,:].squeeze())/2.0\n",
    "    wa_mask  = np.where(wa_level>0.5,wa_level,np.nan)\n",
    "    print(np.nanmean(rho[LEVEL,:,:]*wa_mask))\n",
    "    \n",
    "calculate_CMF(fi_list_wrf[1],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28b028",
   "metadata": {},
   "source": [
    "# 3D visualization (Pyvista- Ben Ascher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd4ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c59f90c1",
   "metadata": {},
   "source": [
    "# Segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ce3ee",
   "metadata": {},
   "source": [
    "## Perform segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e71f1",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904ac1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "865967cd",
   "metadata": {},
   "source": [
    "### Main segmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_segmentation_3D(lite_file_name, head_file_name, pd_dataframe_one_timestep, SEGMENTATION_THRESHOLD, SIMULATION_NAME, debug_print=True):\n",
    "    \"\"\"Run tobac segmentation on the input RAMS file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lite_file_name: str\n",
    "        RAMS HDF5 file (currently must be lite, although it wouldn't be\n",
    "        very much work to switch to allow analysis).\n",
    "    head_file_name: str\n",
    "        RAMS header file name (see caveats in ```calc_zcoords_zmn``` function)\n",
    "    pd_dataframe_one_timestep: str\n",
    "        pandas trackpy output but only for one timestep\n",
    "    debug_print: bool\n",
    "        True to print debug/status statements to stdout. False to not.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Features: pd.DataFrame\n",
    "        the Pandas Dataframe of the RAMS output features.\n",
    "\n",
    "    \"\"\"\n",
    "    # prep the coordinates so that we can load RAMS as an xarray\n",
    "    # note while tobac v1.x requires iris, xarray support is coming.\n",
    "    rams_shared_coords = prep_rams_coordinates(lite_file_name, head_file_name)\n",
    "    if debug_print:\n",
    "        print(lite_file_name)\n",
    "    curr_coords = copy.deepcopy(rams_shared_coords)\n",
    "    lite_file = h5py.File(lite_file_name, \"r\")\n",
    "    lite_file_fn_only = lite_file_name.split(\"/\")[-1]\n",
    "    # get the current time from the input file.\n",
    "    curr_time = datetime.datetime.strptime(\n",
    "        lite_file_fn_only, \"a-L-%Y-%m-%d-%H%M%S-g3.h5\"\n",
    "    )\n",
    "    curr_coords[\"time\"] = [\n",
    "        curr_time,\n",
    "    ]\n",
    "    data_out = dict()\n",
    "    # if we are detecting on a 2D variable\n",
    "    if len(lite_file[var_of_interest].shape) == 2:\n",
    "        data_out[var_of_interest] = (\n",
    "            [\"time\", \"Y\", \"X\"],\n",
    "            np.expand_dims(np.array(lite_file[var_of_interest]), axis=0),\n",
    "        )\n",
    "\n",
    "    # 3D variable, may be z by y by x or patches by y by x\n",
    "    elif len(lite_file[var_of_interest].shape) == 3:\n",
    "        if var_of_interest in patch_variables:\n",
    "            data_out[var_of_interest] = (\n",
    "                [\"time\", \"patch\", \"Y\", \"X\"],\n",
    "                np.expand_dims(np.array(lite_file[var_of_interest]), axis=0),\n",
    "            )\n",
    "        else:\n",
    "            data_out[var_of_interest] = (\n",
    "                [\"time\", \"ztn\", \"Y\", \"X\"],\n",
    "                np.expand_dims(np.array(lite_file[var_of_interest]), axis=0),\n",
    "            )\n",
    "\n",
    "    # convert RAMS to iris for tobac\n",
    "    iris_df = xr.Dataset(data_vars=data_out, coords=curr_coords)[\n",
    "        var_of_interest\n",
    "    ].to_iris()\n",
    "\n",
    "    print('CURR_COORDS',curr_coords,np.shape(data_out))\n",
    "    #print(data_out)\n",
    "    print(iris_df)\n",
    "\n",
    "    # we now have an xarray dataset, let's run tobac segmentation\n",
    "    # see the tobac documentation for documentation of these\n",
    "    parameters_segmentation = {}\n",
    "    parameters_segmentation[\"threshold\"] = SEGMENTATION_THRESHOLD\n",
    "    parameters_segmentation[\"target\"] = \"maximum\" \n",
    "    parameters_segmentation[\"level\"] = None\n",
    "    parameters_segmentation[\"method\"] = 'watershed'\n",
    "    parameters_segmentation[\"max_distance\"] = None\n",
    "    parameters_segmentation[\"vertical_coord\"] = \"ztn\"\n",
    "    parameters_segmentation[\"seed_3D_flag\"] = 'column'\n",
    "    parameters_segmentation[\"seed_3D_size\"] = None\n",
    "    #parameters_segmentation[\"segment_number_below_threshold\"] = 0\n",
    "    #parameters_segmentation[\"segment_number_unassigned\"] = 0\n",
    "\n",
    "    # grid spacing in meters\n",
    "    dxy = 100\n",
    "    segmentation_cube, segmentation_dataframe = tobac.segmentation.segmentation(pd_dataframe_one_timestep , iris_df, dxy, **parameters_segmentation)\n",
    "    \n",
    "    # save the output iris cube with segmentation masks for this one timestep to netCDF\n",
    "    iris_netcdf_filename =  '/home3/isingh2/nobackup/tobac_tracking-main/'+SIMULATION_NAME + \"_segmentation_mask_threshold_\"+str(segmentation_threshold)+\"_\"+pd_time.strftime('%Y%m%d%H%M%S')+\".nc\"\n",
    "    iris.save(segmentation_cube, iris_netcdf_filename) # save the iris cube to netcdf\n",
    "    print('saving segmentation mask to ',iris_netcdf_filename)\n",
    "    \n",
    "    \n",
    "    return segmentation_dataframe\n",
    "\n",
    "def combine_tobac_feats(list_of_feats):\n",
    "    \"\"\"Function to combine the many different individual dataframes\n",
    "    generated by the 'perform_segmentation_3D' function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_feats: list[pd.DataFrame]\n",
    "        A list of the various dataframes from tobac feature detection\n",
    "        output\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A combined DataFrame of Pandas features\n",
    "\n",
    "    \"\"\"\n",
    "    return tobac.utils.combine_tobac_feats(list_of_feats,preserve_old_feat_nums=\"original_feature\")\n",
    "\n",
    "def save_tobac_feats(feat_df, out_file_name):\n",
    "    \"\"\"Function to save tobac features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feat_df: pd.DataFrame\n",
    "        Feature DataFrame\n",
    "    out_file_name: str\n",
    "        Output file name\n",
    "\n",
    "    \"\"\"\n",
    "    feat_df.to_pickle(out_file_name)\n",
    "    \n",
    "argument = []\n",
    "\n",
    "# get the file names from the base folder\n",
    "lite_file_names = sorted(glob.glob(simulation_base_folder + \"a-L-2018-08-28-061*.h5\"))\n",
    "head_file_names = sorted(glob.glob(simulation_base_folder + \"a-L-2018-08-28-061*.txt\"))\n",
    "print('number of total RAMS files: ',len(lite_file_names))\n",
    "tracking_dataframe = pd.read_pickle(tracking_output_file)\n",
    "\n",
    "all_segs = list()\n",
    "\n",
    "# loop over files\n",
    "for lite_file, head_file in zip(lite_file_names, head_file_names):\n",
    "    print('working on file: ',lite_file)\n",
    "    \n",
    "    # Grab time string from RAMS file\n",
    "    cur_time = os.path.split(lite_file)[1][4:21] \n",
    "    print('cur_time: ',cur_time)\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    print('time is : ',pd_time)\n",
    "    \n",
    "    # subset the tracking output: choose the timestep in the file\n",
    "    tracking_dataframe_subset = tracking_dataframe[tracking_dataframe.time==pd_time]\n",
    "    print('tracking_dataframe_subset :')\n",
    "    print(tracking_dataframe_subset)\n",
    "    print('----------------------------\\n\\n')\n",
    "    \n",
    "    # perform segmentation\n",
    "    argument = argument + [(lite_file, head_file, tracking_dataframe_subset, segmentation_threshold, simulation_name)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a015cc7",
   "metadata": {},
   "source": [
    "## Characteristics of thermals from segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2af5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_w_stuff(RAMS_FILE,SEG_FILE,FEATURE_NUMBER,CENTROID_X_INT,CENTROID_Y_INT,DIST):\n",
    "    w_rams=xr.open_dataset(RAMS_FILE,engine='h5netcdf', phony_dims='sort').variables['WP'][:,CENTROID_Y_INT-DIST:CENTROID_Y_INT+DIST,CENTROID_X_INT-DIST:CENTROID_X_INT+DIST]\n",
    "    seg=xr.open_dataset(SEG_FILE).variables['segmentation_mask'][:,CENTROID_Y_INT-DIST:CENTROID_Y_INT+DIST,CENTROID_X_INT-DIST:CENTROID_X_INT+DIST]\n",
    "    #print(w_rams)\n",
    "    mask=np.where(seg==FEATURE_NUMBER,1.0,np.nan)\n",
    "    #print('done with np where')\n",
    "    #print(mask)\n",
    "    mean_cell_w=np.nanmean(w_rams*mask)\n",
    "    max_cell_w=np.nanmax(w_rams*mask)\n",
    "    #print('mean cell w is ',mean_cell_w)\n",
    "    #print('max cell w is ',max_cell_w)\n",
    "    return mean_cell_w,max_cell_w\n",
    "\n",
    "simulation_name='BRA1.1-R'\n",
    "seg_files=sorted(glob.glob('/home3/isingh2/nobackup/tobac_tracking-main/'+simulation_name+'_segmentation_mask_box_threshold_2*.nc'))\n",
    "print(len(seg_files))\n",
    "\n",
    "tobac_tracking_file=glob.glob('/nobackup/pmarines/DATA_FM/'+simulation_name+\\\n",
    "                              '/tobac_data/comb_track_filt_01_02_05_10_20.p')\n",
    "tdata =          pd.read_pickle(tobac_tracking_file[0])\n",
    "tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cells=np.array(tdata.cell.unique())\n",
    "df = pd.DataFrame()\n",
    "print('total number of cells: ',len(all_cells))\n",
    "ii = 0\n",
    "for cell in all_cells:\n",
    "    df['cell']=cell\n",
    "    print('working on cell# ',cell,' ',str(ii),'/',len(all_cells))\n",
    "    tobac_cell_subset = tdata[tdata['cell']==cell]\n",
    "    #print('threshold for this cell: ',tobac_cell_subset.threshold_value.values)\n",
    "    cell_times = pd.to_datetime(tobac_cell_subset.timestr.values)\n",
    "    feature_numbers = tobac_cell_subset.feature.values\n",
    "    print('feature numbers for this cell are: ',feature_numbers)\n",
    "    print('times for which this cell exists are: ')\n",
    "    iii=0\n",
    "    for tim in cell_times:\n",
    "        df['timestr']=tim.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(tim)\n",
    "        print('the cell is represented by feature number: ',feature_numbers[iii])\n",
    "        seg_filename=glob.glob('/home3/isingh2/nobackup/tobac_tracking-main/'+\\\n",
    "               simulation_name+'_segmentation_mask_box_threshold_2_'+tim.strftime('%Y%m%d%H%M%S')+'.nc')\n",
    "        rams_filename = glob.glob('/nobackup/pmarines/DATA_FM/'+simulation_name+'/LES_data/'+'a-L-'+\\\n",
    "               tim.strftime('%Y')+'-'+tim.strftime('%m')+'-'+tim.strftime('%d')+'-'+tim.strftime('%H%M%S')+'-g3.h5')\n",
    "        print('corresponding segmentation file is : ',seg_filename[0])\n",
    "        print('corresponding RAMS file is ',rams_filename[0])\n",
    "        w_avg, w_max = calc_w_stuff(rams_filename[0],seg_filename[0],feature_numbers[iii],int(tobac_cell_subset.X.values[iii]),int(tobac_cell_subset.Y.values[iii]),125)\n",
    "        df['mean_w']=w_avg\n",
    "        df['max_w']=w_max\n",
    "        print('\\n')\n",
    "        iii = iii + 1\n",
    "    print('--')\n",
    "    \n",
    "    print('---------\\n')#\n",
    "    ii = ii + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb600fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample script to get WP at cell centoid\n",
    "\n",
    "from RAMS_Post_Process import fx_postproc_RAMS as RAMS_fxdef get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "\n",
    "domain='WPO1.1-R'\n",
    "\n",
    "path = '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/' # Pleiades\n",
    "tobac_data='/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/' # Pleiades\n",
    "tobac_filename  = 'comb_track_filt_01_02_05_10_20.p'\n",
    "tobac_filepath  = tobac_data+tobac_filename\n",
    "\n",
    "\n",
    "# Get RAMS files info\n",
    "h5filepath = path+'a-L*g3.h5'\n",
    "h5files1 = sorted(glob.glob(h5filepath))\n",
    "hefilepath = path+'a-L*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "#print(h5files1)\n",
    "start_time=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "end_time=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "print('Simulation name: ',domain)\n",
    "print('start time of the G3 ' + domain + ' simulation: ',start_time)\n",
    "print('end time of the G3 '+ domain + ' simulation: ',end_time)\n",
    "\n",
    "\n",
    "zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],h5files1[0])\n",
    "\n",
    "##### read in tobac data #####\n",
    "print('reading ',tobac_filepath)\n",
    "tdata_temp = pd.read_pickle(tobac_filepath)\n",
    "\n",
    "all_cells = tdata.cell.unique()\n",
    "print('number of unique cells identified: ',len(all_cells))\n",
    "\n",
    "for cell_no in [all_cells]: # loop over all cells\n",
    "    print('-----cell#-----',cell_no)#\n",
    "    tdata_neu=tdata_temp[tdata_temp['cell']==cell_no]\n",
    "    print('this cell has '+str(len(tdata_neu))+' time steps -- lifetime of ',len(tdata_neu)*0.5,' mins')\n",
    "    xpos=tdata_neu.X.values.astype(int)\n",
    "    ypos=tdata_neu.Y.values.astype(int)\n",
    "    zpos=tdata_neu.zmn.values.astype(int)\n",
    "    times_tracked=tdata_neu.timestr.values\n",
    "    thresholds=tdata_neu.threshold_value.values\n",
    "    print('x-positions: ',xpos)\n",
    "    print('y-positions: ',ypos)\n",
    "    print('z-positions: ',zpos)\n",
    "    print('thresholds for this cell: ',thresholds)\n",
    "    print('times for this cells: ',times_tracked)\n",
    "\n",
    "    ii = 0 \n",
    "    for tim in times_tracked: # loop over timesteps of this cell \n",
    "        print('------------------------------------------------')\n",
    "        print('timestep '+str(ii)+': '+tim)\n",
    "        tim_pd = pd.to_datetime(tim)\n",
    "        rams_fil='/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5'\n",
    "        print('RAMS file for this time: ',rams_fil)\n",
    "        \n",
    "        # use xarray like here or h5py\n",
    "        da = xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "         \n",
    "        RAMS_closest_level = np.argmin(np.abs(zm-zpos[ii]))\n",
    "        print('RAMS closest vertical level to the thermal centroid is ',RAMS_closest_level)\n",
    "        vertical_vel_centroid = da.WP[RAMS_closest_level,ypos[ii],xpos[ii]].values\n",
    "        print('vertical velocity at the centroid is ',vertical_vel_centroid)\n",
    "        ii = ii + 1\n",
    "        print('===============================\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ad5ea",
   "metadata": {},
   "source": [
    "## Vertical cross-sections with segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d531943",
   "metadata": {
    "code_folding": [
     36,
     87,
     170,
     376,
     442,
     477,
     486,
     494,
     512,
     540,
     641,
     706,
     811,
     865,
     953
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile plot_tobac_crosssections_parallel_plot_cell_dim_box_segmentation.py\n",
    "\n",
    "## plot features (besides the cell being tracked) in the plan view\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import numpy.ma as ma\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.patches import Rectangle\n",
    "import datetime\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from RAMS_Post_Process import fx_postproc_RAMS as RAMS_fx\n",
    "#import cartopy.crs as crs\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib import ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "def radar_colormap():\n",
    "    nws_reflectivity_colors = [\n",
    "    \"#646464\", # ND\n",
    "    \"#ccffff\", # -30\n",
    "    \"#cc99cc\", # -25\n",
    "    \"#996699\", # -20\n",
    "    \"#663366\", # -15\n",
    "    \"#cccc99\", # -10\n",
    "    \"#999966\", # -5\n",
    "    \"#646464\", # 0\n",
    "    \"#04e9e7\", # 5\n",
    "    \"#019ff4\", # 10\n",
    "    \"#0300f4\", # 15\n",
    "    \"#02fd02\", # 20\n",
    "    \"#01c501\", # 25\n",
    "    \"#008e00\", # 30\n",
    "    \"#fdf802\", # 35\n",
    "    \"#e5bc00\", # 40\n",
    "    \"#fd9500\", # 45\n",
    "    \"#fd0000\", # 50\n",
    "    \"#d40000\", # 55\n",
    "    \"#bc0000\", # 60\n",
    "    \"#f800fd\", # 65\n",
    "    \"#9854c6\", # 70\n",
    "    \"#fdfdfd\" # 75\n",
    "    ]\n",
    "\n",
    "    return mpl.colors.ListedColormap(nws_reflectivity_colors)\n",
    "\n",
    "\n",
    "cma1=plt.get_cmap('bwr')\n",
    "cma2=radar_colormap()\n",
    "cma3=plt.get_cmap('tab20c')\n",
    "#cma4=ncm.cmap(\"WhiteBlueGreenYellowRed\")\n",
    "cma5=plt.get_cmap('gray_r')\n",
    "cma6=plt.get_cmap('rainbow')\n",
    "cma7=plt.get_cmap('Oranges')\n",
    "cma8=plt.get_cmap('coolwarm')\n",
    "#cma9=cma4.reversed()\n",
    "cma10=plt.get_cmap('gist_yarg')\n",
    "\n",
    "Cp=1004.\n",
    "Rd=287.0\n",
    "p00 = 100000.0\n",
    "\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def fig_process_vert(AX, TER, CONTOUR, XY_1D, XY1, XY2, Y_CROSS, PLOT_COLORBAR, CBAR_EXP, FONTSIZE, TITLESTRING, TIMESTRING, FILENAMESTRING, PROD, UNITS, VERT_CROSS, HEIGHT, YLABEL, IS_PANEL_PLOT):\n",
    "    #F = plt.gcf()  # Gets the current figure\n",
    "    #ax = plt.gca()  # Gets the current axes\n",
    "\n",
    "    if IS_PANEL_PLOT == False:\n",
    "        AX.set_title('%s (%s) \\n %s' % (TITLESTRING, UNITS, TIMESTRING),\n",
    "                  fontsize=FONTSIZE, stretch='normal')\n",
    "\n",
    "    if VERT_CROSS == \"zonal\":\n",
    "        AX.fill_between(XY_1D[XY1:XY2], 0, TER[int(Y_CROSS),\n",
    "                        XY1:XY2]/1000.0, facecolor='wheat')\n",
    "        AX.set_xlabel('x-distance (km)', fontsize=FONTSIZE)\n",
    "\n",
    "    else:\n",
    "        AX.fill_between(XY_1D[XY1:XY2], 0, TER[XY1:XY2,\n",
    "                        int(Y_CROSS)]/1000.0, facecolor='wheat')\n",
    "        AX.set_xlabel('y-distance (km)', fontsize=FONTSIZE)\n",
    "\n",
    "    AX.patch.set_color(\"white\")\n",
    "\n",
    "    if YLABEL:\n",
    "        AX.set_ylabel('Height (km)', fontsize=FONTSIZE)\n",
    "    AX.set_ylim([0, HEIGHT])\n",
    "    AX.set_xlim([XY1*100.0/1000.0, XY2*100.0/1000.0])\n",
    "\n",
    "    class OOMFormatter(matplotlib.ticker.ScalarFormatter):\n",
    "        def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n",
    "            self.oom = order\n",
    "            self.fformat = fformat\n",
    "            matplotlib.ticker.ScalarFormatter.__init__(\n",
    "                self, useOffset=offset, useMathText=mathText)\n",
    "\n",
    "        def _set_orderOfMagnitude(self, nothing):\n",
    "            self.orderOfMagnitude = self.oom\n",
    "\n",
    "        def _set_format(self, vmin, vmax):\n",
    "            self.format = self.fformat\n",
    "            if self._useMathText:\n",
    "                self.format = '$%s$' % matplotlib.ticker._mathdefault(\n",
    "                    self.format)\n",
    "\n",
    "    if PLOT_COLORBAR:\n",
    "        if IS_PANEL_PLOT == False:\n",
    "            if abs(CBAR_EXP):\n",
    "                divider = make_axes_locatable(AX)\n",
    "                cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.6)\n",
    "                bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",\n",
    "                                   format=OOMFormatter(CBAR_EXP, mathText=False),extend='both')\n",
    "                bar.ax.tick_params(labelsize=FONTSIZE-1)\n",
    "                #file_id = '%s_%s' % (PROD, FILENAMESTRING)\n",
    "                #filename = '%s.png' % (file_id)\n",
    "                #print(filename)\n",
    "                # Saves the figure with small margins\n",
    "                #plt.savefig(filename, dpi=my_dpi, bbox_inches='tight')\n",
    "            else:\n",
    "                divider = make_axes_locatable(AX)\n",
    "                cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.66)\n",
    "                bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",extend='both')\n",
    "                bar.ax.tick_params(labelsize=FONTSIZE-1)\n",
    "                #file_id = '%s_%s' % (PROD, FILENAMESTRING)\n",
    "                #filename = '%s.png' % (file_id)\n",
    "                #print(filename)\n",
    "                # Saves the figure with small margins\n",
    "                #plt.savefig(filename, dpi=my_dpi, bbox_inches='tight')\n",
    "\n",
    "        else:\n",
    "            if abs(CBAR_EXP):\n",
    "                divider = make_axes_locatable(AX)\n",
    "                cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.4)\n",
    "                bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",\n",
    "                                   format=OOMFormatter(CBAR_EXP, mathText=False),extend='both')\n",
    "                bar.ax.tick_params(labelsize=FONTSIZE-1)\n",
    "            else:\n",
    "                divider = make_axes_locatable(AX)\n",
    "                cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.4)\n",
    "                bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",extend='both')\n",
    "                bar.ax.tick_params(labelsize=FONTSIZE-1)\n",
    "        # plt.close() This should remain commented. plt should be closed in the panel plot function\n",
    "        # if export_flag == 1:\n",
    "        # Convert the figure to a gif file\n",
    "        #os.system('convert -render -flatten %s %s.gif' % (filename, file_id))\n",
    "        #os.system('rm -f %s' % filename)\n",
    "\n",
    "def plot_zonal_vertcross(DOMAIN, DATA, DX, DY ,TERR, X_1D, Z_3D, Z_1D, DXY, VAR1, DESTAGGER, STAGGER_DIM, LEVELS_VAR1, CMAP_VAR1,\n",
    "                       VAR2, LEVELS_VAR2, VAR2_COLOR,\n",
    "                       VAR3, LEVELS_VAR3, VAR3_COLOR,\n",
    "                       VAR4, LEVELS_VAR4, VAR4_COLOR,\n",
    "                       yy, x1, x2,\n",
    "                       PLOT_WINDS, PANEL_PLOT, HEIGHT, AX, PLOT_CBAR, EXP_LABEL, PANEL_LABEL, XPOS, YPOS, ZPOS, ZPOS_GRID, CELL_DIM, FEATURE_NUM, PLOT_SEGMENTATION_OUTPUT, NUM_MODEL_LEVS, TITLETIME, FILENAMETIME,EXTERNAL_FIELD=None):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "\n",
    "    y1 = yy - 50\n",
    "    y2 = yy + 50\n",
    "       \n",
    "    XV1, __ = np.meshgrid(X_1D[x1:x2]/1000.0, Z_1D/1000.0)\n",
    "    z_3D_2D_slice    =  (TERR[yy,x1:x2] + Z_1D[:,np.newaxis])/1000.0\n",
    "\n",
    "    if DESTAGGER:\n",
    "        print(var1)\n",
    "        var1 = destagger(var1, STAGGER_DIM, meta=True)\n",
    "    else:\n",
    "        print(' ')\n",
    "\n",
    "    if VAR1=='EXTERNAL':\n",
    "        if (isinstance(LEVELS_VAR1, np.ndarray)):\n",
    "            C1 = AX.contourf(XV1,z_3D_2D_slice, EXTERNAL_FIELD[:, yy,:], levels=LEVELS_VAR1,\n",
    "                           cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "        else:\n",
    "            C1 = AX.contourf(XV1, z_3D_2D_slice, VAR2_XR[:, yy, :],\n",
    "                           cmap=CMAP_VAR1, extend='both')\n",
    "    else:\n",
    "        if (isinstance(LEVELS_VAR1, np.ndarray)):\n",
    "            C1 = AX.contourf(XV1,z_3D_2D_slice, EXTERNAL_FIELD[:, yy, x1:x2], levels=LEVELS_VAR1,\n",
    "                           cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "        else:\n",
    "            C1 = AX.contourf(XV1, z_3D_2D_slice, VAR2_XR[:, yy, x1:x2],\n",
    "                           cmap=CMAP_VAR1, extend='both')\n",
    "        \n",
    "        \n",
    "    if (isinstance(LEVELS_VAR1, np.ndarray)):\n",
    "        C1 = AX.contourf(XV1,z_3D_2D_slice, VAR2_XR[:, yy, x1:x2], levels=LEVELS_VAR1,\n",
    "                           cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "    else:\n",
    "        C1 = AX.contourf(XV1, z_3D_2D_slice, VAR2_XR[:, yy, x1:x2],\n",
    "                           cmap=CMAP_VAR1, extend='both')\n",
    "       \n",
    "    if (isinstance(LEVELS_VAR2, np.ndarray)):\n",
    "        C2 = AX.contour(XV1, z_3D_2D_slice, VAR2_XR[:, yy, x1:x2],\n",
    "                             levels=LEVELS_VAR2, colors=VAR2_COLOR, linewidths=1., linestyles=\"-\")\n",
    "    else:\n",
    "        C2 = AX.contour(XV1, z_3D_2D_slice, VAR2_XR[:, yy, x1:x2],\n",
    "                             colors='k', linewidths=1., linestyles=\"--\")\n",
    "    AX.clabel(C2, inline=1, fontsize=10, fmt='%3.0f')\n",
    "\n",
    "    if VAR3=='RTP-RV_g/kg':\n",
    "        total_condensate_zonal = DATA[\"RTP\"][:, yy, x1:x2]*1000.0 - DATA[\"RV\"][:, yy, x1:x2]*1000.0\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(XV1,z_3D_2D_slice, np.absolute(total_condensate_zonal),\n",
    "                                 levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\") \n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "            \n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    elif VAR3=='RTP-RV_g/m3':\n",
    "        th = DATA['THETA'][:, yy, x1:x2]\n",
    "        pi = DATA['PI'][:, yy, x1:x2]\n",
    "        rv = DATA['RV'][:, yy, x1:x2]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        total_condensate_zonal = (DATA[\"RTP\"][:, yy, x1:x2]*1000.0 - DATA[\"RV\"] [:, yy, x1:x2]*1000.0)*dens\n",
    "        \n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(XV1,z_3D_2D_slice, np.absolute(total_condensate_zonal),\n",
    "                                 levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\") \n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "            \n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    elif VAR3=='precipitating_condensate_g/m3':\n",
    "        th = DATA['THETA'][:, yy, x1:x2]\n",
    "        pi = DATA['PI'][:, yy, x1:x2]\n",
    "        rv = DATA['RV'][:, yy, x1:x2]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        total_condensate_zonal = (DATA[\"RTP\"][:, yy, x1:x2]*1000.0 - DATA[\"RV\"] [:, yy, x1:x2]*1000.0 \\\n",
    "                                                                        - DATA[\"RCP\"][:, yy, x1:x2]*1000.0 \\\n",
    "                                                                        - DATA[\"RPP\"][:, yy, x1:x2]*1000.0)*dens\n",
    "        print('shape of zonal crosssec total_condensate_zonal = ',np.shape(total_condensate_zonal))\n",
    "\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(XV1,z_3D_2D_slice, np.absolute(total_condensate_zonal),\n",
    "                                 levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")\n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "        \n",
    "    else:\n",
    "        print('please provide correct value of VAR3')\n",
    "\n",
    "    if VAR4:\n",
    "        if VAR4=='buoyancy':\n",
    "            th = DATA['THETA'][:, y1:y2, x1:x2]\n",
    "            pi = DATA['PI'][:, y1:y2, x1:x2]\n",
    "            rv = DATA['RV'][:, y1:y2, x1:x2]\n",
    "            pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "            temp = th*(pi/Cp) #\n",
    "            del(th,pi)\n",
    "            dens = np.array(pres/(Rd*temp*(1+0.61*rv)))\n",
    "            print('shape of zonal crosssec density : ',np.shape(dens))\n",
    "            density_hor_mean = np.mean(dens,axis=(1,2))\n",
    "            print('shape of zonal crosssec density_hor_mean : ',np.shape(density_hor_mean))\n",
    "            dens_perturbation = dens - density_hor_mean[:, np.newaxis,np.newaxis]\n",
    "            bouyancy = -1.0*9.81*dens_perturbation/dens\n",
    "            print('shape of zonal crosssec bouyancy : ',np.shape(bouyancy))\n",
    "            print('shape of zonal crosssec bouyancy cross-sec : ',np.shape(bouyancy[:,(y2-y1)//2 , :]))\n",
    "            print('shape of zonal crosssec XV1 : ',np.shape(XV1))\n",
    "            del(pres,temp,rv)\n",
    "            C112_buoyancy = AX.contour(XV1,z_3D_2D_slice,bouyancy[:, (y2-y1)//2, :],levels=LEVELS_VAR4,colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            AX.clabel(C112_buoyancy, inline=1, fontsize=10, fmt='%3.2f')\n",
    "\n",
    "        if VAR4=='xvort':\n",
    "            v = DATA['VP'][:, y1:y2, x1:x2]\n",
    "            w = DATA['WP'][:, y1:y2, x1:x2]\n",
    "            xvort = (np.gradient(w, DY, axis=1) - np.gradient(v, Z_1D, axis=0))\n",
    "            if LEVELS_VAR4:\n",
    "                C112_xvort   = AX.contour(XV1,z_3D_2D_slice,xvort[:, yy, x1:x2],levels=LEVELS_VAR4,colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            else:\n",
    "                C112_xvort   = AX.contour(XV1,z_3D_2D_slice,xvort[:, yy, x1:x2],colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "\n",
    "            AX.clabel(C112_xvort, inline=1, fontsize=10, fmt='%3.2f')\n",
    "            \n",
    "        if VAR4=='yvort':\n",
    "            u = DATA['UP'][:, y1:y2, x1:x2]\n",
    "            w = DATA['WP'][:, y1:y2, x1:x2]\n",
    "            yvort = (np.gradient(u, Z_1D, axis=0) - np.gradient(w, DX, axis=2))\n",
    "            if LEVELS_VAR4:\n",
    "                C112_yvort   = AX.contour(XV1,z_3D_2D_slice,yvort[:, yy, x1:x2],levels=LEVELS_VAR4,colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            else:\n",
    "                C112_yvort   = AX.contour(XV1,z_3D_2D_slice,yvort[:, yy, x1:x2],colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "\n",
    "            AX.clabel(C112_yvort, inline=1, fontsize=10, fmt='%3.2f')\n",
    "\n",
    "        if VAR4=='zvort':\n",
    "            u = DATA['UP'][:, y1:y2, x1:x2]\n",
    "            v = DATA['VP'][:, y1:y2, x1:x2]\n",
    "            zvort = (np.gradient(v, DX, axis=2) - np.gradient(u, DY, axis=1))\n",
    "            if LEVELS_VAR4:\n",
    "                C112_zvort   = AX.contour(XV1,z_3D_2D_slice,zvort[:, yy, x1:x2],levels=LEVELS_VAR4,colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            else:\n",
    "                C112_zvort   = AX.contour(XV1,z_3D_2D_slice,zvort[:, yy, x1:x2],colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "\n",
    "            AX.clabel(C112_zvort, inline=1, fontsize=10, fmt='%3.2f')\n",
    "            \n",
    "\n",
    "    C4zs = AX.plot(X_1D[x1:x2], TERR[yy, x1:x2]/1000., color='sienna', linewidth=3.6)\n",
    "    \n",
    "    if PLOT_SEGMENTATION_OUTPUT:\n",
    "        seg_filename=glob.glob('/nobackupp11/isingh2/tobac_tracking-main/'+DOMAIN+'_segmentation_mask_box_threshold_2_'+pd.to_datetime(FILENAMETIME).strftime('%Y%m%d%H%M%S')+'.nc')\n",
    "        print('segmentation file is: ',seg_filename[0])\n",
    "        print('feature# ',FEATURE_NUM)\n",
    "        segmentation_da = xr.open_dataset(seg_filename[0]).segmentation_mask\n",
    "        C_seg = AX.contour(XV1,z_3D_2D_slice, np.where(segmentation_da[:, yy, x1:x2]==FEATURE_NUM,1.0,0.0), levels=[0.9],  colors='green', linewidths=1.0, linestyles=\"-\") \n",
    "        AX.clabel(C_seg, inline=1, fontsize=13, fmt='%f')\n",
    "            \n",
    "    \n",
    "    title = 'Vertical cross-section (zonal) of $w$'\n",
    "    prodid = 'tests'+'_'+VAR1+'_th_vcross_zonal_'+'y' + \\\n",
    "        str(yy)+'_x1_'+str(x1)+'_x2_'+str(x2)+'.png'\n",
    "    units = 'm/s'#var1.attrs['units']  # '$ x 10^{-5} $'+\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    AX.text(0.55, 0.94, EXP_LABEL, fontsize=12,\n",
    "            verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "    \n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "        \n",
    "    AX.scatter(XPOS*DXY/1000.0,TERR[YPOS,XPOS]/1000.+ZPOS/1000.,marker='+',color='k',s=130.5)\n",
    "    \n",
    "     # Draw box that is used for averaging of hydrometeor conc. for filtering:\n",
    "    if CELL_DIM is not None:\n",
    "        #print('cell dimension is ',CELL_DIM,' grid points')\n",
    "        low_x_km  = np.max([(XPOS - (CELL_DIM/2.)),0.0])*DXY/1000.0\n",
    "        box_width_km = CELL_DIM*DXY/1000.0\n",
    "        #print('width of the box is ',box_width_km,' km')\n",
    "        low_z_grid  = int(np.max([ZPOS_GRID - (CELL_DIM / 2.),0]))\n",
    "        high_z_grid = int(np.min([ZPOS_GRID + (CELL_DIM / 2.),NUM_MODEL_LEVS]))\n",
    "        box_height_km = (Z_1D[high_z_grid] - Z_1D[low_z_grid])/1000.0\n",
    "        #print('width of the box is ',box_height_km,' km')\n",
    "        low_z_km   = (TERR[YPOS,XPOS] + Z_1D[low_z_grid])/1000.0\n",
    "        #print('bottom side of the box is ',low_z_km,' km high')\n",
    "        #print('lower left corner of the box is x= ',low_x_km, ' z=',low_z_km)\n",
    "        rect1  = AX.add_patch(Rectangle((low_x_km,low_z_km), box_width_km, box_height_km , color='green', fc = 'none',lw = 1.8))\n",
    "\n",
    "    PLOT_INCUS_COLUMN_MAXIMA=False\n",
    "    if PLOT_INCUS_COLUMN_MAXIMA:\n",
    "        BOX_SCALING_FACTOR = 1.5\n",
    "        y_small = int(np.max([YPOS - BOX_SCALING_FACTOR*(CELL_DIM/2.),0]))\n",
    "        y_large = int(YPOS + BOX_SCALING_FACTOR*(CELL_DIM / 2.))\n",
    "        x_small = int(np.max([XPOS - BOX_SCALING_FACTOR*(CELL_DIM/2.),0]))\n",
    "        x_large = int(XPOS + BOX_SCALING_FACTOR*(CELL_DIM / 2.))\n",
    "        low_z_grid  = int(np.max([ZPOS_GRID - BOX_SCALING_FACTOR*(CELL_DIM / 2.),0]))\n",
    "        high_z_grid = int(np.min([ZPOS_GRID + BOX_SCALING_FACTOR*(CELL_DIM / 2.),NUM_MODEL_LEVS]))\n",
    "        \n",
    "        wp_array = VAR2_XR[low_z_grid:high_z_grid, y_small:y_large, x_small:x_large].values\n",
    "        \n",
    "        print('shape of cell is ',np.shape(wp_array))\n",
    "        \n",
    "        ind_max = np.unravel_index(np.nanargmax(wp_array),wp_array.shape)\n",
    "        z_coord_wmax = ind_max[0]+low_z_grid\n",
    "        y_coord_wmax = ind_max[1]+y_small\n",
    "        x_coord_wmax = ind_max[2]+x_small\n",
    "    \n",
    "        print('index of w max is ',ind_max)\n",
    "        print('z coordinate is :', z_coord_wmax)\n",
    "        print('y coordinate is :', y_coord_wmax)\n",
    "        print('x coordinate is :', x_coord_wmax)\n",
    "        \n",
    "        zpos_wmax    = (TERR[y_coord_wmax,x_coord_wmax] + Z_1D[z_coord_wmax,np.newaxis])/1000.0\n",
    "        ypos_wmax    = y_coord_wmax*DXY/1000.0\n",
    "        xpos_wmax    = x_coord_wmax*DXY/1000.0\n",
    "        \n",
    "        print('z position is :',zpos_wmax)\n",
    "        print('y position is :',ypos_wmax)\n",
    "        print('x position is :',xpos_wmax)\n",
    "        \n",
    "\n",
    "        th = DATA['THETA'][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]\n",
    "        pi = DATA['PI'][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]\n",
    "        rv = DATA['RV'][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        cond_array =  (DATA[\"RTP\"][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]*1000.0 - DATA[\"RV\"][low_z_grid:high_z_grid, y_small:y_large,x_small:x_large]*1000.0)*dens\n",
    "        print('shape of cell is ',np.shape(cond_array))\n",
    "        ind_max = np.unravel_index(np.nanargmax(cond_array),cond_array.shape)\n",
    "        z_coord_condmax = ind_max[0]+low_z_grid\n",
    "        y_coord_condmax = ind_max[1]+y_small\n",
    "        x_coord_condmax = ind_max[2]+x_small\n",
    "        \n",
    "        print('index of cond max is ',ind_max)\n",
    "        print('z coordinate is :', z_coord_condmax)\n",
    "        print('y coordinate is :', y_coord_condmax)\n",
    "        print('x coordinate is :', x_coord_condmax)\n",
    "        zpos_condmax    =  (TERR[y_coord_wmax,x_coord_condmax] + Z_1D[z_coord_condmax,np.newaxis])/1000.0\n",
    "        ypos_condmax = y_coord_condmax*DXY/1000.0\n",
    "        xpos_condmax = x_coord_condmax*DXY/1000.0\n",
    "        print('z position is :',zpos_condmax)\n",
    "        print('y position is :',ypos_condmax)\n",
    "        print('x position is :',xpos_condmax)\n",
    "        \n",
    "        AX.scatter(xpos_wmax,zpos_wmax[0],s=130.5,marker='+',color='fuchsia')#facecolors='none',edgecolors='blue')\n",
    "        AX.scatter(xpos_condmax,zpos_condmax[0],s=130.5,marker='+',color='blue')#facecolors='none',edgecolors='blue')\n",
    "\n",
    "    \n",
    "    # plot \n",
    "    fig_process_vert(AX, TERR, C1, X_1D, x1, x2, yy, PLOT_CBAR, 0, 15, title, TITLETIME, FILENAMETIME, prodid, units, \"zonal\", HEIGHT, True, PANEL_PLOT)\n",
    "\n",
    "def plot_meridional_vertcross(DOMAIN, DATA, DX, DY ,TERR, Y_1D, Z_3D, Z_1D, DXY, VAR1, DESTAGGER, STAGGER_DIM, LEVELS_VAR1, CMAP_VAR1,\n",
    "                       VAR2_XR, LEVELS_VAR2, VAR2_COLOR,\n",
    "                       VAR3, LEVELS_VAR3, VAR3_COLOR,\n",
    "                       VAR4, LEVELS_VAR4, VAR4_COLOR,\n",
    "                       xx, y1, y2,\n",
    "                       PLOT_WINDS, PANEL_PLOT, HEIGHT, AX, PLOT_CBAR, EXP_LABEL, PANEL_LABEL, XPOS, YPOS, ZPOS, ZPOS_GRID, CELL_DIM, FEATURE_NUM, PLOT_SEGMENTATION_OUTPUT, NUM_MODEL_LEVS, TITLETIME, FILENAMETIME):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "    \n",
    "    x1 = xx - 50\n",
    "    x2 = xx + 50\n",
    "    \n",
    "    YV1, __ = np.meshgrid(Y_1D[y1:y2]/1000.0, Z_1D/1000.0)\n",
    "    z_3D_2D_slice    =  (TERR[y1:y2, xx] + Z_1D[:,np.newaxis])/1000.0\n",
    "\n",
    "    if DESTAGGER:\n",
    "        print(var1)\n",
    "        var1 = destagger(var1, STAGGER_DIM, meta=True)\n",
    "    else:\n",
    "        print(' ')\n",
    "\n",
    "    \n",
    "    if (isinstance(LEVELS_VAR1, np.ndarray)):\n",
    "        #C1 = AX.contourf(YV1, zh[:, y1:y2, xx]/1000.0, VAR2_XR[:, y1:y2, xx], levels=LEVELS_VAR1,\n",
    "        #                  cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "        C1 = AX.contourf(YV1, z_3D_2D_slice, VAR2_XR[:, y1:y2, xx], levels=LEVELS_VAR1,\n",
    "                          cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "    else:\n",
    "        C1 = AX.contourf(YV1, z_3D_2D_slice, VAR2_XR[:, y1:y2, xx],\n",
    "                          cmap=CMAP_VAR1, extend='both')\n",
    "\n",
    "        \n",
    "    #levels_th = np.arange(290.0, 690.0, 2.0)\n",
    "    #C4 = plt.contour(XV1, zh[:, yy, x1:x2]/1000.0, DATA.THETA.values[:, yy, x1:x2], colors='k', levels=levels_th, axis=AX, linewidth=0.6)\n",
    "    #plt.clabel(C4, inline=1, fontsize=14, fmt='%3.0f')\n",
    "\n",
    "        \n",
    "    if (isinstance(LEVELS_VAR2, np.ndarray)):\n",
    "        C2 = AX.contour(YV1, z_3D_2D_slice, VAR2_XR[:, y1:y2, xx],\n",
    "                             levels=LEVELS_VAR2, colors=VAR2_COLOR, linewidths=1., linestyles=\"-\")\n",
    "    else:\n",
    "        C2 = AX.contour(YV1,z_3D_2D_slice, VAR2_XR[:, y1:y2, xx],\n",
    "                             colors='k', linewidths=1., linestyles=\"--\")\n",
    "    AX.clabel(C2, inline=1, fontsize=10, fmt='%3.0f')\n",
    "\n",
    "\n",
    "    if VAR3=='RTP-RV_g/kg':\n",
    "        total_condensate_meridional = DATA[\"RTP\"][:, y1:y2, xx]*1000.0 - DATA[\"RV\"][:, y1:y2, xx]*1000.0\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(YV1, z_3D_2D_slice, np.absolute(total_condensate_meridional),\\\n",
    "                                levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    elif VAR3=='RTP-RV_g/m3':\n",
    "        th = DATA['THETA'][:, y1:y2, xx]\n",
    "        pi = DATA['PI'][:, y1:y2, xx]\n",
    "        rv = DATA['RV'][:, y1:y2, xx]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        total_condensate_meridional = (DATA[\"RTP\"][:, y1:y2, xx]*1000.0 - DATA[\"RV\"][:, y1:y2, xx]*1000.0)*dens\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(YV1, z_3D_2D_slice, np.absolute(total_condensate_meridional),\\\n",
    "                                levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")\n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    elif VAR3=='precipitating_condensate_g/m3':\n",
    "        th = DATA['THETA'][:, y1:y2, xx]\n",
    "        pi = DATA['PI'][:, y1:y2, xx]\n",
    "        rv = DATA['RV'][:, y1:y2, xx]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        total_condensate_meridional = (DATA[\"RTP\"][:, y1:y2, xx]*1000.0 - DATA[\"RV\"] [:, y1:y2, xx]*1000.0 \\\n",
    "                                                                        - DATA[\"RCP\"][:, y1:y2, xx]*1000.0 \\\n",
    "                                                                        - DATA[\"RPP\"][:, y1:y2, xx]*1000.0)*dens\n",
    "        \n",
    "        \n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(YV1, z_3D_2D_slice, np.absolute(total_condensate_meridional),\\\n",
    "                                levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")\n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print('please provide correct value of VAR3')\n",
    " \n",
    "        #del total_condensate_meridional\n",
    "           \n",
    "    if VAR4:\n",
    "        if VAR4=='buoyancy':\n",
    "            th = DATA['THETA'][:, y1:y2, x1:x2]\n",
    "            pi = DATA['PI'][:, y1:y2, x1:x2]\n",
    "            rv = DATA['RV'][:, y1:y2, x1:x2]\n",
    "            pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "            temp = th*(pi/Cp) #\n",
    "            del(th,pi)\n",
    "            dens = np.array(pres/(Rd*temp*(1+0.61*rv)))\n",
    "            print('shape of density : ',np.shape(dens))\n",
    "            density_hor_mean = np.mean(dens,axis=(1,2))\n",
    "            print('shape of density_hor_mean : ',np.shape(density_hor_mean))\n",
    "            dens_perturbation = dens - density_hor_mean[:, np.newaxis,np.newaxis]\n",
    "            bouyancy = -1.0*9.81*dens_perturbation/dens\n",
    "            del(pres,temp,rv)\n",
    "            C112_buoyancy   = AX.contour(YV1,z_3D_2D_slice,bouyancy[:, :, (x2-x1)//2],levels=LEVELS_VAR4,colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            AX.clabel(C112_buoyancy, inline=1, fontsize=10, fmt='%3.2f')\n",
    "\n",
    "        if VAR4=='xvort':\n",
    "            v = DATA['VP'][:, y1:y2, x1:x2]\n",
    "            w = DATA['WP'][:, y1:y2, x1:x2]\n",
    "            xvort = (np.gradient(w, DY, axis=1) - np.gradient(v, Z_1D, axis=0))\n",
    "            if LEVELS_VAR4:\n",
    "                C112_xvort   = AX.contour(YV1,z_3D_2D_slice,xvort[:, :, (x2-x1)//2],levels=LEVELS_VAR4,colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            else:\n",
    "                C112_xvort   = AX.contour(YV1,z_3D_2D_slice,xvort[:, :, (x2-x1)//2],colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "\n",
    "            AX.clabel(C112_xvort, inline=1, fontsize=10, fmt='%3.2f')\n",
    "            \n",
    "        if VAR4=='yvort':\n",
    "            u = DATA['UP'][:, y1:y2, x1:x2]\n",
    "            w = DATA['WP'][:, y1:y2, x1:x2]\n",
    "            yvort = (np.gradient(u, Z_1D, axis=0) - np.gradient(w, DX, axis=2))\n",
    "            if LEVELS_VAR4:\n",
    "                C112_yvort   = AX.contour(YV1,z_3D_2D_slice,yvort[:, :, (x2-x1)//2],levels=LEVELS_VAR4,colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            else:\n",
    "                C112_yvort   = AX.contour(YV1,z_3D_2D_slice,yvort[:, :, (x2-x1)//2],colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "\n",
    "            AX.clabel(C112_yvort, inline=1, fontsize=10, fmt='%3.2f')\n",
    "\n",
    "        if VAR4=='zvort':\n",
    "            u = DATA['UP'][:, y1:y2, x1:x2]\n",
    "            v = DATA['VP'][:, y1:y2, x1:x2]\n",
    "            zvort = (np.gradient(v, DX, axis=2) - np.gradient(u, DY, axis=1))\n",
    "            if LEVELS_VAR4:\n",
    "                C112_zvort   = AX.contour(YV1,z_3D_2D_slice,zvort[:, :, (x2-x1)//2],levels=LEVELS_VAR4,colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            else:\n",
    "                C112_zvort   = AX.contour(YV1,z_3D_2D_slice,zvort[:, :, (x2-x1)//2],colors=VAR4_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "\n",
    "            AX.clabel(C112_zvort, inline=1, fontsize=10, fmt='%3.2f')\n",
    "            \n",
    "#     if PLOT_WINDS:\n",
    "#         winds_thin_x = 4\n",
    "#         winds_thin_z = 4\n",
    "#         YVwind, ZVwind = np.meshgrid(yh[y1:y2:winds_thin_x], z[::winds_thin_z])\n",
    "#         v1 = DATA.variables[\"VP\"][::winds_thin_z,y1:y2:winds_thin_x, xx]*1.94384\n",
    "#         w1 = DATA.variables[\"WP\"][::winds_thin_z,y1:y2:winds_thin_x, xx]*1.94384\n",
    "#         #QV1 = AX.barbs(YVwind, zh[::winds_thin_z,y1:y2:winds_thin_x, xx]/1000.0,\n",
    "#         #               v1, w1, length=7.2, pivot='middle', linewidth=0.60, flip_barb=True)\n",
    "          \n",
    "\n",
    "    C4zs = AX.plot(Y_1D[y1:y2], TERR[y1:y2, xx]/1000., color='sienna', linewidth=3.6)\n",
    "    \n",
    "    if PLOT_SEGMENTATION_OUTPUT:\n",
    "        seg_filename=glob.glob('/nobackupp11/isingh2/tobac_tracking-main/'+DOMAIN+'_segmentation_mask_box_threshold_2_'+pd.to_datetime(FILENAMETIME).strftime('%Y%m%d%H%M%S')+'.nc')\n",
    "        print('segmentation file is: ',seg_filename[0])\n",
    "        print('feature# ',FEATURE_NUM)\n",
    "        segmentation_da = xr.open_dataset(seg_filename[0]).segmentation_mask\n",
    "        C_seg = AX.contour(YV1, z_3D_2D_slice, np.where(segmentation_da[:, y1:y2, xx]==FEATURE_NUM,1.0,0.0), levels=[0.9],colors='green', linewidths=1.0, linestyles=\"-\")\n",
    "        AX.clabel(C_seg, inline=1, fontsize=13, fmt='%f')\n",
    "        \n",
    "    title = 'Vertical cross-section (meridional) of $w$'\n",
    "    prodid = 'tests'+'_'+VAR1+'_th_vcross_zonal_'+'xx' + \\\n",
    "        str(xx)+'_y1_'+str(y1)+'_y2_'+str(y2)+'.png'\n",
    "    units = 'm/s'#var1.attrs['units']  # '$ x 10^{-5} $'+\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    AX.text(0.55, 0.94, EXP_LABEL, fontsize=12,\n",
    "            verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "    \n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "    \n",
    "    AX.scatter(YPOS*DXY/1000.0,TERR[YPOS,XPOS]/1000.+ZPOS/1000.,marker='+',color='k',s=130.5)\n",
    "    \n",
    "    # Draw box that is used for averaging of hydrometeor conc. for filtering:\n",
    "    if CELL_DIM is not None:\n",
    "        low_y_km  = np.max([(YPOS - (CELL_DIM/2.)),0.0])*DXY/1000.0\n",
    "        box_width_km = CELL_DIM*DXY/1000.0\n",
    "        low_z_grid  = int(np.max([ZPOS_GRID - (CELL_DIM / 2.),0]))\n",
    "        high_z_grid = int(np.min([ZPOS_GRID + (CELL_DIM / 2.),NUM_MODEL_LEVS]))\n",
    "        box_height_km = (Z_1D[high_z_grid] - Z_1D[low_z_grid])/1000.0\n",
    "        low_z_km   = (TERR[YPOS,XPOS] + Z_1D[low_z_grid])/1000.0\n",
    "        rect1  =  AX.add_patch(Rectangle((low_y_km,low_z_km), box_width_km, box_height_km , color='green', fc = 'none',lw = 1.8))\n",
    "\n",
    "    PLOT_INCUS_COLUMN_MAXIMA=False\n",
    "    if PLOT_INCUS_COLUMN_MAXIMA:\n",
    "        BOX_SCALING_FACTOR = 1.5\n",
    "        y_small = int(np.max([YPOS - BOX_SCALING_FACTOR*(CELL_DIM/2.),0]))\n",
    "        y_large = int(YPOS + BOX_SCALING_FACTOR*(CELL_DIM / 2.))\n",
    "        x_small = int(np.max([XPOS - BOX_SCALING_FACTOR*(CELL_DIM/2.),0]))\n",
    "        x_large = int(XPOS + BOX_SCALING_FACTOR*(CELL_DIM / 2.))\n",
    "        low_z_grid  = int(np.max([ZPOS_GRID - BOX_SCALING_FACTOR*(CELL_DIM / 2.),0]))\n",
    "        high_z_grid = int(np.min([ZPOS_GRID + BOX_SCALING_FACTOR*(CELL_DIM / 2.),NUM_MODEL_LEVS]))\n",
    "        \n",
    "        wp_array = VAR2_XR[low_z_grid:high_z_grid, y_small:y_large, x_small:x_large].values\n",
    "        \n",
    "        print('shape of cell is ',np.shape(wp_array))\n",
    "        \n",
    "        ind_max = np.unravel_index(np.nanargmax(wp_array),wp_array.shape)\n",
    "        z_coord_wmax = ind_max[0]+low_z_grid\n",
    "        y_coord_wmax = ind_max[1]+y_small\n",
    "        x_coord_wmax = ind_max[2]+x_small\n",
    "    \n",
    "        print('index of w max is ',ind_max)\n",
    "        print('z coordinate is :', z_coord_wmax)\n",
    "        print('y coordinate is :', y_coord_wmax)\n",
    "        print('x coordinate is :', x_coord_wmax)\n",
    "        \n",
    "        zpos_wmax    = (TERR[y_coord_wmax,x_coord_wmax] + Z_1D[z_coord_wmax,np.newaxis])/1000.0\n",
    "        ypos_wmax    = y_coord_wmax*DXY/1000.0\n",
    "        xpos_wmax    = x_coord_wmax*DXY/1000.0\n",
    "        \n",
    "        print('z position is :',zpos_wmax)\n",
    "        print('y position is :',ypos_wmax)\n",
    "        print('x position is :',xpos_wmax)\n",
    "        \n",
    "\n",
    "        th = DATA['THETA'][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]\n",
    "        pi = DATA['PI'][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]\n",
    "        rv = DATA['RV'][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        cond_array =  (DATA[\"RTP\"][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]*1000.0 - DATA[\"RV\"][low_z_grid:high_z_grid, y_small:y_large,x_small:x_large]*1000.0)*dens\n",
    "        print('shape of cell is ',np.shape(cond_array))\n",
    "        ind_max = np.unravel_index(np.nanargmax(cond_array),cond_array.shape)\n",
    "        z_coord_condmax = ind_max[0]+low_z_grid\n",
    "        y_coord_condmax = ind_max[1]+y_small\n",
    "        x_coord_condmax = ind_max[2]+x_small\n",
    "        \n",
    "        print('index of cond max is ',ind_max)\n",
    "        print('z coordinate is :', z_coord_condmax)\n",
    "        print('y coordinate is :', y_coord_condmax)\n",
    "        print('x coordinate is :', x_coord_condmax)\n",
    "        zpos_condmax    =  (TERR[y_coord_wmax,x_coord_condmax] + Z_1D[z_coord_condmax,np.newaxis])/1000.0\n",
    "        ypos_condmax = y_coord_condmax*DXY/1000.0\n",
    "        xpos_condmax = x_coord_condmax*DXY/1000.0\n",
    "        print('z position is :',zpos_condmax)\n",
    "        print('y position is :',ypos_condmax)\n",
    "        print('x position is :',xpos_condmax)\n",
    "        AX.scatter(ypos_wmax,zpos_wmax[0],s=130.5,marker='+',color='fuchsia')#facecolors='none',edgecolors='blue')\n",
    "        AX.scatter(ypos_condmax,zpos_condmax[0],s=130.5,marker='+',color='blue')#facecolors='none',edgecolors='blue')\n",
    "        #AX.axvline(x = y_small*DXY/1000.0, color = 'b')\n",
    "        #AX.axvline(x = y_large*DXY/1000.0, color = 'b')\n",
    "\n",
    "    fig_process_vert(AX, TERR, C1, Y_1D, y1, y2, xx, PLOT_CBAR, 0, 15, title, TITLETIME, FILENAMETIME, prodid, units, \"meridional\", HEIGHT, False, PANEL_PLOT)\n",
    " \n",
    "def plot_plan_view_cell(DOMAIN, DATA, DX, DY, TERR, X_1D, Y_1D, ZM, XPOS, YPOS, ZPOS, DXY, VAR1, LEVELS_VAR1, SINGLE_LEVEL_VAR1, CMAP_VAR1, VAR2, LEVELS_VAR2, VAR2_COLOR, x1,x2, y1, y2, FEATURE_NUM, PLOT_SEGMENTATION_OUTPUT,\n",
    "                    AX, PLOT_CBAR, EXP_LABEL, PANEL_LABEL,  TITLETIME, FILENAMETIME, XPOS_FEATURES=None,YPOS_FEATURES=None,EXTERNAL_FIELD=None):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "    \n",
    "    XH1, YH1 = np.meshgrid(X_1D[x1:x2]/1000.0,Y_1D[y1:y2]/1000.0)\n",
    "    vert_lev = np.argmin(np.abs(ZM-ZPOS))\n",
    "    \n",
    "    if VAR1=='EXTERNAL':\n",
    "        C111   = AX.contourf(XH1 ,YH1,EXTERNAL_FIELD[vert_lev,:,:] ,levels=LEVELS_VAR1,cmap= CMAP_VAR1,extend='both')#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8]) \n",
    "    else:\n",
    "        print('plotting field: ',VAR1)\n",
    "        var_to_plotted=DATA.variables[VAR1][vert_lev,y1:y2,x1:x2].values\n",
    "        C111   = AX.contourf(XH1 ,YH1,var_to_plotted ,levels=LEVELS_VAR1,cmap= CMAP_VAR1,extend='both')#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "        C112   = AX.contour(XH1 ,YH1,var_to_plotted ,levels=SINGLE_LEVEL_VAR1,colors='k')#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "        AX.clabel(C112, inline=1, fontsize=10, fmt='%3.0f')\n",
    "\n",
    "    if XPOS_FEATURES is not None:\n",
    "        tobac_features_scatter1 = AX.scatter(XPOS_FEATURES*DXY/1000.0,YPOS_FEATURES*DXY/1000.0,marker='^',s=100.5,c='green')#facecolors='none', edgecolors='green')\n",
    "        \n",
    "    if VAR2:\n",
    "        if VAR2=='buoyancy':\n",
    "            th = DATA['THETA'][vert_lev, y1:y2, x1:x2]\n",
    "            pi = DATA['PI'][vert_lev, y1:y2, x1:x2]\n",
    "            rv = DATA['RV'][vert_lev, y1:y2, x1:x2]\n",
    "            pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "            temp = th*(pi/Cp) #\n",
    "            del(th,pi)\n",
    "            dens = np.array(pres/(Rd*temp*(1+0.61*rv)))\n",
    "            print('shape of density : ',np.shape(dens))\n",
    "            density_hor_mean = np.mean(dens)\n",
    "            print('shape of density_hor_mean : ',np.shape(density_hor_mean))\n",
    "            dens_perturbation = dens - density_hor_mean[np.newaxis,np.newaxis]\n",
    "            bouyancy = -1.0*9.81*dens_perturbation/dens\n",
    "            del(pres,temp,rv)\n",
    "            if LEVELS_VAR2:\n",
    "                C112_buoyancy   = AX.contour(XH1 ,YH1,bouyancy,levels=LEVELS_VAR2,colors=VAR2_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            else:\n",
    "                C112_buoyancy   = AX.contour(XH1 ,YH1,bouyancy,colors='maroon')#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            \n",
    "            AX.clabel(C112_buoyancy, inline=1, fontsize=10, fmt='%3.2f')\n",
    "\n",
    "        if VAR2=='xvort':\n",
    "            v = DATA['VP'][:, y1:y2, x1:x2]\n",
    "            w = DATA['WP'][:, y1:y2, x1:x2]\n",
    "            xvort = (np.gradient(w, DY, axis=1) - np.gradient(v, ZM, axis=0))\n",
    "            if LEVELS_VAR4:\n",
    "                C112_xvort   = AX.contour(XH1 ,YH1,xvort[vert_lev,:,:],levels=LEVELS_VAR2,colors=VAR2_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            else:\n",
    "                C112_xvort   = AX.contour(XH1 ,YH1,xvort[vert_lev,:,:],colors=VAR2_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "\n",
    "            AX.clabel(C112_xvort, inline=1, fontsize=10, fmt='%3.2f')\n",
    "            \n",
    "        if VAR2=='yvort':\n",
    "            u = DATA['UP'][:, y1:y2, x1:x2]\n",
    "            w = DATA['WP'][:, y1:y2, x1:x2]\n",
    "            yvort = (np.gradient(u, ZM, axis=0) - np.gradient(w, DX, axis=2))\n",
    "            if LEVELS_VAR4:\n",
    "                C112_yvort   = AX.contour(XH1 ,YH1,yvort[vert_lev,:,:],levels=LEVELS_VAR2,colors=VAR2_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            else:\n",
    "                C112_yvort   = AX.contour(XH1 ,YH1,yvort[vert_lev,:,:],colors=VAR2_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "\n",
    "            AX.clabel(C112_yvort, inline=1, fontsize=10, fmt='%3.2f')\n",
    "\n",
    "        if VAR2=='zvort':\n",
    "            u = DATA['UP'][:, y1:y2, x1:x2]\n",
    "            v = DATA['VP'][:, y1:y2, x1:x2]\n",
    "            zvort = (np.gradient(v, DX, axis=2) - np.gradient(u, DY, axis=1))\n",
    "            if LEVELS_VAR4:\n",
    "                C112_zvort   = AX.contour(XH1 ,YH1,zvort[vert_lev,:,:],levels=LEVELS_VAR2,colors=VAR2_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "            else:\n",
    "                C112_zvort   = AX.contour(XH1 ,YH1,zvort[vert_lev,:,:],colors=VAR2_COLOR)#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "\n",
    "            AX.clabel(C112_zvort, inline=1, fontsize=10, fmt='%3.2f')\n",
    "\n",
    "    tobac_features_scatter = AX.scatter(XPOS*DXY/1000.0,YPOS*DXY/1000.0,marker='+',s=130.5,c='k')\n",
    "    \n",
    "    if PLOT_SEGMENTATION_OUTPUT:\n",
    "        seg_filename=glob.glob('/nobackupp11/isingh2/tobac_tracking-main/'+DOMAIN+'_segmentation_mask_box_threshold_2_'+pd.to_datetime(FILENAMETIME).strftime('%Y%m%d%H%M%S')+'.nc')\n",
    "        print('segmentation file is: ',seg_filename[0])\n",
    "        print('feature# ',FEATURE_NUM)\n",
    "        segmentation_da = xr.open_dataset(seg_filename[0]).segmentation_mask\n",
    "        C_plan_seg = AX.contour(XH1 ,YH1, np.where(segmentation_da[vert_lev, y1:y2, x1:x2]==FEATURE_NUM,1.0,0.0), levels=[0.9],colors='green', linewidths=1.0, linestyles=\"-\")\n",
    "        AX.clabel(C_plan_seg, inline=1, fontsize=13, fmt='%f')\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "\n",
    "    if EXP_LABEL:\n",
    "        AX.text(0.55, 0.94, EXP_LABEL, fontsize=12,\n",
    "                verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "\n",
    "    AX.set_title('Vertical velocity at model level '+str(vert_lev)+'\\n'+TITLETIME,fontsize=16)\n",
    "    AX.set_xlabel('x-distance (km)',fontsize=16)\n",
    "    AX.set_ylabel('y-distance (km)',fontsize=16)\n",
    "    \n",
    "    if PLOT_CBAR:\n",
    "        print('not plotting colorbar for the plan view')\n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "        \n",
    "    return C111\n",
    "\n",
    "def plot_vert_zonal_meridional_crosssection_tobac(DOMAIN,TOBAC_DF,CELL_NO,XH,YH,ZZ_M,ZM,TERR,DXY,CMAP,OUTPUT_DIR):\n",
    "        tdata_neu=TOBAC_DF[TOBAC_DF['cell']==CELL_NO]\n",
    "        #print('tracking cell# ',cell_no, '(',str(jj+1), 'cell out of 20)')\n",
    "        print('this cell has '+str(len(tdata_neu))+' time steps')\n",
    "        xpos=tdata_neu.X.values.astype(int)\n",
    "        ypos=tdata_neu.Y.values.astype(int)\n",
    "        zpos=tdata_neu.zmn.values.astype(int)\n",
    "        zpos_grid=tdata_neu.vdim.values.astype(int)\n",
    "        #num=tdata_neu.num.values.astype(int)\n",
    "        times_tracked=tdata_neu.timestr.values\n",
    "        thresholds=tdata_neu.threshold_value.values\n",
    "        print(times_tracked)\n",
    "\n",
    "        ii = 0 \n",
    "\n",
    "        for tim in times_tracked[0:2]:\n",
    "            print('timestep '+str(ii)+': '+tim)\n",
    "            tim_pd = pd.to_datetime(tim)\n",
    "            rams_fil=simulation_base_folder+'a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5'\n",
    "            print('RAMS date file: ',rams_fil)\n",
    "            rams_fil_da=xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "            titletime= get_time_from_RAMS_file(rams_fil)[0]\n",
    "            filenametime= get_time_from_RAMS_file(rams_fil)[1]\n",
    "            wpp = rams_fil_da[\"WP\"]\n",
    "            #print('size of total condensate is ',sys.getsizeof(total_condensate)/1000000)\n",
    "            fig = plt.figure(figsize=(11, 11), frameon=False)  # (16,11)\n",
    "            #ax1=plt.gca()\n",
    "            ax1 = plt.subplot(1, 2, 1)\n",
    "            ax2 = plt.subplot(1, 2, 2)\n",
    "            print('threshold for identification of updraft for this cell is : ',thresholds[0])\n",
    "            plot_zonal_vertcross     (rams_fil_da, TERR, XH, ZZ_M, ZM, DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                       wpp,np.array([thresholds[ii]]), 'k',\n",
    "                                       'precipitating_condensate_g/m3', np.array([0.05]), 'purple',\n",
    "                                       None, np.array([5]), 'green',\n",
    "                                       ypos[ii], min(xpos)-50, max(xpos)+50 ,\n",
    "                                       False, False, 16.0, ax1,'cell#'+str(CELL_NO)+'\\nxpos:'+str(xpos[ii])+' gr pt'+'\\nypos:'+str(ypos[ii])+' gr pt'+'\\nzpos:'+str(zpos[ii])+' m','(a)',xpos[ii],zpos[ii],titletime, filenametime)\n",
    "            \n",
    "            \n",
    "            plot_meridional_vertcross(rams_fil_da, TERR, YH, ZZ_M, ZM,DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                      wpp,np.array([thresholds[ii]]), 'k',\n",
    "                                      'precipitating_condensate_g/m3', np.array([0.05]), 'purple',\n",
    "                                      None, np.array([5]), 'green',\n",
    "                                      xpos[ii], min(ypos)-50, max(ypos)+50 ,\n",
    "                                      False, False, 16.0, ax2,'cell#'+str(CELL_NO)+'\\nxpos:'+str(xpos[ii])+' gr pt'+'\\nypos:'+str(ypos[ii])+' gr pt'+'\\nzpos:'+str(zpos[ii])+' m','(b)',ypos[ii],zpos[ii],titletime,filenametime)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            #png_file='vert_cross_cell_num'+str(cell_no)+'_xpos'+str(xpos[ii])+'_ypos'+str(ypos[ii])+'_zpos'+str(zpos[ii])+'_timestep'+tim_pd.strftime(\"%Y%m%d%H%M%S\")+'.png'\n",
    "            png_file=OUTPUT_DIR+'vert_cross_'+DOMAIN+'_eq_UD_thres-1-2-5-10-20'+'_cellno'+str(CELL_NO)+'_timestep'+tim_pd.strftime(\"%Y%m%d%H%M%S\")+'.png'\n",
    "            print(png_file)\n",
    "            plt.savefig(png_file,dpi=150)\n",
    "            #plt.close()\n",
    "            ii = ii + 1\n",
    "        print('============================================\\n\\n')  \n",
    "    \n",
    "def plot_vert_zonal_meridional_crosssection_plan_view_tobac(DOMAIN,TOBAC_DF,CELL_NO,XH,YH,ZZ_M,ZM,TERR,DXY,CMAP,OUTPUT_DIR,EXPERIMENT_MARKER,TOBAC_FEATURES_DF=None):\n",
    "        tdata_neu=TOBAC_DF[TOBAC_DF['cell']==CELL_NO]\n",
    "        #print('subset of tracking dataframe for this cell: ',tdata_neu)\n",
    "        #print('tracking cell# ',cell_no, '(',str(jj+1), 'cell out of 20)')\n",
    "        print('this cell has '+str(len(tdata_neu))+' time steps')\n",
    "        xpos=list(tdata_neu.X.values.astype(int))\n",
    "        ypos=list(tdata_neu.Y.values.astype(int))\n",
    "        zpos=list(tdata_neu.zmn.values.astype(int))    \n",
    "        zpos_grid=list(tdata_neu.vdim.values.astype(int))\n",
    "        features=list(tdata_neu.feature.values)\n",
    "        cell_dim =list(np.array(tdata_neu.num.values.astype(int))**(1/3))\n",
    "        times_tracked=tdata_neu.timestr.values\n",
    "        times_tracked_pd = pd.to_datetime(times_tracked)\n",
    "        thresholds=tdata_neu.threshold_value.values\n",
    "        cell_labels=['cell#'+str(CELL_NO)+'\\nfeature#'+str(features[kk])+'\\nxpos:'+str(xpos[kk])+' gr pt'+'\\nypos:'+str(ypos[kk])+' gr pt'+'\\nzpos:'+str(zpos[kk])+' m' for kk in range(len(xpos))]\n",
    "        \n",
    "        #original_cell_labels=['cell#'+str(CELL_NO)+'\\nfeature#'+str(features[kk])+'\\nxpos:'+str(xpos[kk])+' gr pt'+'\\nypos:'+str(ypos[kk])+' gr pt'+'\\nzpos:'+str(zpos[kk])+' m' for kk in range(len(xpos))]\n",
    "        \n",
    "\n",
    "        ii = 0 \n",
    "\n",
    "        for tim_pd in times_tracked_pd:\n",
    "            print('timestep '+str(ii)+': '+tim_pd.strftime(\"%Y-%m-%d-%H:%M:%S\"))\n",
    "            #tim_pd = pd.to_datetime(tim)\n",
    "            rams_fil=simulation_base_folder+'a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5'\n",
    "            print('RAMS date file: ',rams_fil)\n",
    "            rams_fil_da=xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "            titletime= get_time_from_RAMS_file(rams_fil)[0]\n",
    "            filenametime= get_time_from_RAMS_file(rams_fil)[1]\n",
    "            wpp = rams_fil_da[\"WP\"]\n",
    "            \n",
    "            \n",
    "            if TOBAC_FEATURES_DF is not None:\n",
    "                # plot features that are wiithin 1 km of the vertical level of the cell that is being plotted\n",
    "                tdata_feat=TOBAC_FEATURES_DF[(TOBAC_FEATURES_DF['time']==tim_pd)   & (abs(TOBAC_FEATURES_DF['zmn']-zpos[ii])<=2000.) & \\\n",
    "                                             (TOBAC_FEATURES_DF['X']>=xpos[ii]-50) & (TOBAC_FEATURES_DF['X']<=xpos[ii]+50)          & \\\n",
    "                                             (TOBAC_FEATURES_DF['Y']>=ypos[ii]-50) & (TOBAC_FEATURES_DF['Y']<=ypos[ii]+50)]\n",
    "                                            \n",
    "                xpos_feat=np.array((tdata_feat.X.values.astype(int)))\n",
    "                ypos_feat=np.array((tdata_feat.Y.values.astype(int)))\n",
    "            else:\n",
    "                xpos_feat=None\n",
    "                ypos_feat=None\n",
    "            \n",
    "            \n",
    "            #print('size of total condensate is ',sys.getsizeof(total_condensate)/1000000)\n",
    "            fig = plt.figure(figsize=(15, 9), frameon=False)  # (16,11)\n",
    "            #ax1=plt.gca()\n",
    "            ax1 = plt.subplot(1, 3, 1)\n",
    "            ax1.set_aspect('equal', adjustable='box')\n",
    "            ax2 = plt.subplot(1, 3, 2)\n",
    "            #ax2.set_aspect('equal', adjustable='box')\n",
    "            ax3 = plt.subplot(1, 3, 3)\n",
    "            #ax3.set_aspect('equal', adjustable='box')\n",
    "            print('threshold for identification of updraft for this cell is : ',thresholds[0])\n",
    "            \n",
    "            cbar_conts = plot_plan_view_cell      (DOMAIN, rams_fil_da, 100.0, 100.0, TERR, XH, YH, ZM, xpos[ii], ypos[ii], zpos[ii], DXY, 'WP', np.arange(-20.,20.1,.1),[thresholds[0]], CMAP, None, [0.01], 'maroon', min(xpos)-50, max(xpos)+50,  min(ypos)-50,  max(ypos)+50, features[ii], False,\n",
    "                                      ax1, False,cell_labels[ii], '(a)'  ,titletime, filenametime,xpos_feat,ypos_feat)\n",
    "            print('plan view done')\n",
    "            plot_zonal_vertcross     (DOMAIN, rams_fil_da,  100.0, 100.0, TERR, XH, ZZ_M, ZM, DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                       wpp,np.array([thresholds[ii]]), 'k',\n",
    "                                       'RTP-RV_g/m3', np.array([0.05]), 'purple',\n",
    "                                       None,[0.01], 'maroon',\n",
    "                                       ypos[ii], min(xpos)-50, max(xpos)+50 ,\n",
    "                                       False, False, 15.0, ax2,False,cell_labels[ii],'(b)',xpos[ii],ypos[ii],zpos[ii],zpos_grid[ii],cell_dim[ii], features[ii], False, 231, titletime,filenametime)\n",
    "            print('vertical zonal cross-section done')\n",
    "         \n",
    "            plot_meridional_vertcross(DOMAIN, rams_fil_da,  100.0, 100.0,TERR, YH, ZZ_M, ZM,DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                      wpp,np.array([thresholds[ii]]), 'k',\n",
    "                                      'RTP-RV_g/m3', np.array([0.05]), 'purple',\n",
    "                                      None, [0.01], 'maroon',\n",
    "                                      xpos[ii], min(ypos)-50, max(ypos)+50 ,\n",
    "                                      False, False, 15.0, ax3,False,cell_labels[ii],'(c)',xpos[ii],ypos[ii],zpos[ii],zpos_grid[ii],cell_dim[ii], features[ii], False, 231,titletime,filenametime)\n",
    "            print('vertical meridional cross-section done')\n",
    "            \n",
    "            cb_ax = fig.add_axes([0.2, 0.0001, 0.6, 0.02])  # two panels\n",
    "            #[left, bottom, width, height]\n",
    "            cbar  = fig.colorbar(cbar_conts, cax=cb_ax, orientation = 'horizontal')\n",
    "            #cb = fig.colorbar(cbar_conts, ax=(ax1, ax2,ax3), orientation='horizontal')\n",
    "            plt.tight_layout()\n",
    "            #png_file='vert_cross_cell_num'+str(cell_no)+'_xpos'+str(xpos[ii])+'_ypos'+str(ypos[ii])+'_zpos'+str(zpos[ii])+'_timestep'+tim_pd.strftime(\"%Y%m%d%H%M%S\")+'.png'\n",
    "            png_file=OUTPUT_DIR+'three_panel_'+DOMAIN+'_'+EXPERIMENT_MARKER+'_UD_thres-1-2-5-10-20'+'_cellno'+str(CELL_NO)+'_timestep'+tim_pd.strftime(\"%Y%m%d%H%M%S\")+'.png'\n",
    "            print(png_file)\n",
    "            plt.savefig(png_file,dpi=150)\n",
    "            #plt.close()\n",
    "            ii = ii + 1\n",
    "        print('============================================\\n\\n')\n",
    "        \n",
    "def plot_vert_zonal_meridional_crosssection_plan_view_tobac_first_step(DOMAIN,TOBAC_DF,CELL_NO,XH,YH,ZZ_M,ZM,TERR,EXTERNAL_FIELD,DXY,CMAP,OUTPUT_DIR,EXPERIMENT_MARKER,TOBAC_FEATURES_DF=None):\n",
    "        tdata_neu=TOBAC_DF[TOBAC_DF['cell']==CELL_NO]\n",
    "        #print('subset of tracking dataframe for this cell: ',tdata_neu)\n",
    "        #print('tracking cell# ',cell_no, '(',str(jj+1), 'cell out of 20)')\n",
    "        print('this cell has '+str(len(tdata_neu))+' time steps')\n",
    "        xpos=list(tdata_neu.X.values.astype(int))\n",
    "        ypos=list(tdata_neu.Y.values.astype(int))\n",
    "        zpos=list(tdata_neu.zmn.values.astype(int))    \n",
    "        zpos_grid=list(tdata_neu.vdim.values.astype(int))\n",
    "        features=list(tdata_neu.feature.values)\n",
    "        cell_dim =list(np.array(tdata_neu.num.values.astype(int))**(1/3))\n",
    "        times_tracked=tdata_neu.timestr.values\n",
    "        times_tracked_pd = pd.to_datetime(times_tracked)\n",
    "        thresholds=tdata_neu.threshold_value.values\n",
    "        cell_labels=['cell#'+str(CELL_NO)+'\\nfeature#'+str(features[kk])+'\\nxpos:'+str(xpos[kk])+' gr pt'+'\\nypos:'+str(ypos[kk])+' gr pt'+'\\nzpos:'+str(zpos[kk])+' m' for kk in range(len(xpos))]\n",
    "        \n",
    "        #original_cell_labels=['cell#'+str(CELL_NO)+'\\nfeature#'+str(features[kk])+'\\nxpos:'+str(xpos[kk])+' gr pt'+'\\nypos:'+str(ypos[kk])+' gr pt'+'\\nzpos:'+str(zpos[kk])+' m' for kk in range(len(xpos))]\n",
    "        \n",
    "\n",
    "        ii = 0 \n",
    "\n",
    "        for tim_pd in [times_tracked_pd[0],times_tracked_pd[-1]]:\n",
    "            print('timestep '+str(ii)+': '+tim_pd.strftime(\"%Y-%m-%d-%H:%M:%S\"))\n",
    "            #tim_pd = pd.to_datetime(tim)\n",
    "            rams_fil=simulation_base_folder+'a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5'\n",
    "            print('RAMS date file: ',rams_fil)\n",
    "            rams_fil_da=xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "            titletime= get_time_from_RAMS_file(rams_fil)[0]\n",
    "            filenametime= get_time_from_RAMS_file(rams_fil)[1]\n",
    "            wpp = rams_fil_da[\"WP\"]\n",
    "            \n",
    "            \n",
    "            if TOBAC_FEATURES_DF is not None:\n",
    "                # plot features that are wiithin 1 km of the vertical level of the cell that is being plotted\n",
    "                tdata_feat=TOBAC_FEATURES_DF[(TOBAC_FEATURES_DF['time']==tim_pd)   & (abs(TOBAC_FEATURES_DF['zmn']-zpos[ii])<=2000.) & \\\n",
    "                                             (TOBAC_FEATURES_DF['X']>=xpos[ii]-50) & (TOBAC_FEATURES_DF['X']<=xpos[ii]+50)          & \\\n",
    "                                             (TOBAC_FEATURES_DF['Y']>=ypos[ii]-50) & (TOBAC_FEATURES_DF['Y']<=ypos[ii]+50)]\n",
    "                                            \n",
    "                xpos_feat=np.array((tdata_feat.X.values.astype(int)))\n",
    "                ypos_feat=np.array((tdata_feat.Y.values.astype(int)))\n",
    "            else:\n",
    "                xpos_feat=None\n",
    "                ypos_feat=None\n",
    "            \n",
    "            \n",
    "            #print('size of total condensate is ',sys.getsizeof(total_condensate)/1000000)\n",
    "            fig = plt.figure(figsize=(15, 9), frameon=False)  # (16,11)\n",
    "            #ax1=plt.gca()\n",
    "            ax1 = plt.subplot(1, 3, 1)\n",
    "            ax1.set_aspect('equal', adjustable='box')\n",
    "            ax2 = plt.subplot(1, 3, 2)\n",
    "            #ax2.set_aspect('equal', adjustable='box')\n",
    "            ax3 = plt.subplot(1, 3, 3)\n",
    "            #ax3.set_aspect('equal', adjustable='box')\n",
    "            print('threshold for identification of updraft for this cell is : ',thresholds[0])\n",
    "            \n",
    "            cbar_conts = plot_plan_view_cell      (DOMAIN, rams_fil_da, 100.0, 100.0, TERR, XH, YH, ZM, xpos[ii], ypos[ii], zpos[ii], DXY, 'WP', np.arange(-20.,20.1,.1),[thresholds[0]], CMAP, None, [0.01], 'maroon', min(xpos)-50, max(xpos)+50,  min(ypos)-50,  max(ypos)+50, features[ii], False,\n",
    "                                      ax1, False,cell_labels[ii], '(a)'  ,titletime, filenametime,xpos_feat,ypos_feat)\n",
    "            print('plan view done')\n",
    "            plot_zonal_vertcross     (DOMAIN, rams_fil_da,  100.0, 100.0, TERR, XH, ZZ_M, ZM, DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                       wpp,np.array([thresholds[ii]]), 'k',\n",
    "                                       'RTP-RV_g/m3', np.array([0.05]), 'purple',\n",
    "                                       None,[0.01], 'maroon',\n",
    "                                       ypos[ii], min(xpos)-50, max(xpos)+50 ,\n",
    "                                       False, False, 15.0, ax2,False,cell_labels[ii],'(b)',xpos[ii],ypos[ii],zpos[ii],zpos_grid[ii],cell_dim[ii], features[ii], False, 231, titletime,filenametime)\n",
    "            print('vertical zonal cross-section done')\n",
    "         \n",
    "            plot_meridional_vertcross(DOMAIN, rams_fil_da,  100.0, 100.0,TERR, YH, ZZ_M, ZM,DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                      wpp,np.array([thresholds[ii]]), 'k',\n",
    "                                      'RTP-RV_g/m3', np.array([0.05]), 'purple',\n",
    "                                      None, [0.01], 'maroon',\n",
    "                                      xpos[ii], min(ypos)-50, max(ypos)+50 ,\n",
    "                                      False, False, 15.0, ax3,False,cell_labels[ii],'(c)',xpos[ii],ypos[ii],zpos[ii],zpos_grid[ii],cell_dim[ii], features[ii], False, 231,titletime,filenametime)\n",
    "            print('vertical meridional cross-section done')\n",
    "            \n",
    "            cb_ax = fig.add_axes([0.2, 0.0001, 0.6, 0.02])  # two panels\n",
    "            #[left, bottom, width, height]\n",
    "            cbar  = fig.colorbar(cbar_conts, cax=cb_ax, orientation = 'horizontal')\n",
    "            #cb = fig.colorbar(cbar_conts, ax=(ax1, ax2,ax3), orientation='horizontal')\n",
    "            plt.tight_layout()\n",
    "            #png_file='vert_cross_cell_num'+str(cell_no)+'_xpos'+str(xpos[ii])+'_ypos'+str(ypos[ii])+'_zpos'+str(zpos[ii])+'_timestep'+tim_pd.strftime(\"%Y%m%d%H%M%S\")+'.png'\n",
    "            png_file=OUTPUT_DIR+'three_panel_'+DOMAIN+'_'+EXPERIMENT_MARKER+'_UD_thres-1-2-5-10-20'+'_cellno'+str(CELL_NO)+'_timestep'+tim_pd.strftime(\"%Y%m%d%H%M%S\")+'.png'\n",
    "            print(png_file)\n",
    "            plt.savefig(png_file,dpi=150)\n",
    "            #plt.close()\n",
    "            ii = ii + 1\n",
    "        print('============================================\\n\\n')\n",
    "\n",
    "def plot_vert_zonal_meridional_crosssection_plan_view_tobac_comparison(DOMAIN,TOBAC_DF1,TOBAC_DF2,CELL_NO1,CELL_NO2,XH,YH,ZZ_M,ZM,TERR,DXY,CMAP,OUTPUT_DIR,EXPERIMENT_MARKER,TOBAC_FEATURES_DF=None):\n",
    "        tdata_neu1=TOBAC_DF1[TOBAC_DF1['cell']==CELL_NO1]\n",
    "        print('this cell has '+str(len(tdata_neu1))+' time steps')\n",
    "        xpos1=list(tdata_neu1.X.values.astype(int))\n",
    "        ypos1=list(tdata_neu1.Y.values.astype(int))\n",
    "        zpos1=list(tdata_neu1.zmn.values.astype(int))    \n",
    "        zpos_grid1=list(tdata_neu1.vdim.values.astype(int))\n",
    "        features1=list(tdata_neu1.feature.values)\n",
    "        cell_dim1 =list(np.array(tdata_neu1.num.values.astype(int))**(1/3))\n",
    "        times_tracked1=tdata_neu1.timestr.values\n",
    "        times_tracked_pd1 = pd.to_datetime(times_tracked1)\n",
    "        thresholds1=tdata_neu1.threshold_value.values\n",
    "        cell_labels1=['cell#'+str(CELL_NO1)+'\\nfeature#'+str(features1[kk])+'\\nxpos:'+str(xpos1[kk])+' gr pt'+'\\nypos:'+str(ypos1[kk])+' gr pt'+'\\nzpos:'+str(zpos1[kk])+' m' for kk in range(len(xpos1))]\n",
    "        print('cell one times vary from ',min(times_tracked_pd1),' to ',max(times_tracked_pd1))\n",
    "        \n",
    "        tdata_neu2=TOBAC_DF2[TOBAC_DF2['cell']==CELL_NO2]\n",
    "        print('this cell has '+str(len(tdata_neu2))+' time steps')\n",
    "        xpos2=list(tdata_neu2.X.values.astype(int))\n",
    "        ypos2=list(tdata_neu2.Y.values.astype(int))\n",
    "        zpos2=list(tdata_neu2.zmn.values.astype(int))    \n",
    "        zpos_grid2=list(tdata_neu2.vdim.values.astype(int))\n",
    "        features2=list(tdata_neu2.feature.values)\n",
    "        cell_dim2 =list(np.array(tdata_neu2.num.values.astype(int))**(1/3))\n",
    "        times_tracked2=tdata_neu2.timestr.values\n",
    "        times_tracked_pd2 = pd.to_datetime(times_tracked2)\n",
    "        thresholds2=tdata_neu2.threshold_value.values\n",
    "        cell_labels2=['cell#'+str(CELL_NO2)+'\\nfeature#'+str(features2[kk])+'\\nxpos:'+str(xpos2[kk])+' gr pt'+'\\nypos:'+str(ypos2[kk])+' gr pt'+'\\nzpos:'+str(zpos2[kk])+' m' for kk in range(len(xpos2))]\n",
    "        print('cell two times vary from ',min(times_tracked_pd2),' to ',max(times_tracked_pd2))\n",
    "              \n",
    "        start_time = min(min(times_tracked_pd1),min(times_tracked_pd2))\n",
    "        end_time = max(max(times_tracked_pd1),max(times_tracked_pd2))\n",
    "        all_times = pd.date_range(start=start_time, end=end_time, freq='30S')\n",
    "        print('\\n=======================')\n",
    "        print('plotting times vary from ',min(all_times),' to ',max(all_times))\n",
    "        print('=======================\\n')\n",
    "        \n",
    "        ii = 0 \n",
    "        ii1 = 0\n",
    "        ii2 = 0\n",
    "        for tim_pd in all_times:\n",
    "            print('-------------------------------------------------------')\n",
    "            if all_times[ii] in times_tracked_pd1:\n",
    "                switch_1=True\n",
    "            else:\n",
    "                switch_1=False\n",
    "                \n",
    "            if all_times[ii] in times_tracked_pd2:\n",
    "                switch_2=True\n",
    "            else:\n",
    "                switch_2=False\n",
    "                \n",
    "            print('timestep '+str(ii)+': '+tim_pd.strftime(\"%Y-%m-%d-%H:%M:%S\"))\n",
    "            #tim_pd = pd.to_datetime(tim)\n",
    "            rams_fil=simulation_base_folder+'a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5'\n",
    "            print('RAMS date file: ',rams_fil)\n",
    "            rams_fil_da=xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "            titletime= get_time_from_RAMS_file(rams_fil)[0]\n",
    "            filenametime= get_time_from_RAMS_file(rams_fil)[1]\n",
    "            wpp = rams_fil_da[\"WP\"]\n",
    "            \n",
    "            #COMPARISON PLOTTING STARTS HERE\n",
    "            fig = plt.figure(figsize=(15, 15), frameon=False)  # (16,11)\n",
    "            ax1 = plt.subplot(2, 3, 1)\n",
    "            ax1.set_aspect('equal', adjustable='box') # plot square plan view\n",
    "            ax2 = plt.subplot(2, 3, 2)\n",
    "            ax3 = plt.subplot(2, 3, 3)\n",
    "            ax4 = plt.subplot(2, 3, 4)\n",
    "            ax4.set_aspect('equal', adjustable='box') # plot square plan view\n",
    "            ax5 = plt.subplot(2, 3, 5)\n",
    "            ax6 = plt.subplot(2, 3, 6)\n",
    "            \n",
    "            # plot first row from tracking file#1\n",
    "            if switch_1:\n",
    "                print('\\nii1 = ',ii1,'\\n')\n",
    "                if TOBAC_FEATURES_DF is not None:\n",
    "                # plot features that are wiithin 1 km of the vertical level of the cell that is being plotted\n",
    "                    tdata_feat1=TOBAC_FEATURES_DF[(TOBAC_FEATURES_DF['time']==tim_pd)   & (abs(TOBAC_FEATURES_DF['zmn']-zpos1[ii1])<=2000.) & \\\n",
    "                                                 (TOBAC_FEATURES_DF['X']>=xpos1[ii1]-50) & (TOBAC_FEATURES_DF['X']<=xpos1[ii1]+50)          & \\\n",
    "                                                 (TOBAC_FEATURES_DF['Y']>=ypos1[ii1]-50) & (TOBAC_FEATURES_DF['Y']<=ypos1[ii1]+50)]\n",
    "                    xpos_feat1=np.array((tdata_feat1.X.values.astype(int)))\n",
    "                    ypos_feat1=np.array((tdata_feat1.Y.values.astype(int)))\n",
    "                else:\n",
    "                    xpos_feat1=None\n",
    "                    ypos_feat1=None\n",
    "               \n",
    "            \n",
    "                cbar_conts1 = plot_plan_view_cell      (DOMAIN, rams_fil_da, 100.0, 100.0, TERR, XH, YH, ZM, xpos1[ii1], ypos1[ii1], zpos1[ii1], DXY, 'WP', np.arange(-20.,20.1,.1),[thresholds1[ii1]], CMAP, None, [0.01], 'maroon', min(xpos1)-50, max(xpos1)+50,  min(ypos1)-50,  max(ypos1)+50, features1[ii1], False,\n",
    "                                          ax1, False,cell_labels1[ii1], '(a)'  ,titletime, filenametime,xpos_feat1,ypos_feat1)\n",
    "                print('plan view done #1')\n",
    "\n",
    "                plot_zonal_vertcross     (DOMAIN, rams_fil_da,  100.0, 100.0, TERR, XH, ZZ_M, ZM, DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                           wpp,np.array([thresholds1[ii1]]), 'k',\n",
    "                                           'RTP-RV_g/m3', np.array([0.05]), 'purple',\n",
    "                                           None,[0.01], 'maroon',\n",
    "                                           ypos1[ii1], min(xpos1)-50, max(xpos1)+50 ,\n",
    "                                           False, False, 15.0, ax2,False,cell_labels1[ii1],'(b)',xpos1[ii1],ypos1[ii1],zpos1[ii1],zpos_grid1[ii1],cell_dim1[ii1], features1[ii1], False, 231, titletime,filenametime)\n",
    "                print('vertical zonal cross-section done #1')\n",
    "\n",
    "                plot_meridional_vertcross(DOMAIN, rams_fil_da,  100.0, 100.0,TERR, YH, ZZ_M, ZM,DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                          wpp,np.array([thresholds1[ii1]]), 'k',\n",
    "                                          'RTP-RV_g/m3', np.array([0.05]), 'purple',\n",
    "                                          None, [0.01], 'maroon',\n",
    "                                          xpos1[ii1], min(ypos1)-50, max(ypos1)+50 ,\n",
    "                                          False, False, 15.0, ax3,False,cell_labels1[ii1],'(c)',xpos1[ii1],ypos1[ii1],zpos1[ii1],zpos_grid1[ii1],cell_dim1[ii1], features1[ii1], False, 231,titletime,filenametime)\n",
    "                print('vertical meridional cross-section done #1')\n",
    "                ii1=ii1+1\n",
    "            #-----#-----#-----#-----#-----#-----#-----#-----#-----#-----#-----\n",
    "            # plot second row from tracking file#2\n",
    "            if switch_2:\n",
    "                print('\\nii2 = ',ii2,'\\n')\n",
    "                if TOBAC_FEATURES_DF is not None:\n",
    "                    # plot features that are wiithin 1 km of the vertical level of the cell that is being plotted\n",
    "                    tdata_feat2=TOBAC_FEATURES_DF[(TOBAC_FEATURES_DF['time']==tim_pd)   & (abs(TOBAC_FEATURES_DF['zmn']-zpos2[ii2])<=2000.) & \\\n",
    "                                                 (TOBAC_FEATURES_DF['X']>=xpos2[ii2]-50) & (TOBAC_FEATURES_DF['X']<=xpos2[ii2]+50)          & \\\n",
    "                                                 (TOBAC_FEATURES_DF['Y']>=ypos2[ii2]-50) & (TOBAC_FEATURES_DF['Y']<=ypos2[ii2]+50)]\n",
    "\n",
    "                    xpos_feat2=np.array((tdata_feat2.X.values.astype(int)))\n",
    "                    ypos_feat2=np.array((tdata_feat2.Y.values.astype(int)))\n",
    "                else:\n",
    "                    xpos_feat2=None\n",
    "                    ypos_feat2=None\n",
    "            \n",
    "                cbar_conts1 = plot_plan_view_cell      (DOMAIN, rams_fil_da, 100.0, 100.0, TERR, XH, YH, ZM, xpos2[ii2], ypos2[ii2], zpos2[ii2], DXY, 'WP', np.arange(-20.,20.1,.1),[thresholds2[0]], CMAP, None, [0.01], 'maroon', min(xpos2)-50, max(xpos2)+50,  min(ypos2)-50,  max(ypos2)+50, features2[ii2], False,\n",
    "                                          ax4, False,cell_labels2[ii2], '(a)'  ,titletime, filenametime,xpos_feat2,ypos_feat2)\n",
    "                print('plan view done #2')\n",
    "\n",
    "                plot_zonal_vertcross     (DOMAIN, rams_fil_da,  100.0, 100.0, TERR, XH, ZZ_M, ZM, DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                           wpp,np.array([thresholds2[ii2]]), 'k',\n",
    "                                           'RTP-RV_g/m3', np.array([0.05]), 'purple',\n",
    "                                           None,[0.01], 'maroon',\n",
    "                                           ypos2[ii2], min(xpos2)-50, max(xpos2)+50 ,\n",
    "                                           False, False, 15.0, ax5,False,cell_labels2[ii2],'(b)',xpos2[ii2],ypos2[ii2],zpos2[ii2],zpos_grid2[ii2],cell_dim2[ii2], features2[ii2], False, 231, titletime,filenametime)\n",
    "                print('vertical zonal cross-section done #2')\n",
    "\n",
    "                plot_meridional_vertcross(DOMAIN, rams_fil_da,  100.0, 100.0,TERR, YH, ZZ_M, ZM,DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                          wpp,np.array([thresholds2[ii2]]), 'k',\n",
    "                                          'RTP-RV_g/m3', np.array([0.05]), 'purple',\n",
    "                                          None, [0.01], 'maroon',\n",
    "                                          xpos2[ii2], min(ypos2)-50, max(ypos2)+50 ,\n",
    "                                          False, False, 15.0, ax6,False,cell_labels2[ii2],'(c)',xpos2[ii2],ypos2[ii2],zpos2[ii2],zpos_grid2[ii2],cell_dim2[ii2], features2[ii2], False, 231,titletime,filenametime)\n",
    "                print('vertical meridional cross-section done #2')\n",
    "                ii2=ii2+1\n",
    "\n",
    "            cb_ax = fig.add_axes([0.2, 0.0001, 0.6, 0.02])  # two panels #[left, bottom, width, height]\n",
    "            cbar  = fig.colorbar(cbar_conts1, cax=cb_ax, orientation = 'horizontal')\n",
    "            plt.tight_layout()\n",
    "            png_file=OUTPUT_DIR+'three_panel_'+DOMAIN+'_'+EXPERIMENT_MARKER+'_UD_thres-1-2-5-10-20'+'_cell1_'+str(CELL_NO1)+'_cell2'+str(CELL_NO2)+'_timestep'+tim_pd.strftime(\"%Y%m%d%H%M%S\")+'.png'\n",
    "            print('\\n')\n",
    "            print(png_file)\n",
    "            plt.savefig(png_file,dpi=150)\n",
    "            plt.close()\n",
    "            ii = ii + 1\n",
    "        print('============================================\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# domain = 'DRC1.1-R'\n",
    "# print('working on simulation: ',domain)\n",
    "# simulation_base_folder= '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/'\n",
    "# tobac_tracking_dirpath = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'\n",
    "# tobac_features_dirpath = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'\n",
    "# les_path = simulation_base_folder\n",
    "# #\n",
    "# tobac_tracking_filename  = 'comb_track_filt_01_02_05_10_20.p'\n",
    "# tobac_features_filename  = 'comb_df_01_02_05_10_20.p'\n",
    "\n",
    "# tobac_tracking_filepath  = tobac_tracking_dirpath+tobac_tracking_filename\n",
    "# tobac_features_filepath  = tobac_features_dirpath+tobac_features_filename\n",
    "\n",
    "# #Grab all the rams files\n",
    "# h5filepath = les_path+'a-L*g3.h5'\n",
    "# h5files1 = sorted(glob.glob(h5filepath))\n",
    "# hefilepath = les_path+'a-L*head.txt'\n",
    "# hefiles1 = sorted(glob.glob(hefilepath))\n",
    "# #print(h5files1)\n",
    "# start_time=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "# end_time=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "# print('starting time in simulations: ',start_time)\n",
    "# print('ending time in simulations: ',end_time)\n",
    "\n",
    "\n",
    "# #### read in RAMS data file to get parameters for plotting ####\n",
    "# rams_terr=xr.open_dataset(h5files1[0],engine='h5netcdf', phony_dims='sort').TOPT.values\n",
    "\n",
    "\n",
    "# zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],h5files1[0])\n",
    "\n",
    "# xh=np.arange(dxy/2,nx*dxy,dxy)\n",
    "# yh=np.arange(dxy/2,ny*dxy,dxy)\n",
    "\n",
    "# ##### read in tobac data #####\n",
    "# print('reading tracking file: ',tobac_tracking_filepath)\n",
    "# tdata =          pd.read_pickle(tobac_tracking_filepath)\n",
    "\n",
    "# print('reading features file',tobac_features_filepath)\n",
    "# tdata_features = pd.read_pickle(tobac_features_filepath)\n",
    "\n",
    "# print('number of unique cells identified: ',len(tdata.cell.unique()))\n",
    "# all_cells=np.array(tdata.cell.unique())\n",
    "\n",
    "\n",
    "# #single processor below\n",
    "# print('number of unique cells : ',len(all_cells))\n",
    "# cl = random.choice(all_cells)\n",
    "# print('randomly chosen cell#: ',cl)\n",
    "\n",
    "#cl = 13277\n",
    "#plot_vert_zonal_meridional_crosssection_plan_view_tobac(domain,tdata,cl,xh,yh,None,zm,rams_terr,dxy,plt.get_cmap('bwr'),'/nobackupp11/isingh2/tobac_plots/','segmentaion_box_thres2',tdata_features)\n",
    "\n",
    "\n",
    "# multiprocessing below\n",
    "# cpu_count1 = cpu_count()\n",
    "# argument = []\n",
    "# for __ in range(100):\n",
    "#     cl = random.choice(all_cells)\n",
    "#     #for cl in all_cells:\n",
    "#     argument = argument + [(domain,tdata,cl,xh,yh,None,zm,rams_terr,dxy,\\\n",
    "#                             plt.get_cmap('bwr'),'/home3/isingh2/nobackup/tobac_plots/',\\\n",
    "#                             'check_box_wmax_condmax',tdata_features)]\n",
    "\n",
    "# print('will make plots for ',len(argument),' cells')\n",
    "\n",
    "\n",
    "\n",
    "# def main(FUNCTION, ARGUMENT):\n",
    "#     pool = Pool(cpu_count1-1)\n",
    "#     #pool = Pool(1)\n",
    "#     start_time = time.perf_counter()\n",
    "#     results = pool.starmap(FUNCTION, ARGUMENT)\n",
    "#     finish_time = time.perf_counter()\n",
    "#     print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(plot_vert_zonal_meridional_crosssection_plan_view_tobac, argument)\n",
    "\n",
    "    \n",
    "##################### PLOT TRACKING DIFFERENCES - SAME FEATURES ############################################\n",
    "# domain = 'USA1.1-R'\n",
    "# print('working on simulation: ',domain)\n",
    "# simulation_base_folder= '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/'\n",
    "# tobac_tracking_dirpath1 = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'\n",
    "# tobac_tracking_dirpath2 = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'\n",
    "# tobac_features_dirpath = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'\n",
    "# les_path = simulation_base_folder\n",
    "\n",
    "# tobac_tracking_filename1  = 'comb_track_01_02_05_10_20_sr10021.p'\n",
    "# tobac_tracking_filename2  = 'comb_track_01_02_05_10_20_sr10050.p'\n",
    "# tobac_features_filename  = 'comb_df_01_02_05_10_20.p'\n",
    "\n",
    "# tobac_tracking_filepath1  = tobac_tracking_dirpath1+tobac_tracking_filename1\n",
    "# tobac_tracking_filepath2  = tobac_tracking_dirpath2+tobac_tracking_filename2\n",
    "# tobac_features_filepath  = tobac_features_dirpath+tobac_features_filename\n",
    "\n",
    "# # Grab all the rams files\n",
    "# h5filepath = les_path+'a-L*g3.h5'\n",
    "# h5files1 = sorted(glob.glob(h5filepath))\n",
    "# hefilepath = les_path+'a-L*head.txt'\n",
    "# hefiles1 = sorted(glob.glob(hefilepath))\n",
    "# #print(h5files1)\n",
    "# start_time=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "# end_time=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "# print('starting time in simulations: ',start_time)\n",
    "# print('ending time in simulations: ',end_time)\n",
    "\n",
    "\n",
    "# #### read in RAMS data file to get parameters for plotting ####\n",
    "# rams_terr=xr.open_dataset(h5files1[0],engine='h5netcdf', phony_dims='sort').TOPT.values\n",
    "\n",
    "\n",
    "# zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],h5files1[0])\n",
    "\n",
    "# xh=np.arange(dxy/2,nx*dxy,dxy)\n",
    "# yh=np.arange(dxy/2,ny*dxy,dxy)\n",
    "\n",
    "# find_mismatched_cells=True\n",
    "# if find_mismatched_cells:\n",
    "#     ##### read in tobac data #####\n",
    "#     print('reading tracking file1: ',tobac_tracking_filepath1)\n",
    "#     tdata1 =          pd.read_pickle(tobac_tracking_filepath1)\n",
    "\n",
    "#     print('reading tracking file2: ',tobac_tracking_filepath2)\n",
    "#     tdata2 =          pd.read_pickle(tobac_tracking_filepath2)\n",
    "\n",
    "#     print('reading features file',tobac_features_filepath)\n",
    "#     tdata_features = pd.read_pickle(tobac_features_filepath)\n",
    "\n",
    "#     print('number of unique cells identified in tracking file 1: ',len(tdata1.cell.unique()))\n",
    "#     print('number of unique cells identified in tracking file 2: ',len(tdata2.cell.unique()))\n",
    "\n",
    "\n",
    "\n",
    "#     def filt_high_thres(g):\n",
    "#         return (g.threshold_value.max() >= 10.0)\n",
    "\n",
    "#     tdata_high_thr1=tdata1.groupby('cell').filter(filt_high_thres)\n",
    "#     print('number of unique cells identified in filtered tracking file 1: ',len(tdata_high_thr1.cell.unique()))\n",
    "\n",
    "#     tdata_high_thr2=tdata2.groupby('cell').filter(filt_high_thres)\n",
    "#     print('number of unique cells identified in filtered tracking file 2: ',len(tdata_high_thr2.cell.unique()))\n",
    "\n",
    "#     mismatched_cells=[]\n",
    "#     times_tracked=tdata_high_thr1.timestr.values\n",
    "#     times_tracked_pd = sorted(pd.to_datetime(times_tracked).unique())\n",
    "#     print('times vary from : ',min(times_tracked_pd))\n",
    "#     print('times vary from : ',max(times_tracked_pd))\n",
    "\n",
    "#     for tim_pd in times_tracked_pd:\n",
    "#         print(tim_pd)\n",
    "#         tdata_neu1=tdata_high_thr1[tdata_high_thr1['time']==tim_pd]\n",
    "#         tdata_neu2=tdata_high_thr2[tdata_high_thr2['time']==tim_pd]\n",
    "#         features=list(tdata_neu1.feature.values)\n",
    "#         print('number of features at this time: ',len(features),'\\n\\n')\n",
    "#         for feature in features:\n",
    "#             print('feature# ',feature)\n",
    "#             try:\n",
    "#                 cellno1= tdata_high_thr1.loc[tdata_high_thr1['feature'] == feature, 'cell'].values[0]\n",
    "#                 cellno2= tdata_high_thr2.loc[tdata_high_thr2['feature'] == feature, 'cell'].values[0]\n",
    "#             except IndexError:\n",
    "#                 print('index error')\n",
    "#                 pass\n",
    "\n",
    "#             print('cell# for this feature in tracking file1 = ',cellno1)\n",
    "#             print('cell# for this feature in tracking file2 = ',cellno2)\n",
    "\n",
    "#             tdata_cell1=tdata_high_thr1[tdata_high_thr1['cell']==cellno1]\n",
    "#             features_cell1 = tdata_cell1.feature.values\n",
    "\n",
    "#             tdata_cell2=tdata_high_thr2[tdata_high_thr2['cell']==cellno2]\n",
    "#             features_cell2 = tdata_cell2.feature.values\n",
    "\n",
    "#             if ((set(features_cell1)==set(features_cell2)) | (len(set(features_cell1) & set(features_cell2))/len(set(features_cell1)) <=0.2 )) :\n",
    "#                 print('cells are the same! :)')\n",
    "#             else:\n",
    "#                 print('\\nmismatch found!!\\n')\n",
    "#                 mismatched_cells.append((cellno1,cellno2))\n",
    "#             print('-----')\n",
    "\n",
    "#     mismatched_cell_list_final = list(set(mismatched_cells))\n",
    "#     print('here is the list of mismatched cells: ')\n",
    "#     print(mismatched_cell_list_final)\n",
    "#     print('-------------------------------------\\n\\n\\n')\n",
    "\n",
    "# #single processor\n",
    "# cl = random.choice(mismatched_cell_list_final)\n",
    "# print('random cell is ',cl)\n",
    "# for cl in mismatched_cell_list_final:\n",
    "#     cl1=cl[0]\n",
    "#     cl2=cl[1]\n",
    "#     plot_vert_zonal_meridional_crosssection_plan_view_tobac_comparison(domain,tdata_high_thr1,tdata_high_thr2,cl1,cl2,xh,yh,None,zm,rams_terr,dxy,plt.get_cmap('bwr'),'/nobackupp11/isingh2/tobac_plots/','tracking_comparison_100_21_21_vs_100_50_50',tdata_features)\n",
    "\n",
    "# multiprocessing below\n",
    "# cpu_count1 = cpu_count()\n",
    "# argument = []\n",
    "# for cl in mismatched_cell_list_final:\n",
    "#     cl1=cl[0]\n",
    "#     cl2=cl[1]\n",
    "#     argument = argument + [(domain,tdata_high_thr1,tdata_high_thr2,cl1,cl2,xh,yh,None,\\\n",
    "#                            zm,rams_terr,dxy,plt.get_cmap('bwr'),'/nobackupp11/isingh2/tobac_plots/',\\\n",
    "#                            'tracking_comparison_100_21_21_vs_100_50_50',tdata_features)]  \n",
    "\n",
    "# print('will make plots for ',len(argument),' cells')\n",
    "\n",
    "# def main(FUNCTION, ARGUMENT):\n",
    "#     pool = Pool(cpu_count1-1)\n",
    "#     #pool = Pool(1)\n",
    "#     start_time = time.perf_counter()\n",
    "#     results = pool.starmap(FUNCTION, ARGUMENT)\n",
    "#     finish_time = time.perf_counter()\n",
    "#     print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(plot_vert_zonal_meridional_crosssection_plan_view_tobac_comparison, argument)\n",
    "\n",
    "##################### PLOT TRACKING DIFFERENCES - DIFFERENT FEATURES ############################################\n",
    "##################### PLOT TRACKING DIFFERENCES - DIFFERENT FEATURES ############################################\n",
    "domain = 'DRC1.1-R'\n",
    "print('working on simulation: ',domain)\n",
    "\n",
    "#simulation_base_folder   = '/Users/isingh/SVH/INCUS/sample_LES_data/'+domain+'/'  # personal macbook\n",
    "simulation_base_folder  = '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/'      # Pleiades\n",
    "#tobac_tracking_dirpath1  = '/Users/isingh/SVH/INCUS/jupyter_nbks/tobac_thermals/peter_tobac_output/'+domain+'/' # personal macbook\n",
    "tobac_tracking_dirpath1 = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'   # Pleiades\n",
    "#tobac_tracking_dirpath2  = '/Users/isingh/SVH/INCUS/jupyter_nbks/tobac_thermals/peter_tobac_output/'+domain+'/' # personal macbook\n",
    "tobac_tracking_dirpath2 = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'  # Pleiades\n",
    "#tobac_features_dirpath  = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'  # personal macbook\n",
    "#tobac_features_dirpath  = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'  # Pleiades\n",
    "les_path                 = simulation_base_folder\n",
    "\n",
    "tobac_tracking_filename1  = 'comb_track_filt_01_02_05_10_20.p'\n",
    "tobac_tracking_filename2  = 'comb_track_filt_01_02_50_02_sr5017_setpos.p'\n",
    "#tobac_features_filename  = 'comb_df_01_02_05_10_20.p'\n",
    "\n",
    "tobac_tracking_filepath1  = tobac_tracking_dirpath1+tobac_tracking_filename1\n",
    "tobac_tracking_filepath2  = tobac_tracking_dirpath2+tobac_tracking_filename2\n",
    "#tobac_features_filepath  = tobac_features_dirpath+tobac_features_filename\n",
    "\n",
    "# Grab all the rams files\n",
    "h5filepath = les_path+'a-L*g3.h5'\n",
    "h5files1 = sorted(glob.glob(h5filepath))\n",
    "hefilepath = les_path+'a-L*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "#print(h5files1)\n",
    "start_time=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "end_time=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "print('starting time in simulations: ',start_time)\n",
    "print('ending time in simulations: ',end_time)\n",
    "\n",
    "\n",
    "#### read in RAMS data file to get parameters for plotting ####\n",
    "rams_terr=xr.open_dataset(h5files1[0],engine='h5netcdf', phony_dims='sort').TOPT.values\n",
    "\n",
    "zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],h5files1[0])\n",
    "\n",
    "xh=np.arange(dxy/2,nx*dxy,dxy)\n",
    "yh=np.arange(dxy/2,ny*dxy,dxy)\n",
    "\n",
    "find_mismatched_cells=True\n",
    "start_time = time.perf_counter()\n",
    "if find_mismatched_cells:\n",
    "    print('finding mismatched cell...\\n')\n",
    "    ##### read in tobac data #####\n",
    "    print('reading tracking file1: ',tobac_tracking_filepath1)\n",
    "    tdata1 =          pd.read_pickle(tobac_tracking_filepath1)\n",
    "\n",
    "    print('reading tracking file2: ',tobac_tracking_filepath2)\n",
    "    tdata2 =          pd.read_pickle(tobac_tracking_filepath2)\n",
    "\n",
    "    #print('reading features file',tobac_features_filepath)\n",
    "    #tdata_features = pd.read_pickle(tobac_features_filepath)\n",
    "\n",
    "    print('number of unique cells identified in tracking file 1: ',len(tdata1.cell.unique()))\n",
    "    print('number of unique cells identified in tracking file 2: ',len(tdata2.cell.unique()),'\\n')\n",
    "\n",
    "    def filt_high_thres(g):\n",
    "        return (g.threshold_value.max() >= 10.0)\n",
    "\n",
    "    tdata_high_thr1=tdata1.groupby('cell').filter(filt_high_thres)\n",
    "    print('number of unique cells identified in filtered tracking file 1: ',len(tdata_high_thr1.cell.unique()))\n",
    "\n",
    "    tdata_high_thr2=tdata2.groupby('cell').filter(filt_high_thres)\n",
    "    print('number of unique cells identified in filtered tracking file 2: ',len(tdata_high_thr2.cell.unique()),'\\n')\n",
    "\n",
    "    first_tracking_file_cells = tdata_high_thr1.cell.unique()\n",
    "    times_tracked=tdata_high_thr1.timestr.values\n",
    "    times_tracked_pd = sorted(pd.to_datetime(times_tracked).unique())\n",
    "    \n",
    "    print('times vary from : ',min(times_tracked_pd))\n",
    "    print('times vary from : ',max(times_tracked_pd),'\\n\\n')\n",
    "    \n",
    "    matched_cell_dictionary = {}\n",
    "    ii = 1\n",
    "    print('finding cells in tracking file#2 matching with the following cells in file#1: \\n')\n",
    "    for cl in first_tracking_file_cells:\n",
    "        potential_cells = []\n",
    "        matched_cell_dictionary.update({cl:[]})\n",
    "        print('working on cell#: ',cl,' - ',ii,'/',len(first_tracking_file_cells))\n",
    "        track_data_cell_subset = tdata_high_thr1[tdata_high_thr1.cell == cl]\n",
    "        #print('times in this cell: ',track_data_cell_subset.timestr.values)\n",
    "        all_times = track_data_cell_subset.timestr.values\n",
    "        all_xpos = track_data_cell_subset.X.values\n",
    "        all_ypos = track_data_cell_subset.Y.values\n",
    "    \n",
    "        for timestep,xpos,ypos in list(zip(all_times,all_xpos,all_ypos)):\n",
    "            #print('time: ',timestep)\n",
    "            second_df_time_subset = tdata_high_thr2[(tdata_high_thr2.timestr.values == timestep)  & \\\n",
    "                                                    (tdata_high_thr2.X.values > (xpos-0.25))      & \\\n",
    "                                                    (tdata_high_thr2.X.values < (xpos+0.25))      & \\\n",
    "                                                    (tdata_high_thr2.Y.values > (ypos-0.25))      & \\\n",
    "                                                    (tdata_high_thr2.Y.values < (ypos+0.25))]\n",
    "            if len(second_df_time_subset)>0:\n",
    "                potential_cells.extend(second_df_time_subset.cell.unique())\n",
    "           \n",
    "        potential_cells = list(set(potential_cells))\n",
    "        #print('potential cells that coincide with this cell: ',potential_cells)\n",
    "        if len(potential_cells) > 0:\n",
    "            for matched_cell in potential_cells:\n",
    "                matched_cell_dictionary[cl].append(matched_cell) \n",
    "\n",
    "        ii = ii + 1\n",
    "        \n",
    "        \n",
    "# #single processor\n",
    "cl1, cl2 = random.choice(list(matched_cell_dictionary.items()))\n",
    "print('matching cells are: ',cl1,' and ' ,cl2[0])\n",
    "#for cl1,cl2 in mismatched_cell_list_final:  \n",
    "#plot_vert_zonal_meridional_crosssection_plan_view_tobac_comparison(domain,tdata_high_thr1,tdata_high_thr2,cl1,cl2,xh,yh,None,zm,rams_terr,dxy,plt.get_cmap('bwr'),'/nobackupp11/isingh2/tobac_plots/','tracking_comparison_100_21_21_vs_100_50_50',tdata_features)\n",
    "\n",
    "# multiprocessing below\n",
    "# cpu_count1 = cpu_count()\n",
    "# argument = []\n",
    "# for cl in mismatched_cell_list_final:\n",
    "#     cl1=cl[0]\n",
    "#     cl2=cl[1]\n",
    "#     argument = argument + [(domain,tdata_high_thr1,tdata_high_thr2,cl1,cl2,xh,yh,None,\\\n",
    "#                            zm,rams_terr,dxy,plt.get_cmap('bwr'),'/nobackupp11/isingh2/tobac_plots/',\\\n",
    "#                            'tracking_comparison_100_21_21_vs_100_50_50',tdata_features)]  \n",
    "\n",
    "# print('will make plots for ',len(argument),' cells')\n",
    "\n",
    "# def main(FUNCTION, ARGUMENT):\n",
    "#     pool = Pool(cpu_count1-1)\n",
    "#     #pool = Pool(1)\n",
    "#     start_time = time.perf_counter()\n",
    "#     results = pool.starmap(FUNCTION, ARGUMENT)\n",
    "#     finish_time = time.perf_counter()\n",
    "#     print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(plot_vert_zonal_meridional_crosssection_plan_view_tobac_comparison, argument)\n",
    "\n",
    "finish_time = time.perf_counter()\n",
    "print(f\"Program finished in {finish_time-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505618af",
   "metadata": {},
   "source": [
    "## Vertical cross-sections different tracks different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f1119a",
   "metadata": {
    "code_folding": [
     58,
     141
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile plot_tobac_comparison_crosssections_different_tracks_different_features.py\n",
    "# Script created by:\n",
    "# Itinderjot Singh\n",
    "# Colorado State University\n",
    "# itinder@colostate.edu\n",
    "## plot features (besides the cell being tracked) in the plan view\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import numpy.ma as ma\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.patches import Rectangle\n",
    "import datetime\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from RAMS_Post_Process import fx_postproc_RAMS as RAMS_fx\n",
    "#import cartopy.crs as crs\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib import ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "cma1=plt.get_cmap('bwr')\n",
    "cma3=plt.get_cmap('tab20c')\n",
    "cma5=plt.get_cmap('gray_r')\n",
    "cma6=plt.get_cmap('rainbow')\n",
    "cma7=plt.get_cmap('Oranges')\n",
    "cma8=plt.get_cmap('coolwarm')\n",
    "#cma9=cma4.reversed()\n",
    "cma10=plt.get_cmap('gist_yarg')\n",
    "\n",
    "Cp=1004.\n",
    "Rd=287.0\n",
    "p00 = 100000.0\n",
    "\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def fig_process_vert(AX, TER, CONTOUR, XY_1D, XY1, XY2, Y_CROSS, PLOT_COLORBAR, CBAR_EXP, FONTSIZE, TITLESTRING, TIMESTRING, FILENAMESTRING, PROD, UNITS, VERT_CROSS, HEIGHT, YLABEL, IS_PANEL_PLOT):\n",
    "    #F = plt.gcf()  # Gets the current figure\n",
    "    #ax = plt.gca()  # Gets the current axes\n",
    "\n",
    "    if IS_PANEL_PLOT == False:\n",
    "        AX.set_title('%s (%s) \\n %s' % (TITLESTRING, UNITS, TIMESTRING),\n",
    "                  fontsize=FONTSIZE, stretch='normal')\n",
    "\n",
    "    if VERT_CROSS == \"zonal\":\n",
    "        AX.fill_between(XY_1D[XY1:XY2], 0, TER[int(Y_CROSS),\n",
    "                        XY1:XY2]/1000.0, facecolor='wheat')\n",
    "        AX.set_xlabel('x-distance (km)', fontsize=FONTSIZE)\n",
    "\n",
    "    else:\n",
    "        AX.fill_between(XY_1D[XY1:XY2], 0, TER[XY1:XY2,\n",
    "                        int(Y_CROSS)]/1000.0, facecolor='wheat')\n",
    "        AX.set_xlabel('y-distance (km)', fontsize=FONTSIZE)\n",
    "\n",
    "    AX.patch.set_color(\"white\")\n",
    "\n",
    "    if YLABEL:\n",
    "        AX.set_ylabel('Height (km)', fontsize=FONTSIZE)\n",
    "    AX.set_ylim([0, HEIGHT])\n",
    "    AX.set_xlim([XY1*100.0/1000.0, XY2*100.0/1000.0])\n",
    "\n",
    "    class OOMFormatter(matplotlib.ticker.ScalarFormatter):\n",
    "        def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n",
    "            self.oom = order\n",
    "            self.fformat = fformat\n",
    "            matplotlib.ticker.ScalarFormatter.__init__(\n",
    "                self, useOffset=offset, useMathText=mathText)\n",
    "\n",
    "        def _set_orderOfMagnitude(self, nothing):\n",
    "            self.orderOfMagnitude = self.oom\n",
    "\n",
    "        def _set_format(self, vmin, vmax):\n",
    "            self.format = self.fformat\n",
    "            if self._useMathText:\n",
    "                self.format = '$%s$' % matplotlib.ticker._mathdefault(\n",
    "                    self.format)\n",
    "\n",
    "    if PLOT_COLORBAR:\n",
    "        if IS_PANEL_PLOT == False:\n",
    "            if abs(CBAR_EXP):\n",
    "                divider = make_axes_locatable(AX)\n",
    "                cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.6)\n",
    "                bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",\n",
    "                                   format=OOMFormatter(CBAR_EXP, mathText=False),extend='both')\n",
    "                bar.ax.tick_params(labelsize=FONTSIZE-1)\n",
    "                #file_id = '%s_%s' % (PROD, FILENAMESTRING)\n",
    "                #filename = '%s.png' % (file_id)\n",
    "                #print(filename)\n",
    "                # Saves the figure with small margins\n",
    "                #plt.savefig(filename, dpi=my_dpi, bbox_inches='tight')\n",
    "            else:\n",
    "                divider = make_axes_locatable(AX)\n",
    "                cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.66)\n",
    "                bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",extend='both')\n",
    "                bar.ax.tick_params(labelsize=FONTSIZE-1)\n",
    "                #file_id = '%s_%s' % (PROD, FILENAMESTRING)\n",
    "                #filename = '%s.png' % (file_id)\n",
    "                #print(filename)\n",
    "                # Saves the figure with small margins\n",
    "                #plt.savefig(filename, dpi=my_dpi, bbox_inches='tight')\n",
    "\n",
    "        else:\n",
    "            if abs(CBAR_EXP):\n",
    "                divider = make_axes_locatable(AX)\n",
    "                cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.4)\n",
    "                bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",\n",
    "                                   format=OOMFormatter(CBAR_EXP, mathText=False),extend='both')\n",
    "                bar.ax.tick_params(labelsize=FONTSIZE-1)\n",
    "            else:\n",
    "                divider = make_axes_locatable(AX)\n",
    "                cax = divider.append_axes(\"bottom\", size=\"2%\", pad=0.4)\n",
    "                bar = plt.colorbar(CONTOUR, cax=cax, orientation=\"horizontal\",extend='both')\n",
    "                bar.ax.tick_params(labelsize=FONTSIZE-1)\n",
    "        # plt.close() This should remain commented. plt should be closed in the panel plot function\n",
    "        # if export_flag == 1:\n",
    "        # Convert the figure to a gif file\n",
    "        #os.system('convert -render -flatten %s %s.gif' % (filename, file_id))\n",
    "        #os.system('rm -f %s' % filename)\n",
    "\n",
    "def plot_zonal_vertcross(DOMAIN, DATA, DX, DY ,TERR, X_1D, Z_3D, Z_1D, DXY, VAR1, DESTAGGER, STAGGER_DIM, LEVELS_VAR1, CMAP_VAR1,\n",
    "                       VAR2_XR, LEVELS_VAR2, VAR2_COLOR,\n",
    "                       VAR3, LEVELS_VAR3, VAR3_COLOR,\n",
    "                       VAR4, LEVELS_VAR4, VAR4_COLOR,\n",
    "                       yy, x1, x2,\n",
    "                       PLOT_WINDS, PANEL_PLOT, HEIGHT, AX, PLOT_CBAR, EXP_LABEL, PANEL_LABEL, XPOS, YPOS, ZPOS, ZPOS_GRID, CELL_DIM, FEATURE_NUM, PLOT_SEGMENTATION_OUTPUT, NUM_MODEL_LEVS, TITLETIME, FILENAMETIME):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "\n",
    "    y1 = yy - 50\n",
    "    y2 = yy + 50\n",
    "       \n",
    "    XV1, __ = np.meshgrid(X_1D[x1:x2]/1000.0, Z_1D/1000.0)\n",
    "    z_3D_2D_slice    =  (TERR[yy,x1:x2] + Z_1D[:,np.newaxis])/1000.0\n",
    "\n",
    "    if DESTAGGER:\n",
    "        print(var1)\n",
    "        var1 = destagger(var1, STAGGER_DIM, meta=True)\n",
    "    else:\n",
    "        print(' ')\n",
    "\n",
    "    if (isinstance(LEVELS_VAR1, np.ndarray)):\n",
    "        C1 = AX.contourf(XV1,z_3D_2D_slice, VAR2_XR[:, yy, x1:x2], levels=LEVELS_VAR1,\n",
    "                           cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "    else:\n",
    "        C1 = AX.contourf(XV1, z_3D_2D_slice, VAR2_XR[:, yy, x1:x2],\n",
    "                           cmap=CMAP_VAR1, extend='both')\n",
    "       \n",
    "    if (isinstance(LEVELS_VAR2, np.ndarray)):\n",
    "        C2 = AX.contour(XV1, z_3D_2D_slice, VAR2_XR[:, yy, x1:x2],\n",
    "                             levels=LEVELS_VAR2, colors=VAR2_COLOR, linewidths=1., linestyles=\"-\")\n",
    "    else:\n",
    "        C2 = AX.contour(XV1, z_3D_2D_slice, VAR2_XR[:, yy, x1:x2],\n",
    "                             colors='k', linewidths=1., linestyles=\"--\")\n",
    "    AX.clabel(C2, inline=1, fontsize=10, fmt='%3.0f')\n",
    "\n",
    "    if VAR3=='RTP-RV_g/kg':\n",
    "        total_condensate_zonal = DATA[\"RTP\"][:, yy, x1:x2]*1000.0 - DATA[\"RV\"][:, yy, x1:x2]*1000.0\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(XV1,z_3D_2D_slice, np.absolute(total_condensate_zonal),\n",
    "                                 levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\") \n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "            \n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    elif VAR3=='RTP-RV_g/m3':\n",
    "        th = DATA['THETA'][:, yy, x1:x2]\n",
    "        pi = DATA['PI'][:, yy, x1:x2]\n",
    "        rv = DATA['RV'][:, yy, x1:x2]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        total_condensate_zonal = (DATA[\"RTP\"][:, yy, x1:x2]*1000.0 - DATA[\"RV\"] [:, yy, x1:x2]*1000.0)*dens\n",
    "        \n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(XV1,z_3D_2D_slice, np.absolute(total_condensate_zonal),\n",
    "                                 levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\") \n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "            \n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    elif VAR3=='precipitating_condensate_g/m3':\n",
    "        th = DATA['THETA'][:, yy, x1:x2]\n",
    "        pi = DATA['PI'][:, yy, x1:x2]\n",
    "        rv = DATA['RV'][:, yy, x1:x2]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        total_condensate_zonal = (DATA[\"RTP\"][:, yy, x1:x2]*1000.0 - DATA[\"RV\"] [:, yy, x1:x2]*1000.0 \\\n",
    "                                                                        - DATA[\"RCP\"][:, yy, x1:x2]*1000.0 \\\n",
    "                                                                        - DATA[\"RPP\"][:, yy, x1:x2]*1000.0)*dens\n",
    "        print('shape of zonal crosssec total_condensate_zonal = ',np.shape(total_condensate_zonal))\n",
    "\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(XV1,z_3D_2D_slice, np.absolute(total_condensate_zonal),\n",
    "                                 levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")\n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "        \n",
    "    else:\n",
    "        print('please provide correct value of VAR3')\n",
    "\n",
    "    if VAR4:\n",
    "        if VAR4=='buoyancy':\n",
    "            print('buoyancy') \n",
    "\n",
    "    C4zs = AX.plot(X_1D[x1:x2], TERR[yy, x1:x2]/1000., color='sienna', linewidth=3.6)\n",
    "    \n",
    "    if PLOT_SEGMENTATION_OUTPUT:\n",
    "        seg_filename=glob.glob('/nobackupp11/isingh2/tobac_tracking-main/'+DOMAIN+'_segmentation_mask_box_threshold_2_'+pd.to_datetime(FILENAMETIME).strftime('%Y%m%d%H%M%S')+'.nc')\n",
    "        print('segmentation file is: ',seg_filename[0])\n",
    "        print('feature# ',FEATURE_NUM)\n",
    "        segmentation_da = xr.open_dataset(seg_filename[0]).segmentation_mask\n",
    "        C_seg = AX.contour(XV1,z_3D_2D_slice, np.where(segmentation_da[:, yy, x1:x2]==FEATURE_NUM,1.0,0.0), levels=[0.9],  colors='green', linewidths=1.0, linestyles=\"-\") \n",
    "        AX.clabel(C_seg, inline=1, fontsize=13, fmt='%f')\n",
    "            \n",
    "    \n",
    "    title = 'Vertical cross-section (zonal) of $w$'\n",
    "    prodid = 'tests'+'_'+VAR1+'_th_vcross_zonal_'+'y' + \\\n",
    "        str(yy)+'_x1_'+str(x1)+'_x2_'+str(x2)+'.png'\n",
    "    units = 'm/s'#var1.attrs['units']  # '$ x 10^{-5} $'+\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    AX.text(0.55, 0.94, EXP_LABEL, fontsize=12,\n",
    "            verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "    \n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "        \n",
    "    AX.scatter(XPOS*DXY/1000.0,TERR[YPOS,XPOS]/1000.+ZPOS/1000.,marker='+',color='k',s=130.5)\n",
    "    \n",
    "     # Draw box that is used for averaging of hydrometeor conc. for filtering:\n",
    "    if CELL_DIM is not None:\n",
    "        #print('cell dimension is ',CELL_DIM,' grid points')\n",
    "        low_x_km  = np.max([(XPOS - (CELL_DIM/2.)),0.0])*DXY/1000.0\n",
    "        box_width_km = CELL_DIM*DXY/1000.0\n",
    "        #print('width of the box is ',box_width_km,' km')\n",
    "        low_z_grid  = int(np.max([ZPOS_GRID - (CELL_DIM / 2.),0]))\n",
    "        high_z_grid = int(np.min([ZPOS_GRID + (CELL_DIM / 2.),NUM_MODEL_LEVS]))\n",
    "        box_height_km = (Z_1D[high_z_grid] - Z_1D[low_z_grid])/1000.0\n",
    "        #print('width of the box is ',box_height_km,' km')\n",
    "        low_z_km   = (TERR[YPOS,XPOS] + Z_1D[low_z_grid])/1000.0\n",
    "        #print('bottom side of the box is ',low_z_km,' km high')\n",
    "        #print('lower left corner of the box is x= ',low_x_km, ' z=',low_z_km)\n",
    "        rect1  = AX.add_patch(Rectangle((low_x_km,low_z_km), box_width_km, box_height_km , color='green', fc = 'none',lw = 1.8))\n",
    "\n",
    "    PLOT_INCUS_COLUMN_MAXIMA=False\n",
    "    if PLOT_INCUS_COLUMN_MAXIMA:\n",
    "        BOX_SCALING_FACTOR = 1.5\n",
    "        y_small = int(np.max([YPOS - BOX_SCALING_FACTOR*(CELL_DIM/2.),0]))\n",
    "        y_large = int(YPOS + BOX_SCALING_FACTOR*(CELL_DIM / 2.))\n",
    "        x_small = int(np.max([XPOS - BOX_SCALING_FACTOR*(CELL_DIM/2.),0]))\n",
    "        x_large = int(XPOS + BOX_SCALING_FACTOR*(CELL_DIM / 2.))\n",
    "        low_z_grid  = int(np.max([ZPOS_GRID - BOX_SCALING_FACTOR*(CELL_DIM / 2.),0]))\n",
    "        high_z_grid = int(np.min([ZPOS_GRID + BOX_SCALING_FACTOR*(CELL_DIM / 2.),NUM_MODEL_LEVS]))\n",
    "        \n",
    "        wp_array = VAR2_XR[low_z_grid:high_z_grid, y_small:y_large, x_small:x_large].values\n",
    "        \n",
    "        print('shape of cell is ',np.shape(wp_array))\n",
    "        \n",
    "        ind_max = np.unravel_index(np.nanargmax(wp_array),wp_array.shape)\n",
    "        z_coord_wmax = ind_max[0]+low_z_grid\n",
    "        y_coord_wmax = ind_max[1]+y_small\n",
    "        x_coord_wmax = ind_max[2]+x_small\n",
    "    \n",
    "        print('index of w max is ',ind_max)\n",
    "        print('z coordinate is :', z_coord_wmax)\n",
    "        print('y coordinate is :', y_coord_wmax)\n",
    "        print('x coordinate is :', x_coord_wmax)\n",
    "        \n",
    "        zpos_wmax    = (TERR[y_coord_wmax,x_coord_wmax] + Z_1D[z_coord_wmax,np.newaxis])/1000.0\n",
    "        ypos_wmax    = y_coord_wmax*DXY/1000.0\n",
    "        xpos_wmax    = x_coord_wmax*DXY/1000.0\n",
    "        \n",
    "        print('z position is :',zpos_wmax)\n",
    "        print('y position is :',ypos_wmax)\n",
    "        print('x position is :',xpos_wmax)\n",
    "        \n",
    "\n",
    "        th = DATA['THETA'][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]\n",
    "        pi = DATA['PI'][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]\n",
    "        rv = DATA['RV'][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        cond_array =  (DATA[\"RTP\"][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]*1000.0 - DATA[\"RV\"][low_z_grid:high_z_grid, y_small:y_large,x_small:x_large]*1000.0)*dens\n",
    "        print('shape of cell is ',np.shape(cond_array))\n",
    "        ind_max = np.unravel_index(np.nanargmax(cond_array),cond_array.shape)\n",
    "        z_coord_condmax = ind_max[0]+low_z_grid\n",
    "        y_coord_condmax = ind_max[1]+y_small\n",
    "        x_coord_condmax = ind_max[2]+x_small\n",
    "        \n",
    "        print('index of cond max is ',ind_max)\n",
    "        print('z coordinate is :', z_coord_condmax)\n",
    "        print('y coordinate is :', y_coord_condmax)\n",
    "        print('x coordinate is :', x_coord_condmax)\n",
    "        zpos_condmax    =  (TERR[y_coord_wmax,x_coord_condmax] + Z_1D[z_coord_condmax,np.newaxis])/1000.0\n",
    "        ypos_condmax = y_coord_condmax*DXY/1000.0\n",
    "        xpos_condmax = x_coord_condmax*DXY/1000.0\n",
    "        print('z position is :',zpos_condmax)\n",
    "        print('y position is :',ypos_condmax)\n",
    "        print('x position is :',xpos_condmax)\n",
    "        \n",
    "        AX.scatter(xpos_wmax,zpos_wmax[0],s=130.5,marker='+',color='fuchsia')#facecolors='none',edgecolors='blue')\n",
    "        AX.scatter(xpos_condmax,zpos_condmax[0],s=130.5,marker='+',color='blue')#facecolors='none',edgecolors='blue')\n",
    "\n",
    "    \n",
    "    # plot \n",
    "    fig_process_vert(AX, TERR, C1, X_1D, x1, x2, yy, PLOT_CBAR, 0, 15, title, TITLETIME, FILENAMETIME, prodid, units, \"zonal\", HEIGHT, True, PANEL_PLOT)\n",
    "\n",
    "def plot_meridional_vertcross(DOMAIN, DATA, DX, DY ,TERR, Y_1D, Z_3D, Z_1D, DXY, VAR1, DESTAGGER, STAGGER_DIM, LEVELS_VAR1, CMAP_VAR1,\n",
    "                       VAR2_XR, LEVELS_VAR2, VAR2_COLOR,\n",
    "                       VAR3, LEVELS_VAR3, VAR3_COLOR,\n",
    "                       VAR4, LEVELS_VAR4, VAR4_COLOR,\n",
    "                       xx, y1, y2,\n",
    "                       PLOT_WINDS, PANEL_PLOT, HEIGHT, AX, PLOT_CBAR, EXP_LABEL, PANEL_LABEL, XPOS, YPOS, ZPOS, ZPOS_GRID, CELL_DIM, FEATURE_NUM, PLOT_SEGMENTATION_OUTPUT, NUM_MODEL_LEVS, TITLETIME, FILENAMETIME):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "    \n",
    "    x1 = xx - 50\n",
    "    x2 = xx + 50\n",
    "    \n",
    "    YV1, __ = np.meshgrid(Y_1D[y1:y2]/1000.0, Z_1D/1000.0)\n",
    "    z_3D_2D_slice    =  (TERR[y1:y2, xx] + Z_1D[:,np.newaxis])/1000.0\n",
    "\n",
    "    if DESTAGGER:\n",
    "        print(var1)\n",
    "        var1 = destagger(var1, STAGGER_DIM, meta=True)\n",
    "    else:\n",
    "        print(' ')\n",
    "\n",
    "    \n",
    "    if (isinstance(LEVELS_VAR1, np.ndarray)):\n",
    "        #C1 = AX.contourf(YV1, zh[:, y1:y2, xx]/1000.0, VAR2_XR[:, y1:y2, xx], levels=LEVELS_VAR1,\n",
    "        #                  cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "        C1 = AX.contourf(YV1, z_3D_2D_slice, VAR2_XR[:, y1:y2, xx], levels=LEVELS_VAR1,\n",
    "                          cmap=CMAP_VAR1, extend='both')  # Spectral for qv\n",
    "    else:\n",
    "        C1 = AX.contourf(YV1, z_3D_2D_slice, VAR2_XR[:, y1:y2, xx],\n",
    "                          cmap=CMAP_VAR1, extend='both')\n",
    "\n",
    "        \n",
    "    #levels_th = np.arange(290.0, 690.0, 2.0)\n",
    "    #C4 = plt.contour(XV1, zh[:, yy, x1:x2]/1000.0, DATA.THETA.values[:, yy, x1:x2], colors='k', levels=levels_th, axis=AX, linewidth=0.6)\n",
    "    #plt.clabel(C4, inline=1, fontsize=14, fmt='%3.0f')\n",
    "\n",
    "        \n",
    "    if (isinstance(LEVELS_VAR2, np.ndarray)):\n",
    "        C2 = AX.contour(YV1, z_3D_2D_slice, VAR2_XR[:, y1:y2, xx],\n",
    "                             levels=LEVELS_VAR2, colors=VAR2_COLOR, linewidths=1., linestyles=\"-\")\n",
    "    else:\n",
    "        C2 = AX.contour(YV1,z_3D_2D_slice, VAR2_XR[:, y1:y2, xx],\n",
    "                             colors='k', linewidths=1., linestyles=\"--\")\n",
    "    AX.clabel(C2, inline=1, fontsize=10, fmt='%3.0f')\n",
    "\n",
    "\n",
    "    if VAR3=='RTP-RV_g/kg':\n",
    "        total_condensate_meridional = DATA[\"RTP\"][:, y1:y2, xx]*1000.0 - DATA[\"RV\"][:, y1:y2, xx]*1000.0\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(YV1, z_3D_2D_slice, np.absolute(total_condensate_meridional),\\\n",
    "                                levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    elif VAR3=='RTP-RV_g/m3':\n",
    "        th = DATA['THETA'][:, y1:y2, xx]\n",
    "        pi = DATA['PI'][:, y1:y2, xx]\n",
    "        rv = DATA['RV'][:, y1:y2, xx]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        total_condensate_meridional = (DATA[\"RTP\"][:, y1:y2, xx]*1000.0 - DATA[\"RV\"][:, y1:y2, xx]*1000.0)*dens\n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(YV1, z_3D_2D_slice, np.absolute(total_condensate_meridional),\\\n",
    "                                levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")\n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "    elif VAR3=='precipitating_condensate_g/m3':\n",
    "        th = DATA['THETA'][:, y1:y2, xx]\n",
    "        pi = DATA['PI'][:, y1:y2, xx]\n",
    "        rv = DATA['RV'][:, y1:y2, xx]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        total_condensate_meridional = (DATA[\"RTP\"][:, y1:y2, xx]*1000.0 - DATA[\"RV\"] [:, y1:y2, xx]*1000.0 \\\n",
    "                                                                        - DATA[\"RCP\"][:, y1:y2, xx]*1000.0 \\\n",
    "                                                                        - DATA[\"RPP\"][:, y1:y2, xx]*1000.0)*dens\n",
    "        \n",
    "        \n",
    "        if (isinstance(LEVELS_VAR3, np.ndarray)):\n",
    "            C3 = AX.contour(YV1, z_3D_2D_slice, np.absolute(total_condensate_meridional),\\\n",
    "                                levels=LEVELS_VAR3, colors=VAR3_COLOR, linewidths=2.0, linestyles=\"--\")\n",
    "            AX.clabel(C3, inline=1, fontsize=10, fmt='%3.3f')\n",
    "        else:\n",
    "            print('please provide levels!')\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print('please provide correct value of VAR3')\n",
    " \n",
    "        #del total_condensate_meridional\n",
    "           \n",
    "    if VAR4:\n",
    "        if VAR4=='buoyancy':\n",
    "            print('buoyancy')    \n",
    "#     if PLOT_WINDS:\n",
    "#         winds_thin_x = 4\n",
    "#         winds_thin_z = 4\n",
    "#         YVwind, ZVwind = np.meshgrid(yh[y1:y2:winds_thin_x], z[::winds_thin_z])\n",
    "#         v1 = DATA.variables[\"VP\"][::winds_thin_z,y1:y2:winds_thin_x, xx]*1.94384\n",
    "#         w1 = DATA.variables[\"WP\"][::winds_thin_z,y1:y2:winds_thin_x, xx]*1.94384\n",
    "#         #QV1 = AX.barbs(YVwind, zh[::winds_thin_z,y1:y2:winds_thin_x, xx]/1000.0,\n",
    "#         #               v1, w1, length=7.2, pivot='middle', linewidth=0.60, flip_barb=True)\n",
    "          \n",
    "\n",
    "    C4zs = AX.plot(Y_1D[y1:y2], TERR[y1:y2, xx]/1000., color='sienna', linewidth=3.6)\n",
    "    \n",
    "    if PLOT_SEGMENTATION_OUTPUT:\n",
    "        seg_filename=glob.glob('/nobackupp11/isingh2/tobac_tracking-main/'+DOMAIN+'_segmentation_mask_box_threshold_2_'+pd.to_datetime(FILENAMETIME).strftime('%Y%m%d%H%M%S')+'.nc')\n",
    "        print('segmentation file is: ',seg_filename[0])\n",
    "        print('feature# ',FEATURE_NUM)\n",
    "        segmentation_da = xr.open_dataset(seg_filename[0]).segmentation_mask\n",
    "        C_seg = AX.contour(YV1, z_3D_2D_slice, np.where(segmentation_da[:, y1:y2, xx]==FEATURE_NUM,1.0,0.0), levels=[0.9],colors='green', linewidths=1.0, linestyles=\"-\")\n",
    "        AX.clabel(C_seg, inline=1, fontsize=13, fmt='%f')\n",
    "        \n",
    "    title = 'Vertical cross-section (meridional) of $w$'\n",
    "    prodid = 'tests'+'_'+VAR1+'_th_vcross_zonal_'+'xx' + \\\n",
    "        str(xx)+'_y1_'+str(y1)+'_y2_'+str(y2)+'.png'\n",
    "    units = 'm/s'#var1.attrs['units']  # '$ x 10^{-5} $'+\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    AX.text(0.55, 0.94, EXP_LABEL, fontsize=12,\n",
    "            verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "    \n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "    \n",
    "    AX.scatter(YPOS*DXY/1000.0,TERR[YPOS,XPOS]/1000.+ZPOS/1000.,marker='+',color='k',s=130.5)\n",
    "    \n",
    "    # Draw box that is used for averaging of hydrometeor conc. for filtering:\n",
    "    if CELL_DIM is not None:\n",
    "        low_y_km  = np.max([(YPOS - (CELL_DIM/2.)),0.0])*DXY/1000.0\n",
    "        box_width_km = CELL_DIM*DXY/1000.0\n",
    "        low_z_grid  = int(np.max([ZPOS_GRID - (CELL_DIM / 2.),0]))\n",
    "        high_z_grid = int(np.min([ZPOS_GRID + (CELL_DIM / 2.),NUM_MODEL_LEVS]))\n",
    "        box_height_km = (Z_1D[high_z_grid] - Z_1D[low_z_grid])/1000.0\n",
    "        low_z_km   = (TERR[YPOS,XPOS] + Z_1D[low_z_grid])/1000.0\n",
    "        rect1  =  AX.add_patch(Rectangle((low_y_km,low_z_km), box_width_km, box_height_km , color='green', fc = 'none',lw = 1.8))\n",
    "\n",
    "    PLOT_INCUS_COLUMN_MAXIMA=False\n",
    "    if PLOT_INCUS_COLUMN_MAXIMA:\n",
    "        BOX_SCALING_FACTOR = 1.5\n",
    "        y_small = int(np.max([YPOS - BOX_SCALING_FACTOR*(CELL_DIM/2.),0]))\n",
    "        y_large = int(YPOS + BOX_SCALING_FACTOR*(CELL_DIM / 2.))\n",
    "        x_small = int(np.max([XPOS - BOX_SCALING_FACTOR*(CELL_DIM/2.),0]))\n",
    "        x_large = int(XPOS + BOX_SCALING_FACTOR*(CELL_DIM / 2.))\n",
    "        low_z_grid  = int(np.max([ZPOS_GRID - BOX_SCALING_FACTOR*(CELL_DIM / 2.),0]))\n",
    "        high_z_grid = int(np.min([ZPOS_GRID + BOX_SCALING_FACTOR*(CELL_DIM / 2.),NUM_MODEL_LEVS]))\n",
    "        \n",
    "        wp_array = VAR2_XR[low_z_grid:high_z_grid, y_small:y_large, x_small:x_large].values\n",
    "        \n",
    "        print('shape of cell is ',np.shape(wp_array))\n",
    "        \n",
    "        ind_max = np.unravel_index(np.nanargmax(wp_array),wp_array.shape)\n",
    "        z_coord_wmax = ind_max[0]+low_z_grid\n",
    "        y_coord_wmax = ind_max[1]+y_small\n",
    "        x_coord_wmax = ind_max[2]+x_small\n",
    "    \n",
    "        print('index of w max is ',ind_max)\n",
    "        print('z coordinate is :', z_coord_wmax)\n",
    "        print('y coordinate is :', y_coord_wmax)\n",
    "        print('x coordinate is :', x_coord_wmax)\n",
    "        \n",
    "        zpos_wmax    = (TERR[y_coord_wmax,x_coord_wmax] + Z_1D[z_coord_wmax,np.newaxis])/1000.0\n",
    "        ypos_wmax    = y_coord_wmax*DXY/1000.0\n",
    "        xpos_wmax    = x_coord_wmax*DXY/1000.0\n",
    "        \n",
    "        print('z position is :',zpos_wmax)\n",
    "        print('y position is :',ypos_wmax)\n",
    "        print('x position is :',xpos_wmax)\n",
    "        \n",
    "\n",
    "        th = DATA['THETA'][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]\n",
    "        pi = DATA['PI'][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]\n",
    "        rv = DATA['RV'][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]\n",
    "        pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "        temp = th*(pi/Cp)\n",
    "        del(th,pi)\n",
    "        dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "        del(pres,temp,rv)\n",
    "        \n",
    "        cond_array =  (DATA[\"RTP\"][low_z_grid:high_z_grid, y_small:y_large, x_small:x_large]*1000.0 - DATA[\"RV\"][low_z_grid:high_z_grid, y_small:y_large,x_small:x_large]*1000.0)*dens\n",
    "        print('shape of cell is ',np.shape(cond_array))\n",
    "        ind_max = np.unravel_index(np.nanargmax(cond_array),cond_array.shape)\n",
    "        z_coord_condmax = ind_max[0]+low_z_grid\n",
    "        y_coord_condmax = ind_max[1]+y_small\n",
    "        x_coord_condmax = ind_max[2]+x_small\n",
    "        \n",
    "        print('index of cond max is ',ind_max)\n",
    "        print('z coordinate is :', z_coord_condmax)\n",
    "        print('y coordinate is :', y_coord_condmax)\n",
    "        print('x coordinate is :', x_coord_condmax)\n",
    "        zpos_condmax    =  (TERR[y_coord_wmax,x_coord_condmax] + Z_1D[z_coord_condmax,np.newaxis])/1000.0\n",
    "        ypos_condmax = y_coord_condmax*DXY/1000.0\n",
    "        xpos_condmax = x_coord_condmax*DXY/1000.0\n",
    "        print('z position is :',zpos_condmax)\n",
    "        print('y position is :',ypos_condmax)\n",
    "        print('x position is :',xpos_condmax)\n",
    "        AX.scatter(ypos_wmax,zpos_wmax[0],s=130.5,marker='+',color='fuchsia')#facecolors='none',edgecolors='blue')\n",
    "        AX.scatter(ypos_condmax,zpos_condmax[0],s=130.5,marker='+',color='blue')#facecolors='none',edgecolors='blue')\n",
    "        #AX.axvline(x = y_small*DXY/1000.0, color = 'b')\n",
    "        #AX.axvline(x = y_large*DXY/1000.0, color = 'b')\n",
    "\n",
    "    fig_process_vert(AX, TERR, C1, Y_1D, y1, y2, xx, PLOT_CBAR, 0, 15, title, TITLETIME, FILENAMETIME, prodid, units, \"meridional\", HEIGHT, False, PANEL_PLOT)\n",
    " \n",
    "def plot_plan_view_cell(DOMAIN, DATA, DX, DY, TERR, X_1D, Y_1D, ZM, XPOS, YPOS, ZPOS, DXY, VAR1, LEVELS_VAR1, SINGLE_LEVEL_VAR1, CMAP_VAR1, VAR2, LEVELS_VAR2, VAR2_COLOR, x1,x2, y1, y2, FEATURE_NUM, PLOT_SEGMENTATION_OUTPUT,\n",
    "                    AX, PLOT_CBAR, EXP_LABEL, PANEL_LABEL,  TITLETIME, FILENAMETIME, XPOS_FEATURES=None,YPOS_FEATURES=None,XPOS_LIST=None,YPOS_LIST=None):  # rcParams[\"contour.negative_linestyle\"] = 'dashed'\n",
    "    \n",
    "    XH1, YH1 = np.meshgrid(X_1D[x1:x2]/1000.0,Y_1D[y1:y2]/1000.0)\n",
    "    vert_lev = np.argmin(np.abs(ZM-ZPOS))\n",
    "    var_to_plotted=DATA.variables[VAR1][vert_lev,y1:y2,x1:x2].values\n",
    "    C111   = AX.contourf(XH1 ,YH1,var_to_plotted ,levels=LEVELS_VAR1,cmap= CMAP_VAR1,extend='both')#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "    \n",
    "    if SINGLE_LEVEL_VAR1:\n",
    "        C112   = AX.contour(XH1 ,YH1,var_to_plotted ,levels=SINGLE_LEVEL_VAR1,colors='k')#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "        AX.clabel(C112, inline=1, fontsize=10, fmt='%3.0f')\n",
    "\n",
    "    if XPOS_FEATURES is not None:\n",
    "        tobac_features_scatter1 = AX.scatter(XPOS_FEATURES*DXY/1000.0,YPOS_FEATURES*DXY/1000.0,marker='^',s=100.5,c='green')#facecolors='none', edgecolors='green')\n",
    "        \n",
    "    if XPOS_LIST:\n",
    "        print('plotting cell track...')\n",
    "        AX.plot(np.array(XPOS_LIST)*DXY/1000.0,np.array(YPOS_LIST)*DXY/1000.0,color='k',linewidth=1.5,linestyle='--')\n",
    "        \n",
    "    if VAR2:\n",
    "        if VAR2=='buoyancy':\n",
    "            print('buoyancy')\n",
    "            \n",
    "    if XPOS:\n",
    "        tobac_features_scatter = AX.scatter(XPOS*DXY/1000.0,YPOS*DXY/1000.0,marker='+',s=130.5,c='k')\n",
    "    \n",
    "    if PLOT_SEGMENTATION_OUTPUT:\n",
    "        seg_filename=glob.glob('/nobackupp11/isingh2/tobac_tracking-main/'+DOMAIN+'_segmentation_mask_box_threshold_2_'+pd.to_datetime(FILENAMETIME).strftime('%Y%m%d%H%M%S')+'.nc')\n",
    "        print('segmentation file is: ',seg_filename[0])\n",
    "        print('feature# ',FEATURE_NUM)\n",
    "        segmentation_da = xr.open_dataset(seg_filename[0]).segmentation_mask\n",
    "        C_plan_seg = AX.contour(XH1 ,YH1, np.where(segmentation_da[vert_lev, y1:y2, x1:x2]==FEATURE_NUM,1.0,0.0), levels=[0.9],colors='green', linewidths=1.0, linestyles=\"-\")\n",
    "        AX.clabel(C_plan_seg, inline=1, fontsize=13, fmt='%f')\n",
    "\n",
    "    import matplotlib.transforms as transforms\n",
    "    trans = transforms.blended_transform_factory(\n",
    "        AX.transAxes, AX.transData)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "\n",
    "    if EXP_LABEL:\n",
    "        AX.text(0.55, 0.94, EXP_LABEL, fontsize=12,\n",
    "                verticalalignment='top', bbox=props, transform=AX.transAxes)\n",
    "\n",
    "    AX.set_title('Vertical velocity at model level '+str(vert_lev)+'\\n'+TITLETIME,fontsize=16)\n",
    "    AX.set_xlabel('x-distance (km)',fontsize=16)\n",
    "    AX.set_ylabel('y-distance (km)',fontsize=16)\n",
    "    \n",
    "    if PLOT_CBAR:\n",
    "        print('not plotting colorbar for the plan view')\n",
    "    if PANEL_LABEL:\n",
    "        props1 = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "        AX.text(0.04, 0.94, PANEL_LABEL, fontsize=18,\n",
    "        verticalalignment='top', bbox=props1, transform=AX.transAxes)\n",
    "        \n",
    "    return C111\n",
    "\n",
    "def plot_vert_zonal_meridional_crosssection_plan_view_tobac_comparison(DOMAIN,TOBAC_DF1,TOBAC_DF2,CELL_NO1,CELL_NO2,XH,YH,ZZ_M,ZM,TERR,DXY,CMAP,OUTPUT_DIR,EXPERIMENT_MARKER,TOBAC_FEATURES_DF1=None,TOBAC_FEATURES_DF2=None):\n",
    "        print('cell in file#1: ',CELL_NO1,' and cell in file#2:',CELL_NO2)\n",
    "        tdata_neu1=TOBAC_DF1[TOBAC_DF1['cell']==CELL_NO1]\n",
    "        print('this cell has '+str(len(tdata_neu1))+' time steps')\n",
    "        xpos1=list(tdata_neu1.X.values.astype(int))\n",
    "        ypos1=list(tdata_neu1.Y.values.astype(int))\n",
    "        zpos1=list(tdata_neu1.zmn.values.astype(int))    \n",
    "        zpos_grid1=list(tdata_neu1.vdim.values.astype(int))\n",
    "        features1=list(tdata_neu1.feature.values)\n",
    "        cell_dim1 =list(np.array(tdata_neu1.num.values.astype(int))**(1/3))\n",
    "        times_tracked1=tdata_neu1.timestr.values\n",
    "        times_tracked_pd1 = pd.to_datetime(times_tracked1)\n",
    "        thresholds1=tdata_neu1.threshold_value.values\n",
    "        cell_labels1=['cell#'+str(CELL_NO1)+'\\nfeature#'+str(features1[kk])+'\\nxpos:'+str(xpos1[kk])+' gr pt'+'\\nypos:'+str(ypos1[kk])+' gr pt'+'\\nzpos:'+str(zpos1[kk])+' m' for kk in range(len(xpos1))]\n",
    "        print('cell one times vary from ',min(times_tracked_pd1),' to ',max(times_tracked_pd1))\n",
    "        \n",
    "        tdata_neu2=TOBAC_DF2[TOBAC_DF2['cell']==CELL_NO2]\n",
    "        print('this cell has '+str(len(tdata_neu2))+' time steps')\n",
    "        xpos2=list(tdata_neu2.X.values.astype(int))\n",
    "        ypos2=list(tdata_neu2.Y.values.astype(int))\n",
    "        zpos2=list(tdata_neu2.zmn.values.astype(int))    \n",
    "        zpos_grid2=list(tdata_neu2.vdim.values.astype(int))\n",
    "        features2=list(tdata_neu2.feature.values)\n",
    "        cell_dim2 =list(np.array(tdata_neu2.num.values.astype(int))**(1/3))\n",
    "        times_tracked2=tdata_neu2.timestr.values\n",
    "        times_tracked_pd2 = pd.to_datetime(times_tracked2)\n",
    "        thresholds2=tdata_neu2.threshold_value.values\n",
    "        cell_labels2=['cell#'+str(CELL_NO2)+'\\nfeature#'+str(features2[kk])+'\\nxpos:'+str(xpos2[kk])+' gr pt'+'\\nypos:'+str(ypos2[kk])+' gr pt'+'\\nzpos:'+str(zpos2[kk])+' m' for kk in range(len(xpos2))]\n",
    "        print('cell two times vary from ',min(times_tracked_pd2),' to ',max(times_tracked_pd2))\n",
    "              \n",
    "        start_time = min(min(times_tracked_pd1),min(times_tracked_pd2)) \n",
    "        end_time = max(max(times_tracked_pd1),max(times_tracked_pd2))\n",
    "        all_times = pd.date_range(start=start_time, end=end_time, freq='30S')\n",
    "        print('\\n=======================')\n",
    "        print('plotting times vary from ',min(all_times),' to ',max(all_times))\n",
    "        print('=======================\\n')\n",
    "        # ADDING TWO MINUTES BEFORE AND AFTER\n",
    "        start_time = min(min(times_tracked_pd1),min(times_tracked_pd2)) - pd.Timedelta(minutes=2) \n",
    "        end_time = max(max(times_tracked_pd1),max(times_tracked_pd2))   + pd.Timedelta(minutes=2) \n",
    "        all_times = pd.date_range(start=start_time, end=end_time, freq='30S')\n",
    "        print('\\n=======================')\n",
    "        print('plotting times vary from (after adding 2 minutes before and after) ',min(all_times),' to ',max(all_times))\n",
    "        print('=======================\\n')\n",
    "        \n",
    "        ii = 0 \n",
    "        ii1 = 0\n",
    "        ii2 = 0\n",
    "        for tim_pd in all_times:\n",
    "            print('-------------------------------------------------------')\n",
    "            if all_times[ii] in times_tracked_pd1:\n",
    "                switch_1=True\n",
    "            else:\n",
    "                switch_1=False\n",
    "                \n",
    "            if all_times[ii] in times_tracked_pd2:\n",
    "                switch_2=True\n",
    "            else:\n",
    "                switch_2=False\n",
    "                \n",
    "            print('timestep '+str(ii)+': '+tim_pd.strftime(\"%Y-%m-%d-%H:%M:%S\"))\n",
    "            #tim_pd = pd.to_datetime(tim)\n",
    "            rams_fil=simulation_base_folder+'a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5'\n",
    "            print('RAMS date file: ',rams_fil)\n",
    "            rams_fil_da=xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "            titletime= get_time_from_RAMS_file(rams_fil)[0]\n",
    "            filenametime= get_time_from_RAMS_file(rams_fil)[1]\n",
    "            wpp = rams_fil_da[\"WP\"]\n",
    "            \n",
    "            #COMPARISON PLOTTING STARTS HERE\n",
    "            fig = plt.figure(figsize=(15, 15), frameon=False)  # (16,11)\n",
    "            ax1 = plt.subplot(2, 3, 1)\n",
    "            ax1.set_aspect('equal', adjustable='box') # plot square plan view\n",
    "            ax2 = plt.subplot(2, 3, 2)\n",
    "            ax3 = plt.subplot(2, 3, 3)\n",
    "            ax4 = plt.subplot(2, 3, 4)\n",
    "            ax4.set_aspect('equal', adjustable='box') # plot square plan view\n",
    "            ax5 = plt.subplot(2, 3, 5)\n",
    "            ax6 = plt.subplot(2, 3, 6)\n",
    "            \n",
    "            # plot first row from tracking file#1\n",
    "            if switch_1:\n",
    "                print('\\nii1 = ',ii1,'\\n')\n",
    "                if TOBAC_FEATURES_DF1 is not None:\n",
    "                # plot features that are wiithin 1 km of the vertical level of the cell that is being plotted\n",
    "                    tdata_feat1=TOBAC_FEATURES_DF1[(TOBAC_FEATURES_DF1['time']==tim_pd)   & (abs(TOBAC_FEATURES_DF1['zmn']-zpos1[ii1])<=2000.) & \\\n",
    "                                                 (TOBAC_FEATURES_DF1['X']>=xpos1[ii1]-50) & (TOBAC_FEATURES_DF1['X']<=xpos1[ii1]+50)          & \\\n",
    "                                                 (TOBAC_FEATURES_DF1['Y']>=ypos1[ii1]-50) & (TOBAC_FEATURES_DF1['Y']<=ypos1[ii1]+50)]\n",
    "                    xpos_feat1=np.array((tdata_feat1.X.values.astype(int)))\n",
    "                    ypos_feat1=np.array((tdata_feat1.Y.values.astype(int)))\n",
    "                else:\n",
    "                    xpos_feat1=None\n",
    "                    ypos_feat1=None\n",
    "               \n",
    "            \n",
    "                cbar_conts1 = plot_plan_view_cell      (DOMAIN, rams_fil_da, 100.0, 100.0, TERR, XH, YH, ZM, xpos1[ii1], ypos1[ii1], zpos1[ii1], DXY, 'WP', np.arange(-20.,20.1,.1),[thresholds1[ii1]], CMAP, None, [0.01], 'maroon', min(xpos1)-50, max(xpos1)+50,  min(ypos1)-50,  max(ypos1)+50, features1[ii1], False,\n",
    "                                          ax1, False,cell_labels1[ii1], '(a)'  ,titletime, filenametime,xpos_feat1,ypos_feat1,xpos1,ypos1)\n",
    "                print('plan view done #1')\n",
    "\n",
    "                plot_zonal_vertcross     (DOMAIN, rams_fil_da,  100.0, 100.0, TERR, XH, ZZ_M, ZM, DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                           wpp,np.array([thresholds1[ii1]]), 'k',\n",
    "                                           'RTP-RV_g/m3', np.array([0.05]), 'purple',\n",
    "                                           None,[0.01], 'maroon',\n",
    "                                           ypos1[ii1], min(xpos1)-50, max(xpos1)+50 ,\n",
    "                                           False, False, 15.0, ax2,False,cell_labels1[ii1],'(b)',xpos1[ii1],ypos1[ii1],zpos1[ii1],zpos_grid1[ii1],cell_dim1[ii1], features1[ii1], False, 231, titletime,filenametime)\n",
    "                print('vertical zonal cross-section done #1')\n",
    "\n",
    "                plot_meridional_vertcross(DOMAIN, rams_fil_da,  100.0, 100.0,TERR, YH, ZZ_M, ZM,DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                          wpp,np.array([thresholds1[ii1]]), 'k',\n",
    "                                          'RTP-RV_g/m3', np.array([0.05]), 'purple',\n",
    "                                          None, [0.01], 'maroon',\n",
    "                                          xpos1[ii1], min(ypos1)-50, max(ypos1)+50 ,\n",
    "                                          False, False, 15.0, ax3,False,cell_labels1[ii1],'(c)',xpos1[ii1],ypos1[ii1],zpos1[ii1],zpos_grid1[ii1],cell_dim1[ii1], features1[ii1], False, 231,titletime,filenametime)\n",
    "                print('vertical meridional cross-section done #1')\n",
    "                ii1=ii1+1\n",
    "                \n",
    "            print('\\n$$###########$$\\n')\n",
    "            #-----#-----#-----#-----#-----#-----#-----#-----#-----#-----#-----\n",
    "            # plot second row from tracking file#2\n",
    "            if switch_2:\n",
    "                print('\\nii2 = ',ii2,'\\n')\n",
    "                if TOBAC_FEATURES_DF2 is not None:\n",
    "                    # plot features that are wiithin 1 km of the vertical level of the cell that is being plotted\n",
    "                    tdata_feat2=TOBAC_FEATURES_DF2[(TOBAC_FEATURES_DF2['time']==tim_pd)   & (abs(TOBAC_FEATURES_DF2['zmn']-zpos2[ii2])<=2000.) & \\\n",
    "                                                 (TOBAC_FEATURES_DF2['X']>=xpos2[ii2]-50) & (TOBAC_FEATURES_DF2['X']<=xpos2[ii2]+50)          & \\\n",
    "                                                 (TOBAC_FEATURES_DF2['Y']>=ypos2[ii2]-50) & (TOBAC_FEATURES_DF2['Y']<=ypos2[ii2]+50)]\n",
    "\n",
    "                    xpos_feat2=np.array((tdata_feat2.X.values.astype(int)))\n",
    "                    ypos_feat2=np.array((tdata_feat2.Y.values.astype(int)))\n",
    "                else:\n",
    "                    xpos_feat2=None\n",
    "                    ypos_feat2=None\n",
    "            \n",
    "                cbar_conts1 = plot_plan_view_cell      (DOMAIN, rams_fil_da, 100.0, 100.0, TERR, XH, YH, ZM, xpos2[ii2], ypos2[ii2], zpos2[ii2], DXY, 'WP', np.arange(-20.,20.1,.1),[thresholds2[0]], CMAP, None, [0.01], 'maroon', min(xpos2)-50, max(xpos2)+50,  min(ypos2)-50,  max(ypos2)+50, features2[ii2], False,\n",
    "                                          ax4, False,cell_labels2[ii2], '(d)'  ,titletime, filenametime,xpos_feat2,ypos_feat2,xpos2,ypos2)\n",
    "                print('plan view done #2')\n",
    "\n",
    "                plot_zonal_vertcross     (DOMAIN, rams_fil_da,  100.0, 100.0, TERR, XH, ZZ_M, ZM, DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                           wpp,np.array([thresholds2[ii2]]), 'k',\n",
    "                                           'RTP-RV_g/m3', np.array([0.05]), 'purple',\n",
    "                                           None,[0.01], 'maroon',\n",
    "                                           ypos2[ii2], min(xpos2)-50, max(xpos2)+50 ,\n",
    "                                           False, False, 15.0, ax5,False,cell_labels2[ii2],'(e)',xpos2[ii2],ypos2[ii2],zpos2[ii2],zpos_grid2[ii2],cell_dim2[ii2], features2[ii2], False, 231, titletime,filenametime)\n",
    "                print('vertical zonal cross-section done #2')\n",
    "\n",
    "                plot_meridional_vertcross(DOMAIN, rams_fil_da,  100.0, 100.0,TERR, YH, ZZ_M, ZM,DXY, 'WP', False, 2, np.arange(-20.,20.1,.1), CMAP,\n",
    "                                          wpp,np.array([thresholds2[ii2]]), 'k',\n",
    "                                          'RTP-RV_g/m3', np.array([0.05]), 'purple',\n",
    "                                          None, [0.01], 'maroon',\n",
    "                                          xpos2[ii2], min(ypos2)-50, max(ypos2)+50 ,\n",
    "                                          False, False, 15.0, ax6,False,cell_labels2[ii2],'(f)',xpos2[ii2],ypos2[ii2],zpos2[ii2],zpos_grid2[ii2],cell_dim2[ii2], features2[ii2], False, 231,titletime,filenametime)\n",
    "                print('vertical meridional cross-section done #2')\n",
    "                ii2=ii2+1\n",
    "\n",
    "            if (switch_1==True) &  (switch_2==False): \n",
    "                req_index = max([(ii2-1),0])\n",
    "                if TOBAC_FEATURES_DF2 is not None:\n",
    "                    # plot features that are wiithin 1 km of the vertical level of the cell that is being plotted\n",
    "                    tdata_feat2=TOBAC_FEATURES_DF2[(TOBAC_FEATURES_DF2['time']==tim_pd)     & (abs(TOBAC_FEATURES_DF2['zmn']-zpos2[req_index])<=2000.) & \\\n",
    "                                                   (TOBAC_FEATURES_DF2['X']>=xpos2[req_index]-50) & (TOBAC_FEATURES_DF2['X']<=xpos2[req_index]+50)          & \\\n",
    "                                                   (TOBAC_FEATURES_DF2['Y']>=ypos2[req_index]-50) & (TOBAC_FEATURES_DF2['Y']<=ypos2[req_index]+50)]\n",
    "\n",
    "                    xpos_feat2=np.array((tdata_feat2.X.values.astype(int)))\n",
    "                    ypos_feat2=np.array((tdata_feat2.Y.values.astype(int)))\n",
    "                else:\n",
    "                    xpos_feat2=None\n",
    "                    ypos_feat2=None\n",
    "                print('found cell in file 1 but not in file 2...\\n plotting features from file 2')\n",
    "                \n",
    "                cbar_conts1 = plot_plan_view_cell      (DOMAIN, rams_fil_da, 100.0, 100.0, TERR, XH, YH, ZM, None, None, zpos2[req_index], DXY, 'WP', np.arange(-20.,20.1,.1),None, CMAP, None, [0.01], 'maroon', min(xpos2)-50, max(xpos2)+50,  min(ypos2)-50,  max(ypos2)+50, None, False,\n",
    "                                          ax4, False,None, '(d)'  ,titletime, filenametime,xpos_feat2,ypos_feat2,xpos2,ypos2)\n",
    "                print('plan view done #2')\n",
    "                \n",
    "            if (switch_1==False) &  (switch_2==True):\n",
    "                req_index = max([ii1-1,0])\n",
    "                if TOBAC_FEATURES_DF1 is not None:\n",
    "                # plot features that are wiithin 1 km of the vertical level of the cell that is being plotted\n",
    "                    tdata_feat1=TOBAC_FEATURES_DF1[(TOBAC_FEATURES_DF1['time']==tim_pd)     & (abs(TOBAC_FEATURES_DF1['zmn']-zpos1[req_index])<=2000.) & \\\n",
    "                                                   (TOBAC_FEATURES_DF1['X']>=xpos1[req_index]-50) &     (TOBAC_FEATURES_DF1['X']<=xpos1[req_index]+50)          & \\\n",
    "                                                   (TOBAC_FEATURES_DF1['Y']>=ypos1[req_index]-50) &     (TOBAC_FEATURES_DF1['Y']<=ypos1[req_index]+50)]\n",
    "                    xpos_feat1=np.array((tdata_feat1.X.values.astype(int)))\n",
    "                    ypos_feat1=np.array((tdata_feat1.Y.values.astype(int)))\n",
    "                else:\n",
    "                    xpos_feat1=None\n",
    "                    ypos_feat1=None\n",
    "                print('found cell in file 2 but not in file 1\\n plotting features from file 1')\n",
    "                \n",
    "                cbar_conts1 = plot_plan_view_cell      (DOMAIN, rams_fil_da, 100.0, 100.0, TERR, XH, YH, ZM, None, None, zpos1[req_index], DXY, 'WP', np.arange(-20.,20.1,.1),None, CMAP, None, [0.01], 'maroon', min(xpos1)-50, max(xpos1)+50,  min(ypos1)-50,  max(ypos1)+50, None, False,\n",
    "                                          ax1, False,None, '(a)'  ,titletime, filenametime,xpos_feat1,ypos_feat1,xpos1,ypos1)\n",
    "                print('plan view done #1')\n",
    "                \n",
    "                \n",
    "            if (switch_1==False) &  (switch_2==False):\n",
    "                print('before and after part!!!')\n",
    "                req_index1 = max([ii1-1,0])\n",
    "                req_index2 = max([ii2-1,0])\n",
    "                \n",
    "                if TOBAC_FEATURES_DF1 is not None:\n",
    "                # plot features that are wiithin 1 km of the vertical level of the cell that is being plotted\n",
    "                    tdata_feat1=TOBAC_FEATURES_DF1[(TOBAC_FEATURES_DF1['time']==tim_pd)     & (abs(TOBAC_FEATURES_DF1['zmn']-zpos1[req_index1])<=2000.) & \\\n",
    "                                                   (TOBAC_FEATURES_DF1['X']>=xpos1[req_index1]-50) &     (TOBAC_FEATURES_DF1['X']<=xpos1[req_index1]+50)          & \\\n",
    "                                                   (TOBAC_FEATURES_DF1['Y']>=ypos1[req_index1]-50) &     (TOBAC_FEATURES_DF1['Y']<=ypos1[req_index1]+50)]\n",
    "                    xpos_feat1=np.array((tdata_feat1.X.values.astype(int)))\n",
    "                    ypos_feat1=np.array((tdata_feat1.Y.values.astype(int)))\n",
    "                else:\n",
    "                    xpos_feat1=None\n",
    "                    ypos_feat1=None\n",
    "                \n",
    "                if TOBAC_FEATURES_DF2 is not None:\n",
    "                    # plot features that are wiithin 1 km of the vertical level of the cell that is being plotted\n",
    "                    tdata_feat2=TOBAC_FEATURES_DF2[(TOBAC_FEATURES_DF2['time']==tim_pd)     & (abs(TOBAC_FEATURES_DF2['zmn']-zpos2[req_index2])<=2000.) & \\\n",
    "                                                   (TOBAC_FEATURES_DF2['X']>=xpos2[req_index2]-50) & (TOBAC_FEATURES_DF2['X']<=xpos2[req_index2]+50)          & \\\n",
    "                                                   (TOBAC_FEATURES_DF2['Y']>=ypos2[req_index2]-50) & (TOBAC_FEATURES_DF2['Y']<=ypos2[req_index2]+50)]\n",
    "                    xpos_feat2=np.array((tdata_feat2.X.values.astype(int)))\n",
    "                    ypos_feat2=np.array((tdata_feat2.Y.values.astype(int)))\n",
    "                else:\n",
    "                    xpos_feat2=None\n",
    "                    ypos_feat2=None\n",
    "\n",
    "                cbar_conts1 = plot_plan_view_cell      (DOMAIN, rams_fil_da, 100.0, 100.0, TERR, XH, YH, ZM, None, None, zpos1[req_index1], DXY, 'WP', np.arange(-20.,20.1,.1),None, CMAP, None, [0.01], 'maroon', min(xpos1)-50, max(xpos1)+50,  min(ypos1)-50,  max(ypos1)+50, None, False,\n",
    "                                          ax1, False,'No cell (2 extra mins)', '(a)'  ,titletime, filenametime,xpos_feat1,ypos_feat1,xpos1,ypos1)\n",
    "                cbar_conts2 = plot_plan_view_cell      (DOMAIN, rams_fil_da, 100.0, 100.0, TERR, XH, YH, ZM, None, None, zpos2[req_index2], DXY, 'WP', np.arange(-20.,20.1,.1),None, CMAP, None, [0.01], 'maroon', min(xpos2)-50, max(xpos2)+50,  min(ypos2)-50,  max(ypos2)+50, None, False,\n",
    "                                          ax4, False,'No cell (2 extra mins)', '(d)'  ,titletime, filenametime,xpos_feat2,ypos_feat2,xpos2,ypos2)\n",
    "     \n",
    "            cb_ax = fig.add_axes([0.2, 0.0001, 0.6, 0.02])  # two panels #[left, bottom, width, height]\n",
    "            cbar  = fig.colorbar(cbar_conts1, cax=cb_ax, orientation = 'horizontal')\n",
    "            plt.tight_layout()\n",
    "            png_file=OUTPUT_DIR+'three_panel_'+DOMAIN+'_'+EXPERIMENT_MARKER+'_cell1_'+str(CELL_NO1)+'_cell2_'+str(CELL_NO2)+'_timestep'+tim_pd.strftime(\"%Y%m%d%H%M%S\")+'.png'\n",
    "            print('\\n')\n",
    "            print(png_file)\n",
    "            plt.savefig(png_file,dpi=150)\n",
    "            plt.close()\n",
    "            ii = ii + 1\n",
    "        print('============================================\\n\\n\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "#plot tracking differences\n",
    "# MAKE CHANGES IN THE LINES MARKED WITH <CHANGE HERE>\n",
    "\n",
    "domain = 'AUS1.1-R' # simulation name <CHANGE HERE>\n",
    "print('working on simulation: ',domain)\n",
    "simulation_base_folder= '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/'\n",
    "tobac_tracking_dirpath1 = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'\n",
    "tobac_tracking_dirpath2 = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'\n",
    "tobac_features_dirpath1 = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'\n",
    "tobac_features_dirpath2 = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'\n",
    "les_path = simulation_base_folder\n",
    "\n",
    "tobac_tracking_filename1   = 'comb_track_01_01_50_sr5035_setpos.p' # name of the first tracking file <CHANGE HERE>\n",
    "tobac_tracking_filename2   = 'comb_track_01_02_50_02_sr5035_setpos.p' # name of the second tracking file <CHANGE HERE>\n",
    "tobac_features_filename1   = 'comb_df_01_01_50.p'\n",
    "tobac_features_filename2   = 'comb_df_01_02_50_02.p'\n",
    "\n",
    "tobac_tracking_filepath1  = tobac_tracking_dirpath1+tobac_tracking_filename1\n",
    "tobac_tracking_filepath2  = tobac_tracking_dirpath2+tobac_tracking_filename2\n",
    "tobac_features_filepath1  = tobac_features_dirpath1+tobac_features_filename1\n",
    "tobac_features_filepath2  = tobac_features_dirpath2+tobac_features_filename2\n",
    "\n",
    "# Grab all the rams files\n",
    "h5filepath = les_path+'a-L*g3.h5'\n",
    "h5files1 = sorted(glob.glob(h5filepath))\n",
    "hefilepath = les_path+'a-L*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "#print(h5files1)\n",
    "start_time_simulation=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "end_time_simulation=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "print('starting time in simulations: ',start_time_simulation)\n",
    "print('ending time in simulations: ',end_time_simulation)\n",
    "\n",
    "#### read in RAMS data file to get parameters for plotting ####\n",
    "rams_terr=xr.open_dataset(h5files1[0],engine='h5netcdf', phony_dims='sort').TOPT.values\n",
    "\n",
    "zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],h5files1[0])\n",
    "\n",
    "xh=np.arange(dxy/2,nx*dxy,dxy)\n",
    "yh=np.arange(dxy/2,ny*dxy,dxy)\n",
    "\n",
    "##### read in tobac data #####\n",
    "print('reading tracking file1: ',tobac_tracking_filepath1)\n",
    "tdata1          = pd.read_pickle(tobac_tracking_filepath1)\n",
    "\n",
    "print('reading tracking file2: ',tobac_tracking_filepath2)\n",
    "tdata2          = pd.read_pickle(tobac_tracking_filepath2)\n",
    "\n",
    "print('reading features file1',tobac_features_filepath1)\n",
    "tdata_features1 = pd.read_pickle(tobac_features_filepath1)\n",
    "\n",
    "print('reading features file2',tobac_features_filepath2)\n",
    "tdata_features2 = pd.read_pickle(tobac_features_filepath2)\n",
    "\n",
    "print('number of unique cells identified in tracking file 1: ',len(tdata1.cell.unique()))\n",
    "print('number of unique cells identified in tracking file 2: ',len(tdata2.cell.unique()),'\\n')\n",
    "\n",
    "find_matched_cells=False\n",
    "if find_matched_cells:\n",
    "    print('finding matched cells...\\n')  \n",
    "    def filt_high_thres(g):\n",
    "        return ((g.threshold_value.max() >= 5.0) & (g.zmn.max() >= 2000.0) & (g.zmn.min() <= 8000.0) & (abs((pd.to_datetime((g.timestr.values)).min() - pd.to_datetime(start_time_simulation)).total_seconds()) >= 120) & \\\n",
    "                                                                                                       (abs((pd.to_datetime((g.timestr.values)).max() - pd.to_datetime(end_time_simulation  )).total_seconds()) >= 120))            \n",
    "    tdata_high_thr1=tdata1.groupby('cell').filter(filt_high_thres)\n",
    "    print('number of unique cells identified in filtered tracking file 1: ',len(tdata_high_thr1.cell.unique()))\n",
    "\n",
    "    tdata_high_thr2=tdata2.groupby('cell').filter(filt_high_thres)\n",
    "    print('number of unique cells identified in filtered tracking file 2: ',len(tdata_high_thr2.cell.unique()),'\\n')\n",
    "\n",
    "    first_tracking_file_cells = tdata_high_thr1.cell.unique()\n",
    "    times_tracked=tdata_high_thr1.timestr.values\n",
    "    times_tracked_pd = sorted(pd.to_datetime(times_tracked).unique())\n",
    "    \n",
    "    print('times vary from : ',min(times_tracked_pd))\n",
    "    print('times vary from : ',max(times_tracked_pd),'\\n\\n')\n",
    "    \n",
    "    matched_cell_dictionary = {}\n",
    "    ii = 1\n",
    "    print('finding cells in tracking file#2 matching with the following cells in file#1: \\n')\n",
    "    for cl in first_tracking_file_cells:\n",
    "        potential_cells = []\n",
    "        matched_cell_dictionary.update({cl:[]})\n",
    "        print('working on cell#: ',cl,' - ',ii,'/',len(first_tracking_file_cells))\n",
    "        track_data_cell_subset = tdata_high_thr1[tdata_high_thr1.cell == cl]\n",
    "        all_times = track_data_cell_subset.timestr.values\n",
    "        all_xpos = track_data_cell_subset.X.values\n",
    "        all_ypos = track_data_cell_subset.Y.values\n",
    "        all_zpos = track_data_cell_subset.zmn.values\n",
    "    \n",
    "        for timestep,xpos,ypos,zpos in list(zip(all_times,all_xpos,all_ypos,all_zpos)):\n",
    "            second_df_time_subset = tdata_high_thr2[(tdata_high_thr2.timestr.values == timestep)     & \\\n",
    "                                                    (tdata_high_thr2.X.values   > (xpos-0.25))       & \\\n",
    "                                                    (tdata_high_thr2.X.values   < (xpos+0.25))       & \\\n",
    "                                                    (tdata_high_thr2.Y.values   > (ypos-0.25))       & \\\n",
    "                                                    (tdata_high_thr2.Y.values   < (ypos+0.25))       & \\\n",
    "                                                    (tdata_high_thr2.zmn.values > (zpos-100.0))      & \\\n",
    "                                                    (tdata_high_thr2.zmn.values < (zpos+100.0))]\n",
    "            # <CHANGE ABOVE> for two cells to be the same entity, they need to be within 0.25 grid point of each \n",
    "            # other for now. \n",
    "            if len(second_df_time_subset)>0:\n",
    "                potential_cells.extend(second_df_time_subset.cell.unique())\n",
    "           \n",
    "        potential_cells = list(set(potential_cells))\n",
    "        if len(potential_cells) > 0:\n",
    "            for matched_cell in potential_cells:\n",
    "                matched_cell_dictionary[cl].append(matched_cell) \n",
    "\n",
    "        ii = ii + 1\n",
    "        \n",
    "    csv_filename = domain+'_matching_cells_file1_'+tobac_tracking_filename1+'_file2_'+tobac_tracking_filename2+'_v2.csv'\n",
    "    print('saving the matched cells to a csv file: ',csv_filename)\n",
    "    pd.DataFrame.from_dict(matched_cell_dictionary,orient='index').to_csv(csv_filename)\n",
    "\n",
    "read_matched_from_csv = True\n",
    "if read_matched_from_csv:\n",
    "    def filt_high_thres(g):\n",
    "        return ((g.threshold_value.max() >= 5.0) & (g.zmn.max() >= 2000.0) & (g.zmn.min() <= 8000.0) & (abs((pd.to_datetime((g.timestr.values)).min() - pd.to_datetime(start_time_simulation)).total_seconds()) >= 120) & \\\n",
    "                                                                                                       (abs((pd.to_datetime((g.timestr.values)).max() - pd.to_datetime(end_time_simulation  )).total_seconds()) >= 120))            \n",
    "    # <CHANGE HERE FOR CHANGING THE CRITERIA>\n",
    "    ## HERE ONLY CELLS THAT GET STRONGER THAN 5 m/s ARE SELECTED ##\n",
    "    tdata_high_thr1=tdata1.groupby('cell').filter(filt_high_thres)\n",
    "    print('number of unique cells identified in filtered tracking file 1: ',len(tdata_high_thr1.cell.unique()))\n",
    "\n",
    "    tdata_high_thr2=tdata2.groupby('cell').filter(filt_high_thres)\n",
    "    print('number of unique cells identified in filtered tracking file 2: ',len(tdata_high_thr2.cell.unique()),'\\n')\n",
    "\n",
    "    first_tracking_file_cells = tdata_high_thr1.cell.unique()\n",
    "    times_tracked=tdata_high_thr1.timestr.values\n",
    "    times_tracked_pd = sorted(pd.to_datetime(times_tracked).unique())\n",
    "    \n",
    "    print('times vary from : ',min(times_tracked_pd))\n",
    "    print('times vary from : ',max(times_tracked_pd),'\\n\\n')\n",
    "    \n",
    "    csv_filename = domain+'_matching_cells_file1_'+tobac_tracking_filename1+'_file2_'+tobac_tracking_filename2+'_v2.csv'\n",
    "    print('reading matching cells from csv: ',csv_filename)\n",
    "    df = pd.read_csv(csv_filename,index_col='Unnamed: 0')\n",
    "    matched_cell_dictionary = df.T.to_dict('list')\n",
    "    for cl1,cl2_list in matched_cell_dictionary.items():\n",
    "        cl2_list_no_nans = [int(item) for item in cl2_list if not(pd.isnull(item)) == True]\n",
    "        matched_cell_dictionary.update({cl1:cl2_list_no_nans})\n",
    "    \n",
    "print('\\n\\n')\n",
    "#multiprocessing below\n",
    "cpu_count1 = cpu_count()\n",
    "argument = []\n",
    "for cl1,cl2_list in matched_cell_dictionary.items():\n",
    "    if len(cl2_list) > 0:\n",
    "        for cl2 in cl2_list:\n",
    "            # IN THE SECOND LINE OF THE ARGUMENT, CHANGE THIS PATH TO WHERE YOU WANT THE OUPUT PNGS <CHANGE HERE>\n",
    "            argument = argument + [(domain,tdata_high_thr1,tdata_high_thr2,cl1,cl2,\\\n",
    "                                    xh,yh,None,zm,rams_terr,dxy,plt.get_cmap('bwr'),'/nobackupp11/isingh2/tobac_plots/',\\\n",
    "                                    'tracking_comparison_different_features_01_01_50_vs_01_02_50_02_before_after_2mins',tdata_features1,tdata_features2)]  # CHANGE THIS DESCRIPTIVE STRING - THIS WILL BE IN THE FILENAME <CHANGE HERE>\n",
    "\n",
    "print('total argument length: ',len(argument),' cells')\n",
    "\n",
    "#run with single processor on a random cell\n",
    "#plot_vert_zonal_meridional_crosssection_plan_view_tobac_comparison(*random.choice(argument))\n",
    "\n",
    "arguments_100 = random.sample(argument, 100)\n",
    "\n",
    "def main(FUNCTION, ARGUMENT):\n",
    "    print('using parallel processing to create plots for ',len(arguments_100),' cells')\n",
    "    pool = Pool(cpu_count1-1)\n",
    "    start_time_function = time.perf_counter()\n",
    "    results = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time_function = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time_function-start_time_function} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(plot_vert_zonal_meridional_crosssection_plan_view_tobac_comparison, arguments_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab075a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vert_zonal_meridional_crosssection_plan_view_tobac_comparison(*random.choice(argument))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d588b2ef",
   "metadata": {},
   "source": [
    "# Plans views of RAMS data and tobac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a17c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to model data and where to save data\n",
    "\n",
    "#domain='ARG1.1-R'\n",
    "#domain='PHI1.1-R' \n",
    "#domain='PHI2.1-R'\n",
    "domain='DRC1.1-R'\n",
    "\n",
    "path = '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/' # Pleiades\n",
    "#path = '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/' # personal macbook\n",
    "savepath = './'\n",
    "tobac_data='/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/' # Pleiades\n",
    "#tobac_data='/avalanche/pmarin/INCUS/C2X/tobac_tests/'         # personal macbook\n",
    "\n",
    "tobac_filename  = 'comb_track_filt_01_02_05_10_20.p'\n",
    "tobac_filepath  = tobac_data+tobac_filename\n",
    "\n",
    "\n",
    "# Grab all the rams files \n",
    "h5filepath = path+'a-L*g3.h5'\n",
    "h5files1 = sorted(glob.glob(h5filepath))\n",
    "hefilepath = path+'a-L*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "#print(h5files1)\n",
    "start_time=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "end_time=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "print('starting time in simulations: ',start_time)\n",
    "print('ending time in simulations: ',end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### read in RAMS data file to get parameters for plotting ####\n",
    "ds=xr.open_dataset(h5files1[0],engine='h5netcdf', phony_dims='sort')\n",
    "rams_lats=ds.GLAT.values\n",
    "rams_lons=ds.GLON.values\n",
    "rams_terr=ds.TOPT.values\n",
    "zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],h5files1[0])\n",
    "lev_8km = np.argmin(np.abs(zm-8000.))\n",
    "lev_5km = np.argmin(np.abs(zm-5000.))\n",
    "lev_2km = np.argmin(np.abs(zm-2000.))\n",
    "print('level corresponsinding to 2 km is :',lev_2km)\n",
    "print('level corresponsinding to 5 km is :',lev_5km)\n",
    "print('level corresponsinding to 8 km is :',lev_8km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only if plotting feature from two different tracking tobac files\n",
    "tobac_data1='/avalanche/pmarin/INCUS/TOBAC_SENS_TESTS/'+domain+'/'\n",
    "tobac_filename1  = 'comb_track_01_02_05_10_20.p'\n",
    "tobac_filepath1=tobac_data1+tobac_filename1\n",
    "tdata_temp1 = pd.read_pickle(tobac_filepath1)\n",
    "#tdata_temp1 = tdata1[(tdata1.time >= pd.to_datetime(start_time)) & (tdata1.time <= pd.to_datetime(end_time))]# & (tdata1.threshold_value.isin(thres_list))]# & (tdata1.zmn >=1000.) & (tdata1.zmn <=15000.)]\n",
    "print('number of unique cells identified: ',len(tdata_temp1.cell.unique()))\n",
    "#tdata_temp = tdata[(tdata.time >= pd.to_datetime('2019-09-10 12:00:00')) & (tdata.time <= pd.to_datetime('2019-09-10 12:09:30'))]\n",
    "tdata_temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "thres_list=[1.0,2.0,5.0,10.0,20.0]#,30.0,40.0,50.0]\n",
    "\n",
    "##### read in tobac data #####\n",
    "print('reading ',tobac_filepath)\n",
    "#tdata1 = pd.read_pickle('tobac_filepath')\n",
    "tdata1 = pd.read_pickle('/nobackup/pmarines/DATA_FM/ARG1.1-R/tobac_data/OLD/combined_df_1_2_5_10_20_nmin64.p')\n",
    "print(tdata1.columns)\n",
    "#tdata_temp = tdata[(tdata.time >= pd.to_datetime(start_time)) & (tdata.time <= pd.to_datetime(end_time))]# & (tdata.threshold_value.isin(thres_list))]# & (tdata.zmn >=1000.) & (tdata.zmn <=15000.)] # regular\n",
    "#tdata_temp1 = tdata[(tdata.time == pd.to_datetime(\"2018-12-13 20:50:00\"))] # specific time\n",
    "#tdata_temp = tdata_temp1[(tdata_temp1.lon >= -62.95) & (tdata_temp1.lon <= -62.7) & (tdata_temp1.lat >= -32.85) & (tdata_temp1.lat <= -32.625)]\n",
    "#print('number of unique cells identified: ',len(tdata_temp.cell.unique()))\n",
    "#tdata_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#levs=[lev_2km,lev_5km,lev_8km]\n",
    "levs=[lev_5km]\n",
    "iii=0\n",
    "level_str=['5km']#,'5km','8km']\n",
    "\n",
    "if domain=='ARG1.1-R':\n",
    "    levels=np.arange(-40,40.5,0.5)\n",
    "elif domain=='PHI1.1-R':\n",
    "    levels=np.arange(-15,15.5,0.5)\n",
    "elif domain=='DRC1.1-R':\n",
    "    levels=np.arange(-15,15.5,0.5)\n",
    "elif domain=='PHI2.1-R':\n",
    "    levels=np.arange(-15,15.5,0.5)\n",
    "else:\n",
    "    print('enter correct domain name')\n",
    "\n",
    "    \n",
    "for lev in levs:\n",
    "    print('#########')\n",
    "    level_in_title=level_str[iii]\n",
    "    print('lev = ',lev)\n",
    "    print('level str = ',level_in_title)\n",
    "    \n",
    "    for fil in h5files1:\n",
    "        print('RAMS file is ',fil)\n",
    "        ds1=xr.open_dataset(fil,engine='h5netcdf', phony_dims='sort')\n",
    "        rams_time, rams_time_savestr = get_time_from_RAMS_file(fil)\n",
    "        print('Time in this file ',rams_time)\n",
    "\n",
    "        print('tobac file 1 is ',tobac_filepath)\n",
    "        tobac_data_subset=tdata_temp[tdata_temp.time==pd.to_datetime(rams_time)]\n",
    "\n",
    "        # lat-lon box\n",
    "        #tobac_data_subset = tobac_data_subset[(tobac_data_subset.lon >= -62.95) & (tobac_data_subset.lon <= -62.7) & (tobac_data_subset.lat >= -32.85) & (tobac_data_subset.lat <= -32.625)]    \n",
    "\n",
    "        tobac_lats=tobac_data_subset.lat\n",
    "        tobac_lons=tobac_data_subset.lon\n",
    "        #tobac_hgts=tobac_data_subset.zmn\n",
    "        #tobac_vols=tobac_data_subset.num\n",
    "\n",
    "#         print('tobac file 2 is ',tobac_filepath1)\n",
    "#         tobac_data_subset1=tdata_temp1[tdata_temp1.time==pd.to_datetime(rams_time)]\n",
    "#         tobac_lats1=tobac_data_subset1.lat\n",
    "#         tobac_lons1=tobac_data_subset1.lon\n",
    "#         #tobac_hgts1=tobac_data_subset1.zmn\n",
    "\n",
    "        fig = plt.figure(figsize=(14,14))\n",
    "        AX = fig.add_subplot(1, 1, 1, projection=crs.PlateCarree())\n",
    "        C111 = AX.contourf(rams_lons ,rams_lats, ds1.WP[lev,:,:].values,transform=crs.PlateCarree(),levels=levels,cmap=plt.get_cmap('bwr'),extend='both')#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "        C_terr = AX.contour(rams_lons, rams_lats, rams_terr, np.array([500.]),linewidths=1.4,colors=\"saddlebrown\",transform=crs.PlateCarree())\n",
    "\n",
    "\n",
    "        # plot tobac data\n",
    "        #tobac_features_scatter = AX.scatter(tobac_lons,tobac_lats,c=tobac_vols,vmin=64.,vmax=2000000.,cmap=plt.get_cmap('viridis'),label=tobac_filename.split(\".\")[0],marker='o',s=7.5)\n",
    "        #tobac_features_scatter = AX.scatter(tobac_lons,tobac_lats,label=tobac_filename.split(\".\")[0],marker='o',s=16,c='aqua',transform=crs.PlateCarree())\n",
    "        #tobac_features_scatter1 = AX.scatter(tobac_lons1,tobac_lats1,label=tobac_filename1.split(\".\")[0],marker='^',s=4.5,c='green',transform=crs.PlateCarree())\n",
    "\n",
    "        # for zoomed in \n",
    "        tobac_features_scatter = AX.scatter(tobac_lons,tobac_lats,label=tobac_filename.split(\".\")[0],marker='o',s=25.5,c='k',transform=crs.PlateCarree())\n",
    "        #tobac_features_scatter1 = AX.scatter(tobac_lons1,tobac_lats1,label=tobac_filename1.split(\".\")[0],marker='^',s=60.5,c='green',transform=crs.PlateCarree())\n",
    "\n",
    "        plt.colorbar(C111,shrink=0.7, pad=0.02,fraction=0.11)\n",
    "        gl = AX.gridlines()#color=\"gray\",alpha=0.5, linestyle='--',draw_labels=True,linewidth=2)\n",
    "        AX.coastlines(resolution='110m')\n",
    "        AX.set_title('Vertical velocity at '+level_in_title+' (m/s)\\n'+rams_time)\n",
    "        #AX.set_title('Vertical velocity at 5 km (m/s)\\n'+rams_time)\n",
    "        #AX.set_title('Vertical velocity at 8 km (m/s)\\n'+rams_time)\n",
    "        gl.xlines = True\n",
    "        gl.ylines = True\n",
    "        LATLON_LABELS=True\n",
    "        if LATLON_LABELS:\n",
    "            print('LATLON labels are on')\n",
    "            gl.xlabels_top = True\n",
    "            gl.ylabels_right = False\n",
    "            gl.ylabels_left = True\n",
    "            gl.ylabels_bottom = True\n",
    "        else:\n",
    "            gl.xlabels_top = False\n",
    "            gl.ylabels_right = False\n",
    "            gl.ylabels_left = False\n",
    "            gl.ylabels_bottom = True\n",
    "        #gl.xlines = False\n",
    "        #gl.xlocator = mticker.FixedLocator([-67, -66, 0, 45, 180])\n",
    "        #gl.xformatter = LONGITUDE_FORMATTER\n",
    "        #gl.yformatter = LATITUDE_FORMATTER\n",
    "        gl.xlabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "        gl.ylabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "\n",
    "        #plt.colorbar(tobac_features_scatter,pad=0.03,extend='both',orientation='horizontal',shrink=0.8)\n",
    "        #AX.set_extent([-63.,-62.5,-33,-32.5])\n",
    "        #AX.set_extent([119.3,119.8,17.2,17.6])\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        #outfilepng='RAMS_tobac_plan_view_'+domain+'_w_'+level_in_title+'_comparison_file1-'+tobac_filename.split(\".\")[0]+'_file2-'+tobac_filename1.split(\".\")[0]+'_'+rams_time_savestr+'.png'\n",
    "        #sigma1-2_thres-2-5-10-20_zmn-3000-15000_\n",
    "        outfilepng='RAMS_tobac_plan_view_'+domain+'_latlonbox_'+tobac_filename.split(\".\")[0]+'_'+rams_time_savestr+'.png'\n",
    "        print('output png is ',outfilepng)\n",
    "        plt.savefig(outfilepng,dpi=150)\n",
    "        #plt.close()\n",
    "        print('--------------\\n')\n",
    "    iii=iii+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520fbe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def plot_RAMS_tobac_planview_parallel(DOMAIN,FIL,LEV,RAMS_LONS,RAMS_LATS,RAMS_TERR,TOBAC_FILE,TOBAC_DF):\n",
    "\n",
    "    if domain=='ARG1.1-R':\n",
    "        levels=np.arange(-40,40.5,0.5)\n",
    "    elif domain=='PHI1.1-R':\n",
    "        levels=np.arange(-15,15.5,0.5)\n",
    "    elif domain=='DRC1.1-R':\n",
    "        levels=np.arange(-15,15.5,0.5)\n",
    "    elif domain=='PHI2.1-R':\n",
    "        levels=np.arange(-15,15.5,0.5)\n",
    "    else:\n",
    "        print('enter correct domain name')\n",
    "\n",
    "    print('RAMS file is ',FIL)\n",
    "    ds1=xr.open_dataset(FIL,engine='h5netcdf', phony_dims='sort')\n",
    "    rams_time, rams_time_savestr = get_time_from_RAMS_file(FIL)\n",
    "    print('Time in this file ',rams_time)\n",
    "\n",
    "    print('tobac file 1 is ',TOBAC_FILE)\n",
    "    tobac_data_subset=TOBAC_DF[TOBAC_DF.time==pd.to_datetime(rams_time)]\n",
    "\n",
    "    # lat-lon box\n",
    "    #tobac_data_subset = tobac_data_subset[(tobac_data_subset.lon >= -62.95) & (tobac_data_subset.lon <= -62.7) & (tobac_data_subset.lat >= -32.85) & (tobac_data_subset.lat <= -32.625)]    \n",
    "\n",
    "    tobac_lats=tobac_data_subset.lat\n",
    "    tobac_lons=tobac_data_subset.lon\n",
    "\n",
    "    fig = plt.figure(figsize=(14,14))\n",
    "    AX = fig.add_subplot(1, 1, 1, projection=crs.PlateCarree())\n",
    "    C111 = AX.contourf(RAMS_LONS ,RAMS_LATS, ds1.WP[LEV,:,:].values,transform=crs.PlateCarree(),levels=levels,cmap=plt.get_cmap('bwr'),extend='both')#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "    C_terr = AX.contour(RAMS_LONS, RAMS_LATS, RAMS_TERR, np.array([500.]),linewidths=1.4,colors=\"saddlebrown\",transform=crs.PlateCarree())\n",
    "\n",
    "    tobac_features_scatter = AX.scatter(tobac_lons,tobac_lats,label=tobac_filename.split(\".\")[0],marker='o',s=25.5,c='k',transform=crs.PlateCarree())\n",
    "\n",
    "    plt.colorbar(C111,shrink=0.7, pad=0.02,fraction=0.11)\n",
    "    gl = AX.gridlines()#color=\"gray\",alpha=0.5, linestyle='--',draw_labels=True,linewidth=2)\n",
    "    AX.coastlines(resolution='110m')\n",
    "    AX.set_title('Vertical velocity (m/s) at 5km \\n'+rams_time)\n",
    "    #AX.set_title('Vertical velocity at 5 km (m/s)\\n'+rams_time)\n",
    "    #AX.set_title('Vertical velocity at 8 km (m/s)\\n'+rams_time)\n",
    "    gl.xlines = True\n",
    "    gl.ylines = True\n",
    "    LATLON_LABELS=True\n",
    "    if LATLON_LABELS:\n",
    "        print('LATLON labels are on')\n",
    "        gl.xlabels_top = True\n",
    "        gl.ylabels_right = False\n",
    "        gl.ylabels_left = True\n",
    "        gl.ylabels_bottom = True\n",
    "    else:\n",
    "        gl.xlabels_top = False\n",
    "        gl.ylabels_right = False\n",
    "        gl.ylabels_left = False\n",
    "        gl.ylabels_bottom = True\n",
    "\n",
    "    gl.xlabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    gl.ylabel_style = {'size': 15, 'color': 'gray'}#, 'weight': 'bold'}\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    outfilepng='RAMS_tobac_plan_view_'+DOMAIN+TOBAC_FILE.split(\".\")[0]+'_'+rams_time_savestr+'.png'\n",
    "    print('output png is ',outfilepng)\n",
    "    plt.savefig(outfilepng,dpi=150)\n",
    "    #plt.close()\n",
    "    print('--------------\\n')\n",
    "    \n",
    "    \n",
    "# Paths to model data and where to save data\n",
    "\n",
    "#domain='ARG1.1-R'\n",
    "#domain='PHI1.1-R' \n",
    "#domain='PHI2.1-R'\n",
    "domain='DRC1.1-R'\n",
    "\n",
    "path = '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/' # Pleiades\n",
    "#path = '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/' # personal macbook\n",
    "savepath = './'\n",
    "tobac_data='/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/' # Pleiades\n",
    "#tobac_data='/avalanche/pmarin/INCUS/C2X/tobac_tests/'         # personal macbook\n",
    "\n",
    "tobac_filename  = 'comb_track_filt_01_02_05_10_20.p'\n",
    "tobac_filepath  = tobac_data+tobac_filename\n",
    "\n",
    "\n",
    "# Grab all the rams files \n",
    "h5filepath = path+'a-L*g3.h5'\n",
    "h5files1 = sorted(glob.glob(h5filepath))\n",
    "hefilepath = path+'a-L*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "#print(h5files1)\n",
    "start_time=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "end_time=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "print('starting time in simulations: ',start_time)\n",
    "print('ending time in simulations: ',end_time)\n",
    "\n",
    "\n",
    "#### read in RAMS data file to get parameters for plotting ####\n",
    "ds=xr.open_dataset(h5files1[0],engine='h5netcdf', phony_dims='sort')\n",
    "rams_lats=ds.GLAT.values\n",
    "rams_lons=ds.GLON.values\n",
    "rams_terr=ds.TOPT.values\n",
    "zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],h5files1[0])\n",
    "lev_5km = np.argmin(np.abs(zm-5000.))\n",
    "print('level corresponsinding to 5 km is :',lev_5km)\n",
    "\n",
    "\n",
    "thres_list=[1.0,2.0,5.0,10.0,20.0]#,30.0,40.0,50.0]\n",
    "\n",
    "##### read in tobac data #####\n",
    "print('reading ',tobac_filepath)\n",
    "tdata = pd.read_pickle(tobac_filepath)\n",
    "#tdata_temp = tdata[(tdata.time >= pd.to_datetime(start_time)) & (tdata.time <= pd.to_datetime(end_time))]# & (tdata.threshold_value.isin(thres_list))]# & (tdata.zmn >=1000.) & (tdata.zmn <=15000.)] # regular\n",
    "#tdata_temp1 = tdata[(tdata.time == pd.to_datetime(\"2018-12-13 20:50:00\"))] # specific time\n",
    "#tdata_temp = tdata_temp1[(tdata_temp1.lon >= -62.95) & (tdata_temp1.lon <= -62.7) & (tdata_temp1.lat >= -32.85) & (tdata_temp1.lat <= -32.625)]\n",
    "print('number of unique cells identified: ',len(tdata.cell.unique()))\n",
    "\n",
    "\n",
    "# single processor\n",
    "plot_RAMS_tobac_planview_parallel(domain,h5files1[0],lev_5km,rams_lons,rams_lats,rams_terr,tobac_filename,tdata)\n",
    "\n",
    "\n",
    "# multiprocessing\n",
    "\n",
    "############################### FIRST OF ALL ################################\n",
    "#cpu_count1 = cpu_count()\n",
    "#print('number of cpus: ',cpu_count1)\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "# argument = []\n",
    "# for fi in h5files1:\n",
    "#     argument = argument + [(domain,fi,rams_lons,rams_lats,rams_terr,tobac_filename,tdata)]\n",
    "\n",
    "# print(len(argument))\n",
    "\n",
    "# def main(FUNCTION, ARGUMENT):\n",
    "#     pool = Pool(cpu_count1-1)\n",
    "#     start_time = time.perf_counter()\n",
    "#     results = pool.starmap(FUNCTION, ARGUMENT)\n",
    "#     finish_time = time.perf_counter()\n",
    "#     print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(plot_RAMS_tobac_planview_parallel, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c148a7",
   "metadata": {},
   "source": [
    "# Statistics of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbc0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain='DRC1.1-R'\n",
    "path = '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/'\n",
    "savepath = './'\n",
    "# Grab all the rams files \n",
    "h5filepath = path+'a-L*g3.h5'\n",
    "h5files1 = sorted(glob.glob(h5filepath))\n",
    "hefilepath = path+'a-L*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "#print(h5files1)\n",
    "start_time=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "end_time=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "print('starting time in simulations: ',start_time)\n",
    "print('ending time in simulations: ',end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7286552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "simulation_labels=['Phillipines 1: oceanic scattered','Phillipines 2: oceanic MCS','Argentina 1: MCS+isolated','Argentina 2: terrain-forced deep',\\\n",
    "                  'Congo: scattered deep','Western Pacific: scattered','Darwin: organized linear','Amazon: wet season scattered','USA: TRACER']\n",
    "\n",
    "simulation_labels_short=['PHI1','PHI2','ARG1','ARG2','DRC','WPO','DARWIN','AMAZON','TRACER']\n",
    "simulation_names=['PHI1.1-R','PHI2.1-R','ARG1.1-R','ARG1.2-R','DRC1.1-R','WPO1.1-R','AUS1.1-R','BRA1.1-R','USA1.1-R']\n",
    "tobac_track_dirs=['/nobackup/pmarines/DATA_FM/'+sim_name+'/tobac_data/' for sim_name in simulation_names]\n",
    "tobac_track_dirs_old=['/nobackup/pmarines/DATA_FM/'+sim_name+'/tobac_data/OLD/' for sim_name in simulation_names]\n",
    "\n",
    "print(tobac_track_dirs)\n",
    "\n",
    "\n",
    "tobac_file='comb_track_filt_01_02_05_10_20.p'\n",
    "\n",
    "tobac_filepath_list=[vv+tobac_file for vv in tobac_track_dirs]\n",
    "tobac_filepath_list_old=[vv+tobac_file for vv in tobac_track_dirs_old]\n",
    "print('-----')\n",
    "print(tobac_filepath_list)\n",
    "print(tobac_filepath_list_old)\n",
    "\n",
    "# Here data is being filtered to make sure that thermals that reach a max height of 2000 km AGL are not included\n",
    "track_df_list = [pd.read_pickle(track_fil).sort_values(['time'],ascending=True) for track_fil in tobac_filepath_list]\n",
    "track_df_list_old = [pd.read_pickle(track_fil).sort_values(['time'],ascending=True) for track_fil in tobac_filepath_list]\n",
    "\n",
    "# min_dz_thermal=1000.0\n",
    "# max_z_thermal=1000.0\n",
    "# min_z_thermal=15000.0\n",
    "\n",
    "# def custom_filt_func(g):\n",
    "#     return ((g.zmn.max() >= max_z_thermal) & ((g.zmn.max()-g.zmn.min()) >= min_dz_thermal) \\\n",
    "#          &  (g.zmn.min() <= min_z_thermal)                                                )\n",
    "   \n",
    "\n",
    "# def custom_filt_func(g):\n",
    "#     return ((g.zmn.max() >= max_z_thermal) \\\n",
    "#          &  (g.zmn.min() <= min_z_thermal)                                                )\n",
    "\n",
    "    \n",
    "def filt_positive_vert_vel(g):\n",
    "    #print(g)\n",
    "    y = g['zmn'].values\n",
    "    x = g.time_cell.dt.total_seconds().values\n",
    "    del_t=max(x)-min(x)\n",
    "    del_z=y[-1]-y[0]\n",
    "    w_calc = del_z/del_t\n",
    "    return ((g.zmn.max() >= 1500.0) & (g.zmn.min() <= 15000.))# & (w_calc > 0.1))\n",
    "\n",
    "total_cells_list=[]\n",
    "\n",
    "filter_condition_string='standard'\n",
    "plot_base_name = 'tobac_tracking_stats_new_vmax'\n",
    "#print(track_df_list)\n",
    "track_df_filtered_list=[tdata.groupby('cell').filter(filt_positive_vert_vel) for tdata in track_df_list]\n",
    "\n",
    "total_cells_list=[len(tdata.cell.unique()) for tdata in track_df_list]\n",
    "filtered_cells_list=[len(tdata.cell.unique()) for tdata in track_df_filtered_list]\n",
    "print(total_cells_list)\n",
    "print(filtered_cells_list)\n",
    "#plt.hist([total_cells_list, filtered_cells_list], label=simulation_labels_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638a090",
   "metadata": {},
   "source": [
    "## Cell# histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ef9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.bar(x=range(len(total_cells_list)),height=total_cells_list,label='total')\n",
    "plt.bar(x=range(len(filtered_cells_list)),height=filtered_cells_list,label='filtered')\n",
    "ax=plt.gca()\n",
    "ax.set_xticks(range(len(total_cells_list)))\n",
    "ax.set_xticklabels(simulation_names,rotation=45)\n",
    "plt.title('#cells tracked')\n",
    "plt.legend()\n",
    "plt.savefig(plot_base_name+'_number_'+filter_condition_string+'.png',dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076472f",
   "metadata": {},
   "source": [
    "## Cell# time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ii=0\n",
    "for df in track_df_filtered_list:\n",
    "    plt.plot(np.arange(0,len(df.groupby(\"time\")))/2.0,df.groupby(\"time\").cell.nunique().values,label=simulation_labels[ii])\n",
    "    ii=ii+1\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Time since the beginning of tracking (min)') \n",
    "\n",
    "plt.ylabel('#cells')\n",
    "plt.title('Time series of #cells')\n",
    "plt.legend()\n",
    "plt.savefig(plot_base_name+'_timeseries_number_'+filter_condition_string+'.png',dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba413a7",
   "metadata": {},
   "source": [
    "## Cell lifetime histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cell_lifetimes_list = [df.groupby(\"cell\").time.count()*0.5 for df in track_df_filtered_list]\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ii=0\n",
    "for cell_lif in cell_lifetimes_list:\n",
    "    ss1 = sns.kdeplot(cell_lif,label=simulation_labels[ii],fill=False) \n",
    "    ii = ii + 1\n",
    "    \n",
    "plt.xlabel('Cell lifetime (minutes)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('PDF of cell lifetimes')\n",
    "#plt.yscale('log')\n",
    "plt.xlim([1,65])\n",
    "plt.legend()\n",
    "filenamepng=plot_base_name+'_pdf_lifetime_'+filter_condition_string+'.png'\n",
    "print(filenamepng)\n",
    "plt.savefig(filenamepng,dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c917d",
   "metadata": {},
   "source": [
    "### Cell lifetime difference histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cell_lifetimes_list = [df.groupby(\"cell\").time.count()*0.5 for df in track_df_filtered_list]\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ii=0\n",
    "for cell_lif in cell_lifetimes_list:\n",
    "    ss1 = sns.kdeplot(cell_lif,label=simulation_labels[ii],fill=False) \n",
    "    ii = ii + 1\n",
    "    \n",
    "plt.xlabel('Cell lifetime (minutes)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('PDF of cell lifetimes')\n",
    "#plt.yscale('log')\n",
    "plt.xlim([1,65])\n",
    "plt.legend()\n",
    "filenamepng=plot_base_name+'_pdf_lifetime_'+filter_condition_string+'.png'\n",
    "print(filenamepng)\n",
    "plt.savefig(filenamepng,dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7f7b8",
   "metadata": {},
   "source": [
    "## Cell lifetime violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4244253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots of the vertical ascension of thermals\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "snsbxplt=sns.violinplot(data = cell_lifetimes_list)\n",
    "snsbxplt.set_xticklabels(simulation_labels_short)\n",
    "snsbxplt.set_xlabel('Simulation')\n",
    "snsbxplt.set_ylabel('Cell lifetime (minutes)')\n",
    "snsbxplt.set_title('Cell lifetime (minutes)')\n",
    "filenamepng=plot_base_name+'_violinplot_lifetime_'+filter_condition_string+'.png'\n",
    "print(filenamepng)\n",
    "plt.savefig(filenamepng,dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1f857",
   "metadata": {},
   "source": [
    "## Cell max height histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d294d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cell_heights_list = [df.groupby(\"cell\").zmn.max()/1000. for df in track_df_filtered_list]\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ii=0\n",
    "for cell_hgt in cell_heights_list:\n",
    "    ss1 = sns.kdeplot(cell_hgt,label=simulation_labels[ii],fill=False) \n",
    "    ii = ii + 1\n",
    "    \n",
    "plt.xlabel('Maximum height AGL (km)') \n",
    "plt.ylabel('Density')\n",
    "plt.title('PDF of cell maximum height')\n",
    "plt.xlim([0.25,20.])\n",
    "plt.legend()\n",
    "filenamepng=plot_base_name+'_pdf_max_hgt_'+filter_condition_string+'.png'\n",
    "print(filenamepng)\n",
    "plt.savefig(filenamepng,dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e83cc",
   "metadata": {},
   "source": [
    "## Cell ascent rate histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertical velocities\n",
    "# function applied to each group\n",
    "\n",
    "from scipy import odr\n",
    "import seaborn as sns\n",
    "\n",
    "def fit_polynomial(g):\n",
    "    # get height data from the group\n",
    "    y = g['zmn'].values\n",
    "    #print('heights: ',y)\n",
    "    x = g.time_cell.dt.total_seconds().values\n",
    "    #print('times: ',x)\n",
    "    x_high_res = np.arange(min(x),max(x)+20,30)\n",
    "    #print('high res times: ',x_high_res)\n",
    "    #print('tobac centroid heights are : ',y)\n",
    "    #print('tobac times are are : ',x)\n",
    "    poly_model = odr.polynomial(2)  # using second order polynomial model\n",
    "    data = odr.Data(x, y)\n",
    "    odr_obj = odr.ODR(data, poly_model)\n",
    "    output = odr_obj.run()  # running ODR fitting\n",
    "    poly = np.poly1d(output.beta[::-1])\n",
    "    poly_y = poly(x_high_res)\n",
    "    #print('high res polynomial heights',poly_y)\n",
    "    w_calc = np.gradient(np.array(poly_y),x_high_res)\n",
    "    #print('vertical velocities: ',w_calc)\n",
    "    w_calc = w_calc[w_calc>0.5]  # choose only + w\n",
    "    #print('filtered, positive vertical velocities: ',w_calc)\n",
    "    return np.nanmean(w_calc)\n",
    "\n",
    "def linear_vel(g):\n",
    "    y = g['zmn'].values\n",
    "    x = g.time_cell.dt.total_seconds().values\n",
    "    del_t=max(x)-min(x)\n",
    "    del_z=y[-1]-y[0]\n",
    "    w_calc = del_z/del_t\n",
    "    #if w_calc<0.0:\n",
    "    #    w_calc=0.0\n",
    "    return w_calc\n",
    "    \n",
    "\n",
    "cell_mean_w_list  = [df.groupby(\"cell\").apply(linear_vel) for df in track_df_filtered_list] \n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "\n",
    "ii=0\n",
    "for cell_w in cell_mean_w_list:\n",
    "    ss1 = sns.kdeplot(cell_w,label=simulation_labels[ii],fill=False) \n",
    "    ii = ii + 1\n",
    "\n",
    "plt.xlabel('Mean cell ascent rate $(ms^{-1})$') \n",
    "plt.ylabel('Density')\n",
    "plt.title('PDF of mean cell ascent rate')\n",
    "plt.xlim([-5,25.])\n",
    "plt.legend()\n",
    "filenamepng=plot_base_name+'_pdf_w_'+filter_condition_string+'.png'\n",
    "print(filenamepng)\n",
    "plt.savefig(filenamepng,dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678dbd5f",
   "metadata": {},
   "source": [
    "## Cell lifetime violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba1fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots of the vertical ascension of thermals\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "snsbxplt=sns.violinplot(data = cell_mean_w_list)\n",
    "snsbxplt.set_xticklabels(simulation_labels_short)\n",
    "snsbxplt.set_xlabel('Simulation')\n",
    "snsbxplt.set_ylabel('Mean cell ascent rate $(ms^{-1})$')\n",
    "snsbxplt.set_title('Mean cell ascent rate $(ms^{-1})$')\n",
    "filenamepng=plot_base_name+'_violinplot_mean_thermal_ascent_rate_'+filter_condition_string+'.png'\n",
    "print(filenamepng)\n",
    "plt.savefig(filenamepng,dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395cd6ee",
   "metadata": {},
   "source": [
    "## Cell z-distance covered  histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots of the vertical ascension of thermals\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "thermal_z_list = [df.groupby(\"cell\").apply(lambda x: (x.zmn.max() - x.zmn.min())) for df in track_df_filtered_list]\n",
    "snsbxplt=sns.violinplot(data = thermal_z_list)\n",
    "snsbxplt.set_xticklabels(simulation_labels_short)\n",
    "snsbxplt.set_xlabel('Simulation')\n",
    "snsbxplt.set_ylabel('Distance covered in z-direction (m)')\n",
    "snsbxplt.set_title('Distance covered in z-direction (m)')\n",
    "filenamepng=plot_base_name+'_violinplot_dist_z_'+filter_condition_string+'.png'\n",
    "print(filenamepng)\n",
    "plt.savefig(filenamepng,dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c07391",
   "metadata": {},
   "source": [
    "## Cell origin height violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f01ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots of the vertical ascension of thermals\n",
    "\n",
    "cell_orig_heights_list = [df.groupby(\"cell\").apply(lambda x: (x.zmn.min())) for df in track_df_filtered_list]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "snsbxplt=sns.violinplot(data=cell_orig_heights_list)\n",
    "snsbxplt.set_xticklabels(simulation_labels_short)\n",
    "snsbxplt.set_xlabel('Simulation')\n",
    "snsbxplt.set_ylabel('Origin height AGL (AGL; m)')\n",
    "snsbxplt.set_title('Origin height (AGL; m) of tobac-tracked cells')\n",
    "filenamepng=plot_base_name+'_violinplot_orig_hgt_'+filter_condition_string+'.png'\n",
    "print(filenamepng)\n",
    "plt.savefig(filenamepng,dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168b9f5",
   "metadata": {},
   "source": [
    "## Cell origin height histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ii=0\n",
    "for cell_hgt in cell_orig_heights_list:\n",
    "    ss1 = sns.kdeplot(cell_hgt,label=simulation_labels[ii],fill=False) \n",
    "    ii = ii + 1\n",
    "    \n",
    "plt.xlabel('Cell origin height AGL (m)') \n",
    "plt.ylabel('Density')\n",
    "plt.title('PDF of cell origin height')\n",
    "#plt.xlim([0.25,20.])\n",
    "plt.legend()\n",
    "filenamepng=plot_base_name+'_pdf_orig_hgt_'+filter_condition_string+'.png'\n",
    "print(filenamepng)\n",
    "plt.savefig(filenamepng,dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b6848",
   "metadata": {},
   "source": [
    "## Cell max height violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba68f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots of the vertical ascension of thermals\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "thermal_max_z_list = [df.groupby(\"cell\").apply(lambda x: (x.zmn.max())) for df in track_df_filtered_list]\n",
    "snsbxplt=sns.violinplot(data=thermal_max_z_list)\n",
    "snsbxplt.set_xticklabels(simulation_labels_short)\n",
    "snsbxplt.set_xlabel('Simulation')\n",
    "snsbxplt.set_ylabel('Max height AGL (m)')\n",
    "snsbxplt.set_title('Max height (AGL; m) of tobac-tracked cells')\n",
    "filenamepng=plot_base_name+'_violinplot_max_z_'+filter_condition_string+'.png'\n",
    "print(filenamepng)\n",
    "plt.savefig(filenamepng,dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4023011",
   "metadata": {},
   "source": [
    "# Vertical momentum budget of thermals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e682326",
   "metadata": {},
   "source": [
    "## Jeevanjee's formulation of $a_{b}$ and $a_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2986e",
   "metadata": {},
   "source": [
    "-$\\nabla^{2}a_{b}\\bar{\\rho} = g\\nabla^{2}_{h} \\rho$\n",
    "*  $a_{b} = 0$ at top and bottom boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fca8eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def read_var(filename,varname):\n",
    "    with h5py.File(filename,\"r\") as f:\n",
    "        data_out = f[varname][:]\n",
    "    return data_out\n",
    "\n",
    "def read_3dvar_subset(filename,varname,X1,X2,Y1,Y2):\n",
    "    with h5py.File(filename,\"r\") as f:\n",
    "        data_out = f[varname][:,Y1:Y2,X1:X2]\n",
    "    return data_out\n",
    "\n",
    "def read_2dvar_subset(filename,varname,X1,X2,Y1,Y2):\n",
    "    with h5py.File(filename,\"r\") as f:\n",
    "        data_out = f[varname][Y1:Y2,X1:X2]\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True) \n",
    "def gauss_seidel_numba(FIELD_3D,DELTA_X,ZM,NX,NY,NZ,RHS):\n",
    "    FIELD_3D_new = np.zeros_like(FIELD_3D)\n",
    "    for k in range(1, NZ-1):\n",
    "        for j in range(1, NY-1):\n",
    "            for i in range(1, NX-1):\n",
    "                delta_z = ZM[k]-ZM[k-1]\n",
    "                multiplication_factor = 1.0/((4.0/DELTA_X**2)+(2.0/delta_z**2))\n",
    "                #print('delta_z = ',delta_z)\n",
    "                FIELD_3D_new[k,j,i] = ( (FIELD_3D[k,j,i-1]+FIELD_3D[k,j,i+1])/DELTA_X**2 + (FIELD_3D[k,j-1,i]+FIELD_3D[k,j+1,i])/DELTA_X**2 \\\n",
    "                              + (FIELD_3D[k-1,j,i]+FIELD_3D[k+1,j,i])/delta_z**2 - RHS[k,j,i]) * multiplication_factor\n",
    "    \n",
    "    return FIELD_3D_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b51a85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cell by cell approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be344bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "# Paths to model data and where to save data\n",
    "Cp=1004.\n",
    "Rd=287.0\n",
    "p00 = 100000.0\n",
    "\n",
    "\n",
    "\n",
    "domain = 'DRC1.1-R'\n",
    "print('working on simulation: ',domain)\n",
    "simulation_base_folder= '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/'\n",
    "tobac_tracking_dirpath = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'\n",
    "tobac_features_dirpath = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'\n",
    "les_path = simulation_base_folder\n",
    "#\n",
    "tobac_tracking_filename  = 'comb_track_filt_01_02_05_10_20.p'\n",
    "tobac_features_filename  = 'comb_df_01_02_05_10_20.p'\n",
    "\n",
    "tobac_tracking_filepath  = tobac_tracking_dirpath+tobac_tracking_filename\n",
    "tobac_features_filepath  = tobac_features_dirpath+tobac_features_filename\n",
    "\n",
    "#Grab all the rams files\n",
    "h5filepath = les_path+'a-L*g3.h5'\n",
    "h5files1 = sorted(glob.glob(h5filepath))\n",
    "hefilepath = les_path+'a-L*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "#print(h5files1)\n",
    "start_time=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "end_time=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "print('starting time in simulations: ',start_time)\n",
    "print('ending time in simulations: ',end_time)\n",
    "\n",
    "\n",
    "#### read in RAMS data file to get parameters for plotting ####\n",
    "rams_terr=xr.open_dataset(h5files1[0],engine='h5netcdf', phony_dims='sort').TOPT.values\n",
    "\n",
    "\n",
    "zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],h5files1[0])\n",
    "\n",
    "xh=np.arange(dxy/2,nx*dxy,dxy)\n",
    "yh=np.arange(dxy/2,ny*dxy,dxy)\n",
    "\n",
    "##### read in tobac data #####\n",
    "print('reading tracking file: ',tobac_tracking_filepath)\n",
    "tdata =          pd.read_pickle(tobac_tracking_filepath)\n",
    "\n",
    "print('reading features file',tobac_features_filepath)\n",
    "tdata_features = pd.read_pickle(tobac_features_filepath)\n",
    "\n",
    "print('number of unique cells identified: ',len(tdata.cell.unique()))\n",
    "all_cells=np.array(tdata.cell.unique())\n",
    "\n",
    "\n",
    "#single processor below\n",
    "print('number of unique cells : ',len(all_cells))\n",
    "cl = random.choice(all_cells)\n",
    "print('randomly chosen cell#: ',cl)\n",
    "\n",
    "#cl = 13277\n",
    "print('randomly chosen cell is : ',cl)\n",
    "print('plotting vertical cross-sections of w for the first and last time steps for cell # ',cl)\n",
    "plot_vert_zonal_meridional_crosssection_plan_view_tobac_first_last(domain,tdata,cl,xh,yh,None,zm,rams_terr,dxy,plt.get_cmap('bwr'),'/nobackupp11/isingh2/tobac_plots/','segmentaion_box_thres2',tdata_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b13de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata_neu=tdata[tdata['cell']==cl]\n",
    "print('this cell has '+str(len(tdata_neu))+' time steps')\n",
    "xpos=list(tdata_neu.X.values.astype(int))\n",
    "ypos=list(tdata_neu.Y.values.astype(int))\n",
    "zpos=list(tdata_neu.zmn.values.astype(int))    \n",
    "zpos_grid=list(tdata_neu.vdim.values.astype(int))\n",
    "features=list(tdata_neu.feature.values)\n",
    "cell_dim =list(np.array(tdata_neu.num.values.astype(int))**(1/3))\n",
    "times_tracked=tdata_neu.timestr.values\n",
    "times_tracked_pd = pd.to_datetime(times_tracked)\n",
    "thresholds=tdata_neu.threshold_value.values\n",
    "print('---------')\n",
    "x1=min(xpos)-50\n",
    "x2=min(xpos)+50\n",
    "y1=min(ypos)-50\n",
    "y2=min(ypos)+50\n",
    "\n",
    "WP = read_3dvar_subset(h5files1[0],'WP',x1,x2,y1,y2)\n",
    "th = read_3dvar_subset(h5files1[0],'THETA',x1,x2,y1,y2)\n",
    "pi = read_3dvar_subset(h5files1[0],'PI',x1,x2,y1,y2)\n",
    "rv = read_3dvar_subset(h5files1[0],'RV',x1,x2,y1,y2)\n",
    "\n",
    "pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "temp = th*(pi/Cp)\n",
    "plt.imshow(WP[120,:,:])\n",
    "del(th,pi)\n",
    "dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "print('nans in the density field : ',np.isnan(dens).any())\n",
    "del(pres,temp,rv)\n",
    "\n",
    "hor_avgd_rho = np.mean(dens,axis=(1,2))\n",
    "print('shape of hor_avgd_rho',np.shape(hor_avgd_rho))\n",
    "hor_avgd_rho = hor_avgd_rho[:,None,None]\n",
    "print('shape of expanded hor_avgd_rho',np.shape(hor_avgd_rho))\n",
    "#print(dens/hor_avgd_rho)\n",
    "#print('shape of expanded hor_avgd_rho',np.shape(hor_avgd_rho[:,np.newaxis]))\n",
    "\n",
    "\n",
    "nx = np.shape(dens)[2]\n",
    "print('nx = ',nx)\n",
    "ny = np.shape(dens)[1] \n",
    "print('ny = ',ny)\n",
    "nz = np.shape(dens)[0]\n",
    "print('nz = ',nz)\n",
    "\n",
    "rhs = -1.0*9.81*(np.gradient(np.gradient(dens,100.0,axis=(2)),100.0,axis=(2)) + \\\n",
    "                 np.gradient(np.gradient(dens,100.0,axis=(1)),100.0,axis=(1)))\n",
    "print('shape of RHS: ',np.shape(rhs))\n",
    "\n",
    "tolerance = 1e-4\n",
    "max_iter = 10000\n",
    "\n",
    "x  = np.zeros((nz, ny, nx))\n",
    "\n",
    "it = 0 # iteration counter\n",
    "diff = 1.0\n",
    "tol_hist_gs = []\n",
    "\n",
    "p0 = np.zeros((nz, ny, nx))\n",
    "p  = np.zeros((nz, ny, nx))\n",
    "pnew = p0.copy() # initialize with zeros\n",
    "\n",
    "while (diff > tolerance):\n",
    "    if it > max_iter:\n",
    "        print('\\nSolution did not converged within the maximum'\n",
    "              ' number of iterations'\n",
    "              f'\\nLast l2_diff was: {diff:.5e}')\n",
    "        break\n",
    "\n",
    "    # We only modify interior nodes. The boundary nodes remain equal to\n",
    "    # zero and the Dirichlet boundary conditions are therefore automatically\n",
    "    # enforced.\n",
    "    np.copyto(p, pnew)\n",
    "    pnew = gauss_seidel_numba(p,100.,zm,nx,ny,nz,rhs)\n",
    "    diff = LA.norm(pnew.ravel()-p.ravel(), 2)\n",
    "    print('diff = ',diff)\n",
    "    tol_hist_gs.append(diff)\n",
    "\n",
    "    it += 1\n",
    "    print('iteration --> ',it)\n",
    "    #pbar.update(1)\n",
    "\n",
    "else:\n",
    "    print(f'\\nThe solution converged after {it} iterations')\n",
    "\n",
    "#del(pbar)\n",
    "print('shape of pnew = ',np.shape(pnew))\n",
    "a_b_jeevanjee =  pnew/hor_avgd_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ed5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a_b_jeevanjee[150,:,:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e89f18",
   "metadata": {},
   "source": [
    "## For the entire domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('reading file: ',h5files1[20])\n",
    "da=xr.open_dataset(h5files1[20],engine='h5netcdf', phony_dims='sort')\n",
    "\n",
    "WP = da['WP']\n",
    "th = da['THETA']\n",
    "pi = da['PI']\n",
    "rv = da['RV']\n",
    "\n",
    "pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "temp = th*(pi/Cp)\n",
    "plt.imshow(WP[120,:,:])\n",
    "del(th,pi)\n",
    "dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "print('nans in the density field : ',np.isnan(dens).any())\n",
    "del(pres,temp,rv)\n",
    "\n",
    "hor_avgd_rho = np.mean(dens,axis=(1,2))\n",
    "print('shape of hor_avgd_rho',np.shape(hor_avgd_rho))\n",
    "hor_avgd_rho = hor_avgd_rho[:,None,None]\n",
    "print('shape of expanded hor_avgd_rho',np.shape(hor_avgd_rho))\n",
    "#print(dens/hor_avgd_rho)\n",
    "#print('shape of expanded hor_avgd_rho',np.shape(hor_avgd_rho[:,np.newaxis]))\n",
    "\n",
    "\n",
    "nx = np.shape(dens)[2]\n",
    "print('nx = ',nx)\n",
    "ny = np.shape(dens)[1] \n",
    "print('ny = ',ny)\n",
    "nz = np.shape(dens)[0]\n",
    "print('nz = ',nz)\n",
    "\n",
    "rhs = -1.0*9.81*(np.gradient(np.gradient(dens,100.0,axis=(2)),100.0,axis=(2)) + \\\n",
    "                 np.gradient(np.gradient(dens,100.0,axis=(1)),100.0,axis=(1)))\n",
    "print('shape of RHS: ',np.shape(rhs))\n",
    "\n",
    "tolerance = 1e-5\n",
    "max_iter = 10000\n",
    "\n",
    "x  = np.zeros((nz, ny, nx))\n",
    "\n",
    "it = 0 # iteration counter\n",
    "diff = 1.0\n",
    "tol_hist_gs = []\n",
    "\n",
    "p0 = np.zeros((nz, ny, nx))\n",
    "p  = np.zeros((nz, ny, nx))\n",
    "pnew = p0.copy() # initialize with zeros\n",
    "\n",
    "while (diff > tolerance):\n",
    "    if it > max_iter:\n",
    "        print('\\nSolution did not converged within the maximum'\n",
    "              ' number of iterations'\n",
    "              f'\\nLast l2_diff was: {diff:.5e}')\n",
    "        break\n",
    "\n",
    "    # We only modify interior nodes. The boundary nodes remain equal to\n",
    "    # zero and the Dirichlet boundary conditions are therefore automatically\n",
    "    # enforced.\n",
    "    np.copyto(p, pnew)\n",
    "    pnew = gauss_seidel_numba(p,100.,zm,nx,ny,nz,rhs)\n",
    "    diff = LA.norm(pnew.ravel()-p.ravel(), 2)\n",
    "    print('diff = ',diff)\n",
    "    tol_hist_gs.append(diff)\n",
    "\n",
    "    it += 1\n",
    "    print('iteration --> ',it)\n",
    "    #pbar.update(1)\n",
    "\n",
    "else:\n",
    "    print(f'\\nThe solution converged after {it} iterations')\n",
    "\n",
    "#del(pbar)\n",
    "print('shape of pnew = ',np.shape(pnew))\n",
    "a_b_jeevanjee =  pnew/hor_avgd_rho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ae361-f9f0-4264-8fff-b4aebb7f200b",
   "metadata": {},
   "source": [
    "# Variability within the domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28cd8d1-368b-4b6d-83bf-413d13ed815f",
   "metadata": {},
   "source": [
    "## Variograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed52b1-c438-4dc7-92fc-42a1aba79c40",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f80cf8d-5152-4260-aec9-b878ef47191f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting variogram_helper_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile variogram_helper_functions.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import random\n",
    "import skgstat as skg\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import read_vars_WRF_RAMS\n",
    "from libpysal.weights.distance import DistanceBand\n",
    "import libpysal \n",
    "from esda.moran import Moran\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from wrf import smooth2d\n",
    "\n",
    "def find_WRF_file(SIMULATION,DOMAIN,WHICH_TIME):\n",
    "    print('/monsoon/MODEL/LES_MODEL_DATA/V0/'+SIMULATION+'-V0/G'+DOMAIN+'/wrfout*')\n",
    "    wrf_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+SIMULATION+'-V0/G'+DOMAIN+'/wrfout*'))# CSU machine\n",
    "    print('        total # files = ',len(wrf_files))\n",
    "    print('        first file is ',wrf_files[0])\n",
    "    print('        last file is ',wrf_files[-1])\n",
    "    if WHICH_TIME=='start':\n",
    "        selected_fil    = wrf_files[0]\n",
    "    if WHICH_TIME=='middle':\n",
    "        selected_fil    = wrf_files[int(len(wrf_files)/2)]\n",
    "    if WHICH_TIME=='end':\n",
    "        selected_fil    = wrf_files[-1]\n",
    "    print('        choosing the middle file: ',selected_fil)\n",
    "\n",
    "    return selected_fil\n",
    "\n",
    "def find_RAMS_file(SIMULATION, DOMAIN, WHICH_TIME):\n",
    "    if DOMAIN=='1' or DOMAIN =='2':\n",
    "        try:\n",
    "            first_folder = '/monsoon/MODEL/LES_MODEL_DATA/V0/'+SIMULATION+'-V0/G3/out_30s/'\n",
    "            print('searching in ',first_folder)\n",
    "            rams_files=sorted(glob.glob(first_folder+'a-L-*g'+DOMAIN+'.h5'))\n",
    "            print('        total # files = ',len(rams_files))\n",
    "            print('        first file is ',rams_files[0])\n",
    "            print('        last file is ',rams_files[-1])\n",
    "\n",
    "            if WHICH_TIME=='start':\n",
    "                selected_fil    = rams_files[0]\n",
    "            if WHICH_TIME=='middle':\n",
    "                selected_fil    = rams_files[int(len(rams_files)/2)]\n",
    "            if WHICH_TIME=='end':\n",
    "                selected_fil    = rams_files[-1]\n",
    "            print('        choosing the middle file: ',selected_fil)\n",
    "        except (IndexError, FileNotFoundError):\n",
    "            second_folder = '/monsoon/MODEL/LES_MODEL_DATA/V0/'+SIMULATION+'-V0/G'+DOMAIN+'/out/'\n",
    "            print('No files found or folder does not exist. Now searching in '+second_folder)\n",
    "            # Change directory to a different folder and try again\n",
    "            if os.path.isdir(second_folder):\n",
    "                rams_files=sorted(glob.glob(second_folder+'a-A-*g'+DOMAIN+'.h5'))\n",
    "                print('        total # files = ',len(rams_files))\n",
    "                print('        first file is ',rams_files[0])\n",
    "                print('        last file is ',rams_files[-1])\n",
    "\n",
    "                if WHICH_TIME=='start':\n",
    "                    selected_fil    = rams_files[0]\n",
    "                if WHICH_TIME=='middle':\n",
    "                    selected_fil    = rams_files[int(len(rams_files)/2)]\n",
    "                if WHICH_TIME=='end':\n",
    "                    selected_fil    = rams_files[-1]\n",
    "                print('        choosing the middle file: ',selected_fil)           \n",
    "            else:\n",
    "                print(\"Alternate folder does not exist. Exiting function.\")\n",
    "        \n",
    "    if DOMAIN=='3':\n",
    "        try:\n",
    "            first_folder = '/monsoon/MODEL/LES_MODEL_DATA/V0/'+SIMULATION+'-V0/G'+DOMAIN+'/out_30s/'\n",
    "            print('searching in ',first_folder)\n",
    "            rams_files=sorted(glob.glob(first_folder+'a-L-*g3.h5'))\n",
    "            print('        total # files = ',len(rams_files))\n",
    "            print('        first file is ',rams_files[0])\n",
    "            print('        last file is ',rams_files[-1])\n",
    "\n",
    "            if WHICH_TIME=='start':\n",
    "                selected_fil    = rams_files[0]\n",
    "            if WHICH_TIME=='middle':\n",
    "                selected_fil    = rams_files[int(len(rams_files)/2)]\n",
    "            if WHICH_TIME=='end':\n",
    "                selected_fil    = rams_files[-1]\n",
    "            print('        choosing the middle file: ',selected_fil)\n",
    "        except (IndexError, FileNotFoundError):\n",
    "            second_folder = '/monsoon/MODEL/LES_MODEL_DATA/V0/'+SIMULATION+'-V0/G'+DOMAIN+'_old/out_30s/'\n",
    "            print('No files found or folder does not exist. Now searching in '+second_folder)\n",
    "            # Change directory to a different folder and try again\n",
    "            if os.path.isdir(second_folder):\n",
    "                rams_files=sorted(glob.glob(second_folder+'a-L-*g3.h5'))#\n",
    "                print('        total # files = ',len(rams_files))\n",
    "                print('        first file is ',rams_files[0])\n",
    "                print('        last file is ',rams_files[-1])\n",
    "\n",
    "                if WHICH_TIME=='start':\n",
    "                    selected_fil    = rams_files[0]\n",
    "                if WHICH_TIME=='middle':\n",
    "                    selected_fil    = rams_files[int(len(rams_files)/2)]\n",
    "                if WHICH_TIME=='end':\n",
    "                    selected_fil    = rams_files[-1]\n",
    "                print('        choosing the middle file: ',selected_fil)\n",
    "            else:\n",
    "                print(\"Alternate folder does not exist. Exiting function.\")\n",
    "\n",
    "    return selected_fil\n",
    "   \n",
    "def read_head(headfile,h5file):\n",
    "        # Function that reads header files from RAMS\n",
    "\n",
    "        # Inputs:\n",
    "        #   headfile: header file including full path in str format\n",
    "        #   h5file: h5 datafile including full path in str format\n",
    "\n",
    "        # Returns:\n",
    "        #   zmn: height levels for momentum values (i.e., grid box upper and lower levels)\n",
    "        #   ztn: height levels for thermodynaic values (i.e., grid box centers)\n",
    "        #   nx:: the number of x points for the domain associated with the h5file\n",
    "        #   ny: the number of y points for the domain associated with the h5file\n",
    "        #   npa: the number of surface patches\n",
    "\n",
    "\n",
    "        dom_num = h5file[h5file.index('.h5')-1] # Find index of .h5 to determine position showing which nest domain to use\n",
    "\n",
    "        with open(headfile) as f:\n",
    "            contents = f.readlines()\n",
    "\n",
    "        idx_zmn = contents.index('__zmn0'+dom_num+'\\n')\n",
    "        nz_m = int(contents[idx_zmn+1])\n",
    "        zmn = np.zeros(nz_m)\n",
    "        for i in np.arange(0,nz_m):\n",
    "            zmn[i] =  float(contents[idx_zmn+2+i])\n",
    "\n",
    "        idx_ztn = contents.index('__ztn0'+dom_num+'\\n')\n",
    "        nz_t = int(contents[idx_ztn+1])\n",
    "        ztn = np.zeros(nz_t)\n",
    "        for i in np.arange(0,nz_t):\n",
    "            ztn[i] =  float(contents[idx_ztn+2+i])\n",
    "\n",
    "        ztop = np.max(ztn) # Model domain top (m)\n",
    "\n",
    "        # Grad the size of the horizontal grid spacing\n",
    "        idx_dxy = contents.index('__deltaxn\\n')\n",
    "        dxy = float(contents[idx_dxy+1+int(dom_num)].strip())\n",
    "\n",
    "        idx_npatch = contents.index('__npatch\\n')\n",
    "        npa = int(contents[idx_npatch+2])\n",
    "\n",
    "        idx_ny = contents.index('__nnyp\\n')\n",
    "        idx_nx = contents.index('__nnxp\\n')\n",
    "        ny = np.ones(int(contents[idx_ny+1]))\n",
    "        nx = np.ones(int(contents[idx_ny+1]))\n",
    "        for i in np.arange(0,len(ny)):\n",
    "            nx[i] = int(contents[idx_nx+2+i])\n",
    "            ny[i] = int(contents[idx_ny+2+i])\n",
    "\n",
    "        ny_out = ny[int(dom_num)-1]\n",
    "        nx_out = nx[int(dom_num)-1]\n",
    "\n",
    "        return zmn, ztn, nx_out, ny_out, dxy, npa \n",
    "    \n",
    "def produce_random_coords(X_DIM,Y_DIM,SAMPLE_SIZE,COORDS_RETURN_TYPE='list'):\n",
    "    print('getting a random sample of coordinates...')\n",
    "    print('        shape of the arrays is ',Y_DIM,'x',X_DIM)\n",
    "    x      = np.arange(0,X_DIM)\n",
    "    y      = np.arange(0,Y_DIM)\n",
    "    # # full coordinate arrays\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "        \n",
    "    coords_tuples_2d = np.vstack(([yy.T], [xx.T])).T\n",
    "    print('        shape of combined coords matrix: ',np.shape(coords_tuples_2d))\n",
    "    coords_all = coords_tuples_2d.reshape(-1, 2).tolist()\n",
    "    print('        shape of 1d list of coords: ',np.shape(coords_all))\n",
    "    \n",
    "    if COORDS_RETURN_TYPE=='tuple':\n",
    "        coords_all =  [tuple(sublist) for sublist in coords_all]\n",
    "    \n",
    "    if SAMPLE_SIZE>=(X_DIM*Y_DIM):\n",
    "        print('        sample = or > than the population; choosing all points')\n",
    "        coords = coords_all \n",
    "    else:\n",
    "        coords = random.sample(coords_all,SAMPLE_SIZE)\n",
    "        \n",
    "    return coords\n",
    "\n",
    "def produce_random_coords_conditional(SAMPLE_SIZE,TWOD_CONDITIONAL_FIELD, CONDITION_STATEMENT=lambda x: x != np.nan,COORDS_RETURN_TYPE='list'):\n",
    "    print('getting a random sample of coordinates where ',CONDITION_STATEMENT)\n",
    "    print('        shape of the 2D condition field is ',np.shape(TWOD_CONDITIONAL_FIELD))\n",
    "    \n",
    "    def indices_where_condition_met(array, condition):\n",
    "        indices = np.where(condition(array))\n",
    "        return list(zip(indices[0], indices[1]))\n",
    "\n",
    "    # Get indices where condition is met\n",
    "    coords_all = indices_where_condition_met(TWOD_CONDITIONAL_FIELD, CONDITION_STATEMENT)\n",
    "    print('length of all coordinates where condition is met is ',len(coords_all),' about ',int(len(coords_all)*100.0/TWOD_CONDITIONAL_FIELD.size), ' percent of the total grid points')\n",
    "\n",
    "    if COORDS_RETURN_TYPE=='list':\n",
    "        coords_all =  [list(sublist) for sublist in coords_all]\n",
    "    if COORDS_RETURN_TYPE=='tuple':\n",
    "        pass\n",
    "\n",
    "    print('        shape of 1d list of coords: ',np.shape(coords_all))\n",
    "    \n",
    "    if SAMPLE_SIZE>=(np.shape(TWOD_CONDITIONAL_FIELD)[0]*np.shape(TWOD_CONDITIONAL_FIELD)[1]):\n",
    "        print('        sample = or > than the population; choosing all points')\n",
    "        coords = coords_all \n",
    "    if SAMPLE_SIZE>len(coords_all):\n",
    "        coords = coords_all \n",
    "    else:\n",
    "        coords = random.sample(coords_all,SAMPLE_SIZE)\n",
    "    return coords\n",
    "    \n",
    "def get_values_at_random_coords(TWOD_FIELD, COORDS, COORDS_RETURN_TYPE='list'):\n",
    "    print('getting values at the chosen coordinates...')\n",
    "    print('        got the data... min = ',np.nanmin(TWOD_FIELD),' max = ',np.nanmax(TWOD_FIELD))\n",
    "    print('        percentage of nans is ',np.count_nonzero(np.isnan(TWOD_FIELD))/len(TWOD_FIELD.flatten()))\n",
    "    print('        choosing '+str(len(COORDS))+' random points...')\n",
    "    print('        get field values from these points...')\n",
    "    values = np.fromiter((TWOD_FIELD[c[0], c[1]] for c in COORDS), dtype=float)\n",
    "    # Remove nan values\n",
    "    print('        Removing nan values and the corresponding coordinates...')\n",
    "    nan_mask = ~np.isnan(values)\n",
    "    print('        # non-nan values',np.count_nonzero(nan_mask))\n",
    "    values   = values[nan_mask]\n",
    "    sampled_coords_array = np.array(COORDS)\n",
    "    coords   = sampled_coords_array[nan_mask].tolist()\n",
    "    \n",
    "    if COORDS_RETURN_TYPE=='tuple':\n",
    "        coords =  [tuple(sublist) for sublist in coords]\n",
    "        \n",
    "    print('        final shape of coords is ',np.shape(coords))\n",
    "    print('        final shape of values is ',np.shape(values))\n",
    "    return coords, values\n",
    "\n",
    "def make_variogram(COORDS, VALUES, NBINS, MAXLAG, DX=1.0, BIN_FUNCTION='even',ESTIMATOR='matheron'):\n",
    "    \"\"\"\n",
    "    Estimator options:\n",
    "    1. matheron [Matheron, default]\n",
    "    2. cressie [Cressie-Hawkins]\n",
    "    3. dowd [Dowd-Estimator]\n",
    "    4. genton [Genton]\n",
    "    5. minmax [MinMax Scaler]\n",
    "    6. entropy [Shannon Entropy]\n",
    "    \"\"\"\n",
    "    print('        creating variogram...')\n",
    "    print('        MAXLAG= ',MAXLAG,'grid points')\n",
    "    V        = skg.Variogram(COORDS, VALUES,n_lags=NBINS,maxlag = MAXLAG, bin_func=BIN_FUNCTION,estimator=ESTIMATOR)\n",
    "    bins     = V.bins*DX # convert from integer coordinates to physical coordinates (km)\n",
    "    #print('        upper edges of bins: ',bins,'\\n')\n",
    "    bins = np.subtract(bins, np.diff([0] + bins.tolist()) / 2)\n",
    "    #print('        mid points of bins: ',bins)\n",
    "    exp_variogram =  V.experimental\n",
    "    #matrix_for_saving = np.array([bins,exp_variogram]).T\n",
    "    return V , bins, exp_variogram#, matrix_for_saving\n",
    "    \n",
    "def retrieve_histogram(VARIOGRAM,DX=1.0):\n",
    "    print('        retreiving counts of pairwise obs per lag class ...')\n",
    "    bins_upper_edges = VARIOGRAM.bins*DX\n",
    "    counts = np.fromiter((g.size for g in VARIOGRAM.lag_classes()), dtype=int)\n",
    "    widths = np.diff([0] + bins_upper_edges.tolist())\n",
    "    bins_middle_points   = np.subtract(bins_upper_edges, np.diff([0] + bins_upper_edges.tolist()) / 2)\n",
    "    #print('        widths of lag classes are: ',widths)\n",
    "    #print('length of bins_middle_points:',len(bins_middle_points))\n",
    "    #print('length of width:',len(widths))\n",
    "    return bins_middle_points, counts, widths\n",
    "\n",
    "def grab_intersection_gbig_gsmall_RAMS(VARIABLE,RAMS_G1_or_G2_FILE,RAMS_G3_FILE):\n",
    "    z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(RAMS_G1_or_G2_FILE,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "    #print(np.min(z))\n",
    "    #print(np.max(z))\n",
    "    #z2, z_name2, z_units2, z_time2 = read_vars_WRF_RAMS.read_variable(RAMS_G3_FILE,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "    print('        done getting the variable ',VARIABLE[0],' with shape: ',np.shape(z),'\\n')\n",
    "    print('        subsetting the larger domain...\\n')\n",
    "    # read the variables for which you want the variogram\n",
    "    ds_big   = xr.open_dataset(RAMS_G1_or_G2_FILE,engine='h5netcdf',phony_dims='sort')[['GLAT','GLON']]\n",
    "    ds_small = xr.open_dataset(RAMS_G3_FILE,engine='h5netcdf',phony_dims='sort')[['GLAT','GLON']]\n",
    "    dim1, dim2 = ds_big.GLAT.dims\n",
    "    #print(ds_big)\n",
    "    #print(ds_small)\n",
    "    #ds_big = ds_big.rename_dims({'phony_dim_0': 'y','phony_dim_1': 'x'})\n",
    "    #ds_small = ds_small.rename_dims({'phony_dim_0': 'y','phony_dim_1': 'x'})\n",
    "    min_lat_big = ds_big.GLAT.min().values\n",
    "    max_lat_big = ds_big.GLAT.max().values\n",
    "    min_lon_big = ds_big.GLON.min().values\n",
    "    max_lon_big = ds_big.GLON.max().values\n",
    "    print('        min and max lat for big domain = ',min_lat_big,' ',max_lat_big)\n",
    "    print('        min and max lon for big domain = ',min_lon_big,' ',max_lon_big)\n",
    "    print('        ----')\n",
    "    min_lat_small = ds_small.GLAT.min().values\n",
    "    max_lat_small = ds_small.GLAT.max().values\n",
    "    min_lon_small = ds_small.GLON.min().values\n",
    "    max_lon_small = ds_small.GLON.max().values\n",
    "    print('        min and max lat for small domain = ',min_lat_small,' ',max_lat_small)\n",
    "    print('        min and max lon for small domain = ',min_lon_small,' ',max_lon_small)\n",
    "    print('        ----')\n",
    "    #subset by lat/lon - used so only region covered by inner grid is compared\n",
    "    ds = xr.Dataset({VARIABLE[0]: xr.DataArray(data   = z,  dims   = [dim1,dim2])})\n",
    "    ds = ds.assign(GLAT=ds_big.GLAT)\n",
    "    ds = ds.assign(GLON=ds_big.GLON)\n",
    "    #print(ds)\n",
    "    ds = ds.where((ds.GLAT>=min_lat_small) & (ds.GLAT<=max_lat_small) & (ds.GLON>=min_lon_small) & (ds.GLON<=max_lon_small), drop=True)\n",
    "    #print(ds)\n",
    "    min_lat = ds.GLAT.min().values\n",
    "    max_lat = ds.GLAT.max().values\n",
    "    min_lon = ds.GLON.min().values\n",
    "    max_lon = ds.GLON.max().values\n",
    "    \n",
    "    print('        min and max lat for modified domain = ',min_lat,' ',max_lat)\n",
    "    print('        min and max lon for modified domain = ',min_lon,' ',max_lon)\n",
    "    print('        ----')\n",
    "    \n",
    "    #print(ds)\n",
    "    print('        shape of small domain: ',np.shape(ds_small.GLAT))\n",
    "    print('        shape of big domain: ',np.shape(ds_big.GLAT))\n",
    "    print('        shape of modified domain: ',np.shape(ds.GLAT))\n",
    "    #return z, z_name, z_units, z_time\n",
    "    return ds.variables[VARIABLE[0]].values, z_name, z_units, z_time\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S'), pd_time\n",
    "\n",
    "def find_closest_datetime_index(datetime_list, target_datetime):\n",
    "    \"\"\"\n",
    "    Find the index of the closest datetime in the datetime_list to the target_datetime.\n",
    "    \"\"\"\n",
    "    closest_datetime = min(datetime_list, key=lambda x: abs(x - target_datetime))\n",
    "    closest_index = datetime_list.index(closest_datetime)\n",
    "    return closest_index\n",
    "\n",
    "def compute_moran(DISTANCE_INTERVAL, COORDS, VALUES):\n",
    "    # Create binary spatial weights matrix based on distance interval\n",
    "    w = libpysal.weights.DistanceBand(COORDS, threshold=DISTANCE_INTERVAL, binary=True, silence_warnings=True)\n",
    "    # Compute Moran's I\n",
    "    moran = Moran(VALUES, w)\n",
    "    return DISTANCE_INTERVAL, moran.I, moran.EI, moran.VI_norm, moran.p_norm, moran.z_norm\n",
    "\n",
    "\n",
    "def arrange_images_with_wildcard(input_folder, output_file, wildcard_pattern, non_target_string):\n",
    "    # Get a list of PNG images in the input folder matching the wildcard pattern\n",
    "    if non_target_string:\n",
    "        image_files = sorted([f for f in glob.glob(os.path.join(input_folder, wildcard_pattern)) if f.lower().endswith('.png') and non_target_string not in f])[1::2]\n",
    "    else:\n",
    "        image_files = sorted([f for f in glob.glob(os.path.join(input_folder, wildcard_pattern)) if f.lower().endswith('.png')])[1::2]\n",
    "\n",
    "    print('found ',len(image_files),' images')\n",
    "    for fil in image_files:\n",
    "        print(fil)\n",
    "    # Check if there are any matching images\n",
    "    if not image_files:\n",
    "        print(f\"Error: No PNG images matching the wildcard pattern '{wildcard_pattern}' found in the folder.\")\n",
    "        return\n",
    "\n",
    "    # Calculate the number of rows and columns for the matrix\n",
    "    num_images = len(image_files)\n",
    "    num_cols = int(math.sqrt(num_images))\n",
    "    num_rows = math.ceil(num_images / num_cols)\n",
    "\n",
    "    # Create a new image with dimensions for the matrix and reduced white space\n",
    "    img_width, img_height = Image.open(image_files[0]).size\n",
    "    margin = 60  # Adjust this value to control the margin\n",
    "    result_image = Image.new('RGB', (num_cols * (img_width - margin), num_rows * (img_height - margin)))\n",
    "\n",
    "    # Loop through the matching images and paste them onto the result image with reduced white space\n",
    "    for i in range(num_images):\n",
    "        img = Image.open(image_files[i])\n",
    "\n",
    "        # Calculate the position with margin to paste the image\n",
    "        col = i % num_cols\n",
    "        row = i // num_cols\n",
    "        position = (col * (img_width - margin), row * (img_height - margin))\n",
    "\n",
    "        # Paste the image onto the result image\n",
    "        result_image.paste(img, position)\n",
    "\n",
    "    # Save the result image\n",
    "    result_image.save(output_file)\n",
    "    \n",
    "    \n",
    "def make_plan_view(WHICH_TIME, VARIABLE, SIMULATION, DOMAIN, CMAP, SAMPLE_SIZE, SAVEFILE, CONDITION_INFO=None, MASKED_PLOT=False):\n",
    "\n",
    "    units_dict = {'Tk':'$K$','QV':'$kg kg^{-1}$','RH':'percent','WSPD':'$m s^{-1}$','U':'$m s^{-1}$',\\\n",
    "              'V':'$m s^{-1}$','W':'$m s^{-1}$','MCAPE':'$J kg^{-1}$','MCIN':'$J kg^{-1}$','THETA':'$K$','QTC':'$kg kg^{-1}$',\\\n",
    "                  'SHF':'$W m^{-2}$', 'LHF':'$W m^{-2}$','MAXCOL_W':'$m s^{-1}$'}\n",
    "    \n",
    "    vmin_vmax_dict = {'Tk':[290,331,1],'QV':[0.006,0.0024,0.001],'RH':[70,101,1],'WSPD':[1,20,1],'U':[1,20,1],\\\n",
    "              'V':[1,20,1],'W':[-5,21,1],'MCAPE':[100,3100,100],'MCIN':[0,310,10],'THETA':[290,331,1]}\n",
    "\n",
    "    print('Contour plotting ',VARIABLE,'\\n')\n",
    "    \n",
    "\n",
    "    fig    = plt.figure(figsize=(8,8))\n",
    "    print('    working on simulation: ',SIMULATION)\n",
    "    #if model_name=='RAMS':   \n",
    "    selected_fil = find_RAMS_file(SIMULATION=SIMULATION,DOMAIN=DOMAIN,WHICH_TIME=WHICH_TIME)\n",
    "    #if model_name=='WRF':\n",
    "    #        selected_fil =  variogram_helper_functions.find_WRF_file(SIMULATION=simulation,DOMAIN=DOMAIN,WHICH_TIME=WHICH_TIME)\n",
    "    z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(selected_fil,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "    y_dim,x_dim = np.shape(z)\n",
    "\n",
    "    if DOMAIN=='1':\n",
    "        dx=1.6\n",
    "    if DOMAIN=='2':\n",
    "        dx=0.4\n",
    "    if DOMAIN=='3':\n",
    "        dx=0.1\n",
    "    xx = np.arange(0,dx*x_dim,dx)\n",
    "    yy = np.arange(0,dx*y_dim,dx)\n",
    "    timestep_pd     = pd.to_datetime(z_time,format='%Y%m%d%H%M%S')\n",
    "\n",
    "    if CONDITION_INFO:\n",
    "        print('conditional information given')\n",
    "        if CONDITION_INFO[0]=='environment':\n",
    "            print('        getting random coordinates over ',CONDITION_INFO[0],' points')\n",
    "            print('        conditioned on total condensate')\n",
    "            if VARIABLE[1]<0:\n",
    "                conditional_field, _, _, _ = read_vars_WRF_RAMS.read_variable(selected_fil,'QTC','RAMS',output_height=False,interpolate=True,level=0,interptype='model')\n",
    "            else:\n",
    "                conditional_field, _, _, _ = read_vars_WRF_RAMS.read_variable(selected_fil,'QTC','RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "            if CONDITION_INFO[2]:\n",
    "                print('        smoothing the condition field')\n",
    "                conditional_field = smooth2d(conditional_field, passes=1, meta=False)\n",
    "                #conditional_field = gaussian_filter(conditional_field, sigma=1) \n",
    "            if MASKED_PLOT:\n",
    "                masked_z = np.ma.masked_where(conditional_field > CONDITION_INFO[1], z)\n",
    "                main_cont =plt.contourf(xx,yy,masked_z,levels=30,cmap=CMAP,extend='both')\n",
    "            else:\n",
    "                main_cont =plt.contourf(xx,yy,z,levels=30,cmap=CMAP,extend='both')\n",
    "            print('        min, max for the condensate field is ',np.min(conditional_field),' ',np.max(conditional_field))\n",
    "            coords = produce_random_coords_conditional(SAMPLE_SIZE, conditional_field, CONDITION_STATEMENT=lambda x: x < CONDITION_INFO[1])\n",
    "        if CONDITION_INFO[0]=='storm': \n",
    "            print('        getting random coordinates over ',CONDITION_INFO[0],' points')\n",
    "            print('        conditioned on total condensate')\n",
    "            if VARIABLE[1]<0:\n",
    "                conditional_field, _, _, _ = read_vars_WRF_RAMS.read_variable(selected_fil,'QTC','RAMS',output_height=False,interpolate=True,level=0,interptype='model')\n",
    "            else:\n",
    "                conditional_field, _, _, _ = read_vars_WRF_RAMS.read_variable(selected_fil,'QTC','RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "            if CONDITION_INFO[2]:\n",
    "                print('        smoothing the condition field')\n",
    "                conditional_field = smooth2d(conditional_field, passes=1, meta=False)\n",
    "                #conditional_field = gaussian_filter(conditional_field, sigma=1) \n",
    "            if MASKED_PLOT:\n",
    "                masked_z = np.ma.masked_where(conditional_field <=CONDITION_INFO[1], z)\n",
    "                main_cont =plt.contourf(xx,yy,masked_z,levels=30,cmap=CMAP,extend='both')\n",
    "            else:\n",
    "                main_cont =plt.contourf(xx,yy,z,levels=30,cmap=CMAP,extend='both')\n",
    "            print('        min, max for the condensate field is ',np.min(conditional_field),' ',np.max(conditional_field))\n",
    "            coords = produce_random_coords_conditional(SAMPLE_SIZE, conditional_field, CONDITION_STATEMENT=lambda x: x >= CONDITION_INFO[1])\n",
    "        if CONDITION_INFO[0]=='all':\n",
    "            print('getting random coordinates over ',CONDITION_INFO[0],' points')\n",
    "            coords = produce_random_coords(x_dim,y_dim,SAMPLE_SIZE)   \n",
    "            main_cont =plt.contourf(xx,yy,z,levels=30,cmap=CMAP,extend='both')\n",
    "\n",
    "        # Create scatter plot\n",
    "        y_coords, x_coords = zip(*coords)\n",
    "        plt.scatter(np.array(x_coords)*dx, np.array(y_coords)*dx, color='red', marker='o',s=.07)\n",
    "    else:\n",
    "        main_cont =plt.contourf(xx,yy,z,levels=30,cmap=CMAP,extend='both')\n",
    "        \n",
    "    if VARIABLE[2]:\n",
    "        if  VARIABLE[2]=='pressure':\n",
    "            level_units = ' mb'\n",
    "            lev = int(VARIABLE[1])\n",
    "        if  VARIABLE[2]=='model':\n",
    "            level_units = ''\n",
    "            lev = int(VARIABLE[1]+1)\n",
    "        title_string = SIMULATION+' '+VARIABLE[0]+' ('+units_dict[VARIABLE[0]]+')'+' at '+VARIABLE[2]+' level '+str(lev)+level_units+' for G'+DOMAIN+'\\n'+timestep_pd.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        title_string = SIMULATION+' '+VARIABLE[0]+' ('+units_dict[VARIABLE[0]]+')'+' for G'+DOMAIN+'\\n'+timestep_pd.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    plt.title(title_string,fontsize=16)\n",
    "    plt.xlabel('x (km)',fontsize=16)\n",
    "    plt.ylabel('y (km)',fontsize=16)\n",
    "    plt.colorbar(main_cont)\n",
    "    \n",
    "    if SAVEFILE:\n",
    "        if VARIABLE[2]:\n",
    "            filename = 'plan_view_RAMS_'+SIMULATION+'_G'+DOMAIN+'_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_'+z_time+'.png'\n",
    "        else:\n",
    "            filename = 'plan_view_RAMS_'+SIMULATION+'_G'+DOMAIN+'_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_'+z_time+'.png'\n",
    "        print('saving to png file: ',filename)\n",
    "        plt.savefig(filename,dpi=150)\n",
    "    #plt.close()\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa39b71-864d-4257-bba9-448ae8fc313a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Drawing multiple samples for the same variable (box plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e6539b-62ed-4567-a75b-b674faf2bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random coordinates\n",
    "#from random import sample \n",
    "def create_box_plot_multiple_variograms(simulations,variables,ylabels,nsamples,sample_size,nbins,area_type='all'):\n",
    "    for ii, var in enumerate(variables):\n",
    "        print('working on ',var,'\\n')\n",
    "        for simulation in simulations:  \n",
    "            vario_experimental_list = []\n",
    "            vario_bin_list =[]\n",
    "            print('    working on ',simulation,'\\n')\n",
    "            rams_files_g3=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G3/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "            print('    total # files = ',len(rams_files_g3))\n",
    "            print('    first file is ',rams_files_g3[0])\n",
    "            print('    last file is ',rams_files_g3[-1])\n",
    "            rams_fil     = rams_files_g3[int(len(rams_files_g3)/2)]\n",
    "            print('    choosing the middle file: ',rams_fil)\n",
    "            timestr = get_time_from_RAMS_file(rams_fil)[1]\n",
    "            da     = xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "            z_temp = da['TOPT'].values\n",
    "            y_dim, x_dim = np.shape(z_temp)\n",
    "            print('    shape of the arrays is ',y_dim,'x',x_dim)\n",
    "            \n",
    "            x      = np.arange(0,x_dim)\n",
    "            y      = np.arange(0,y_dim)\n",
    "            \n",
    "            if area_type=='near_storm':\n",
    "                print('    choosing points near storms...')\n",
    "                mask_name = 'storm_mask_'+simulation+'_'+timestr+'.npy'\n",
    "                print('    using storm mask file: ',mask_name)\n",
    "                mask    = np.load(mask_name)\n",
    "                xx, yy = np.meshgrid(x, y)*mask\n",
    "            elif area_type=='not_near_storm':\n",
    "                print('    choosing points away from storms...')\n",
    "                mask_name = 'storm_mask_'+simulation+'_'+timestr+'.npy'\n",
    "                print('    using storm mask file: ',mask_name)\n",
    "                mask    = np.load(mask_name)\n",
    "                mask    = np.where((mask < 1.1) & (mask > 0.9), np.nan , 1.0)\n",
    "                xx, yy = np.meshgrid(x, y)*mask\n",
    "            elif area_type=='all':\n",
    "                print('    choosing all points...')\n",
    "                xx, yy = np.meshgrid(x, y)\n",
    "            else:\n",
    "                print('    please provide a corect value of area type!!!')\n",
    "                #exit()\n",
    "            # # full coordinate arrays\n",
    "            \n",
    "            coords_tuples_2d = np.vstack(([yy.T], [xx.T])).T\n",
    "            print('    shape of combined coords matrix: ',np.shape(coords_tuples_2d))\n",
    "            coords_all_np    = coords_tuples_2d.reshape(-1, 2)#.tolist()\n",
    "            print('    shape of flattened combined coords matrix: ',np.shape(coords_all_np))\n",
    "            \n",
    "            if area_type!='all':\n",
    "                # Create a boolean mask for rows with NaN values\n",
    "                nan_rows_mask = np.any(np.isnan(coords_all_np), axis=1)\n",
    "                # Use the mask to select rows without NaN values\n",
    "                coords_all_np = coords_all_np[~nan_rows_mask]\n",
    "                coords_all    = coords_all_np.tolist()\n",
    "            else:\n",
    "                coords_all    = coords_all_np.tolist()\n",
    "            \n",
    "            print('    shape of flattened combined coords list with nans removed: ',np.shape(coords_all))\n",
    "            values_all = z_temp.flatten()# reshape(-1,0)\n",
    "            print('    shape of flattened values array : ',np.shape(values_all))\n",
    "\n",
    "            if var=='WSPD':\n",
    "                print('    calculating WSPD')\n",
    "                z      = np.sqrt((da['UP'][0,:,:].values**2 + da['VP'][0,:,:].values**2))\n",
    "            if var=='500mb_RH':\n",
    "                print('    calculating WSPD')\n",
    "                z      = np.sqrt((da['UP'][0,:,:].values**2 + da['VP'][0,:,:].values**2))\n",
    "            else:\n",
    "                z      = da[var][0,:,:].values\n",
    "            \n",
    "            fig     = plt.figure(figsize=(8,8))\n",
    "            \n",
    "            for sample_no in range(nsamples):\n",
    "                print('        working on sample#: ',sample_no)\n",
    "                print('        choosing ',sample_size,' random points...')\n",
    "                coords = random.sample(coords_all,sample_size)\n",
    "                print('        shape of '+str(sample_size)+' random selected coords :',np.shape(coords))\n",
    "                print('        get field values from these points...')\n",
    "                values = np.fromiter((z[int(c[0]), int(c[1])] for c in coords), dtype=float)\n",
    "                V      = skg.Variogram(coords, values,n_lags=nbins,bin_func='even')\n",
    "                #print('        ',V.experimental)\n",
    "                #print('        ',np.shape(V.experimental))\n",
    "                vario_experimental_list.append(V.experimental)\n",
    "                vario_bin_list.append(V.bins/10.0)\n",
    "                print('        --\\n')\n",
    "                \n",
    "            #print('        Variogram properties: ')\n",
    "            #pprint(V.describe())\n",
    "            #print('\\n')\n",
    "            vario_experimental_array = np.array(vario_experimental_list)\n",
    "            vario_bin_array = np.array(vario_bin_list)\n",
    "            average_bin_values = np.mean(vario_bin_array,axis=0)\n",
    "            #print('average bin values are: ',average_bin_values)\n",
    "            df = pd.DataFrame(vario_experimental_array, columns=average_bin_values)\n",
    "            #print(df)\n",
    "            #print(vario_bin_array)\n",
    "            #print(vario_experimental_array)\n",
    "            print(np.shape(vario_experimental_array))\n",
    "            print(np.shape(vario_bin_list))\n",
    "            #for vario in vario_list:\n",
    "            #    plt.plot(vario.bins/10.0, vario.experimental,label=simulation)\n",
    "            ax = plt.gca()\n",
    "            #bp = ax.boxplot(vario_experimental_array)\n",
    "            bp = sns.boxplot(data=df)\n",
    "            ax.set_xticklabels([int(val) for val in average_bin_values])\n",
    "            for ind, label in enumerate(bp.get_xticklabels()):\n",
    "                if ind % 8 == 0:  # every 10th label is kept\n",
    "                    label.set_visible(True)\n",
    "                else:\n",
    "                    label.set_visible(False)\n",
    "            #print('bin values are: ',average_bin_values)\n",
    "            #print('x-ticks are: ',average_bin_values[::8])\n",
    "            #ax.xaxis.set_major_locator(ticker.MultipleLocator(8))\n",
    "            #ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "            #ax.set_xticklabels([int(val) for val in average_bin_values[::8]])\n",
    "            plt.title('Box plot of variograms for '+simulation+':'+str(nsamples)+' samples of size '+str(sample_size)+' each for\\n surface '+var)\n",
    "            plt.xlabel('distance (km)')\n",
    "            plt.ylabel(ylabels[ii])\n",
    "            #plt.legend(loc='upper right')\n",
    "            # l, b, h, w = .15, .72, .1, .2\n",
    "            # axins = fig.add_axes([l, b, w, h])\n",
    "            # axins.hist(output_variogram.distance/10.0,bins=nbins) \n",
    "            # axins.set_yticks([])\n",
    "            # axins.set_xticks([50,150,250],labels=['50','150','250'])\n",
    "            # axins.set_xticklabels(axins.get_xticklabels(), rotation=0, fontsize=8)\n",
    "            # axins.set_xlabel('distance (km)',fontsize=8)\n",
    "            # axins.set_title('#pairs by distance',fontsize=8)\n",
    "            filename = 'box_plot_experimental_variogram_'+simulation+'_'+var+'_'+str(nsamples)+'_samples_d03_'+area_type+'_points.png'\n",
    "            print('    saving to file: ',filename)\n",
    "            plt.savefig(filename,dpi=150)\n",
    "            print('\\n\\n')\n",
    "    return\n",
    "\n",
    "sims=['AUS1.1-R','DRC1.1-R']#,'PHI1.1-R','PHI2.1-R','WPO1.1-R','BRA1.1-R','USA1.1-R','RSA1.1-R']\n",
    "varnames = ['THETA','RV','UP','VP','WSPD']\n",
    "ylabs=['$\\Theta^{2} (K^{2})$','$RV^{2} (kg^{2}kg^{-2})$','$u^{2} (m^{2}s^{-2})$','$v^{2} (m^{2}s^{-2})$',\\\n",
    "        '$WSPD^{2} (m^{2}s^{-2})$']\n",
    "\n",
    "create_box_plot_multiple_variograms(sims,varnames,ylabs,20,10000,200,area_type='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e709078-5acc-436d-b064-43937e111027",
   "metadata": {},
   "source": [
    "## Regular Variograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae59aa-3e11-46f9-86da-2dcf22f72b01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conceptual demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15969927-962b-4440-98d8-374fe8ce8ae5",
   "metadata": {},
   "source": [
    "#### Create dummy data with circular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0b9d2-3d5e-4137-8e96-b9226e69872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import variogram_helper_functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "\n",
    "def create_two_circles_data(DOMAIN_SIZE, RADIUS1, RADIUS2, VALUE1, VALUE2, CENTER1, CENTER2, ADD_GAUSSIAN_NOISE=True, CIRCLE_VARIATION=None):\n",
    "    #temp_data = np.random.rand(DOMAIN_SIZE, DOMAIN_SIZE)*2  # Example random temperature field data\n",
    "    array = np.zeros((DOMAIN_SIZE, DOMAIN_SIZE))\n",
    "\n",
    "    # Define the center and radius of the circle\n",
    "    center_x1 = CENTER1[0]\n",
    "    center_y1 = CENTER1[1]\n",
    "    radius1   = RADIUS1\n",
    "\n",
    "    center_x2 = CENTER2[0]\n",
    "    center_y2 = CENTER2[1]\n",
    "    radius2   = RADIUS2\n",
    "\n",
    "    # Generate indices for the circle\n",
    "    y_indices, x_indices = np.ogrid[:DOMAIN_SIZE, :DOMAIN_SIZE]\n",
    "    # Calculate the distance from each point to the center\n",
    "    distances1 = np.sqrt((x_indices - center_x1)**2 + (y_indices - center_y1)**2)\n",
    "    distances2 = np.sqrt((x_indices - center_x2)**2 + (y_indices - center_y2)**2)\n",
    "   \n",
    "   # Set the values of the array based on the variation type\n",
    "    if CIRCLE_VARIATION == 'linear':\n",
    "        array[distances1 <= RADIUS1] = VALUE1*(1.0 - (distances1[distances1 <= RADIUS1] / RADIUS1))\n",
    "        array[distances2 <= RADIUS2] = VALUE2*(1.0 - (distances2[distances2 <= RADIUS2] / RADIUS2))\n",
    "    elif CIRCLE_VARIATION == 'sinusoidal':\n",
    "        # Sinusoidal variation from the center to the edge of the circles\n",
    "        array[distances1 <= RADIUS1] = VALUE1 * np.sin(np.pi * distances1[distances1 <= RADIUS1] / RADIUS1)\n",
    "        array[distances2 <= RADIUS2] = VALUE2 * np.sin(np.pi * distances2[distances2 <= RADIUS2] / RADIUS2)\n",
    "    elif CIRCLE_VARIATION == 'gaussian':\n",
    "        # Gaussian variation from the center to the edge of the circles\n",
    "        array[distances1 <= RADIUS1] = VALUE1 * np.exp(-(distances1[distances1 <= RADIUS1]**2) / (2 * RADIUS1**2))\n",
    "        array[distances2 <= RADIUS2] = VALUE2 * np.exp(-(distances2[distances2 <= RADIUS2]**2) / (2 * RADIUS2**2))\n",
    "    else:\n",
    "        # Set the values inside the circle to 1\n",
    "        array[distances1 <= RADIUS1] = VALUE1\n",
    "        array[distances2 <= RADIUS2] = VALUE2\n",
    "        #temp_data = temp_data  + array\n",
    "    \n",
    "    if ADD_GAUSSIAN_NOISE:\n",
    "        print('Adding Gaussian noise to the array...')\n",
    "        noise_mean = 3\n",
    "        noise_std = 3.0\n",
    "        noise = np.random.normal(noise_mean, noise_std, size=array.shape)\n",
    "        #print(noise)\n",
    "        array = array + noise\n",
    "    \n",
    "    title_string = 'r1 = '+str(RADIUS1)+', r2 = '+str(RADIUS2)+'\\nvalue1 = '+str(VALUE1)+', value2 = '+str(VALUE2)    \n",
    "    corner_distances1 = []\n",
    "    corner_distances2 = []\n",
    "    corner_distances1.append(RADIUS1)\n",
    "    corner_distances2.append(RADIUS2)\n",
    "    for corner_x in [0, DOMAIN_SIZE-1]:\n",
    "        for corner_y in [0, DOMAIN_SIZE-1]:\n",
    "            distance = np.sqrt((CENTER1[0] - corner_x)**2 + (CENTER1[1] - corner_y)**2)\n",
    "            corner_distances1.append(distance)\n",
    "            \n",
    "    for corner_x in [0, DOMAIN_SIZE-1]:\n",
    "        for corner_y in [0, DOMAIN_SIZE-1]:\n",
    "            distance = np.sqrt((CENTER2[0] - corner_x)**2 + (CENTER2[1] - corner_y)**2)\n",
    "            corner_distances2.append(distance)\n",
    "\n",
    "    #print('corner distances for circle 1 ',corner_distances1)\n",
    "    #print('corner distances for circle 2 ',corner_distances2)\n",
    "    #plt.savefig('circle_test5.png',dpi=150)\n",
    "    return array, title_string\n",
    "\n",
    "def plot_variogram_toy(TEMP_DATA,SAMPLE_SIZE,BIN_FUNCTION):\n",
    "    y_dim, x_dim     = np.shape(TEMP_DATA)\n",
    "    coords = variogram_helper_functions.produce_random_coords(x_dim,y_dim,SAMPLE_SIZE)                           \n",
    "    nonnan_coords, nonnan_values = variogram_helper_functions.get_values_at_random_coords(TEMP_DATA, coords)\n",
    "    max_lag = np.sqrt(x_dim**2 + y_dim**2)/2.0\n",
    "    V, x1, y1, _ = variogram_helper_functions.make_variogram(nonnan_coords, nonnan_values, 50, max_lag, DX = 1.0, BIN_FUNCTION=BIN_FUNCTION)\n",
    "    return V, x1, y1\n",
    "\n",
    "def plot_conditional_variogram_toy(TEMP_DATA,SAMPLE_SIZE,CONDITION,BIN_FUNCTION):\n",
    "    y_dim, x_dim     = np.shape(TEMP_DATA)\n",
    "    coords = variogram_helper_functions.produce_random_coords_conditional(SAMPLE_SIZE,TEMP_DATA,CONDITION)   \n",
    "    nonnan_coords, nonnan_values = variogram_helper_functions.get_values_at_random_coords(TEMP_DATA, coords)\n",
    "    max_lag = np.sqrt(x_dim**2 + y_dim**2)/2.0\n",
    "    V, x1, y1, _ = variogram_helper_functions.make_variogram(nonnan_coords, nonnan_values, 50, max_lag, DX = 1.0, BIN_FUNCTION=BIN_FUNCTION)\n",
    "    return V, x1, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c28fd6b-dc65-4167-881f-7dbe2f2b8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "ii = 0 \n",
    "#for variation in [None,'linear','sinusoidal','gaussian']:\n",
    "#     for rad2 in [5,20]:\n",
    "#         for val2 in [0,25]:\n",
    "            \n",
    "for variation in [None,'linear','sinusoidal','gaussian']:\n",
    "    for rad2 in [5,20]:\n",
    "        for val2 in [25]:\n",
    "            for lag_class_method in ['even','uniform']:\n",
    "                ii = ii + 1\n",
    "                temperature_data, title_string = create_two_circles_data(100,20,rad2,25,val2,[40,30],[70,80],True,variation)\n",
    "                print('created dummy data for test ',ii,'\\n')\n",
    "                fig   = plt.figure()\n",
    "                V, bins1, vario1  = plot_variogram_toy(temperature_data,5000,lag_class_method)\n",
    "                bins_histogram1, counts_histogram1, widths_histogram1 = variogram_helper_functions.retrieve_histogram(V)\n",
    "                V, bins2, vario2  = plot_conditional_variogram_toy(temperature_data,5000,lambda x: x > 10.0,lag_class_method)\n",
    "                bins_histogram2, counts_histogram2, widths_histogram2 = variogram_helper_functions.retrieve_histogram(V)\n",
    "\n",
    "\n",
    "                fig, axes    = plt.subplots(1,3,figsize=(14,5))\n",
    "                cont         = axes[0].contourf(temperature_data,levels=np.arange(np.min(temperature_data),np.max(temperature_data)+1,1),extend='both')\n",
    "                if variation==None:\n",
    "                    variation_title = 'constant'\n",
    "                else:\n",
    "                    variation_title = variation\n",
    "                axes[0].set_title('2D field\\n'+title_string+' :'+variation_title)\n",
    "                axes[0].set_xlabel('x-direction (units)')\n",
    "                axes[0].set_ylabel('y-direction (units)')\n",
    "                axes[0].set_aspect('equal', 'box')\n",
    "                fig.colorbar(cont, ax=axes[0])\n",
    "\n",
    "                #formatter = plt.FuncFormatter(lambda x, _: '{:.0e}'.format(x))\n",
    "                formatter = mticker.ScalarFormatter(useMathText=True)\n",
    "                formatter.set_powerlimits((-2,2))\n",
    "\n",
    "                ax_twin1 = axes[1].twinx()\n",
    "                ax_twin1.yaxis.set_major_formatter(formatter)\n",
    "                #ax_twin1.yaxis.offsetText.set_visible(False)\n",
    "                #offset = ax_twin1.yaxis.get_major_formatter().get_offset()\n",
    "                #ax_twin1.get_yaxis().get_major_formatter().set_scientific(True)\n",
    "                ax_twin1.bar(bins_histogram1, counts_histogram1, width = widths_histogram1, color='yellowgreen', alpha=0.3)\n",
    "                ax_twin1.set_ylabel('# pairwise observations')\n",
    "\n",
    "                axes[1].plot(bins1, vario1, label='Variogram Curve',color='k')\n",
    "                print('plotting variogram for for test ',ii)\n",
    "                axes[1].set_title('Variogram (all points)')\n",
    "                axes[1].set_xlabel('Spatial lag (units)')\n",
    "                axes[1].set_ylabel('Semi-variogram $(arbitrary-unit^{2})$')\n",
    "\n",
    "                ax_twin2 = axes[2].twinx()\n",
    "                ax_twin2.yaxis.set_major_formatter(formatter)\n",
    "                #ax_twin2.yaxis.offsetText.set_visible(False)\n",
    "                #offset = ax_twin2.yaxis.get_major_formatter().get_offset()\n",
    "                #ax_twin2.get_yaxis().get_major_formatter().set_scientific(True)\n",
    "                ax_twin2.bar(bins_histogram2, counts_histogram2, width = widths_histogram2, color='yellowgreen', alpha=0.3)\n",
    "                ax_twin2.set_ylabel('# pairwise observations')\n",
    "\n",
    "                axes[2].plot(bins2, vario2, label='Variogram Curve',color='k')\n",
    "                print('plotting the conditional variogram for for test ',ii)\n",
    "                axes[2].set_title('Variogram only for \\npoints with values > 10')\n",
    "                axes[2].set_xlabel('Spatial lag (units)')\n",
    "                axes[2].set_ylabel('Semi-variogram $(arbitrary-unit^{2})$')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                pngname = 'vario_test_'+str(ii)+'.png'\n",
    "                print('saving to file: ',pngname)\n",
    "                plt.savefig(pngname,dpi=150)\n",
    "                print('============\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dce053-ef90-4678-a16d-0fa4f75a618c",
   "metadata": {},
   "source": [
    "#### Dummy data with fractal features (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbca49d-bac3-4da9-a870-d82cfd284fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib as mpl\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "\n",
    "def fractal_iteration(z, max_iter):\n",
    "    for i in range(max_iter):\n",
    "        z = z ** 4 - (2 + 3j) / 6\n",
    "        if abs(z) > 2:\n",
    "            return i\n",
    "    return max_iter\n",
    "\n",
    "def generate_cloud_fractal(x_min, x_max, y_min, y_max, width, height, max_iter):\n",
    "    fractal = np.zeros((width, height))\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            real = x_min + (x / width) * (x_max - x_min)\n",
    "            imag = y_min + (y / height) * (y_max - y_min)\n",
    "            z = complex(real, imag)\n",
    "            fractal[x, y] = fractal_iteration(z, max_iter)\n",
    "\n",
    "\n",
    "    return gaussian_filter(fractal**2, sigma=0.01)\n",
    "    #return fractal#**2\n",
    "\n",
    "# Define the parameters\n",
    "\n",
    "def create_fractal_main():\n",
    "    x_min, x_max = -1, 1\n",
    "    y_min, y_max = -1, 1\n",
    "    width, height = 2000, 2000\n",
    "    max_iter = 100\n",
    "\n",
    "    # Generate the fractal\n",
    "    temperature_data = generate_cloud_fractal(x_min, x_max, y_min, y_max, width, height, max_iter)\n",
    "\n",
    "    print('max value of the image is ',np.max(temperature_data))\n",
    "    print('shape of temperature_data is',np.shape(temperature_data))\n",
    "    #cmap = mpl.cm.Blues_r(np.linspace(0,1,20))\n",
    "    #cmap = mpl.colors.ListedColormap(cmap[7:,:-1])\n",
    "\n",
    "\n",
    "    #fig  = plt.figure(figsize = (8,8))\n",
    "    cropped_array = temperature_data[height*3//4:, :width//4]\n",
    "    print('shape of cropped array is ',np.shape(cropped_array))\n",
    "    #plt.contourf(cropped_array,levels = np.arange(np.min(temperature_data),np.max(temperature_data)+10,10),cmap=cmap, extend='both') #extent=[x_min, x_max, y_min, y_max],\n",
    "    #levels = np.arange(np.min(temperature_data),np.max(temperature_data)+1,1)\n",
    "    #plt.colorbar()\n",
    "    #plt.imshow(fractal.T, cmap='PuBu')\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.show()\n",
    "    \n",
    "    return cropped_array\n",
    "    \n",
    "    \n",
    "temperature_data = create_fractal_main()\n",
    "\n",
    "V, bins1, vario1  = plot_variogram_toy(temperature_data,5000,'uniform')\n",
    "bins_histogram1, counts_histogram1, widths_histogram1 = variogram_helper_functions.retrieve_histogram(V)\n",
    "\n",
    "fig, axes    = plt.subplots(1,2,figsize=(9.5,5))\n",
    "cont         = axes[0].contourf(temperature_data,levels=np.arange(np.min(temperature_data),np.max(temperature_data)+1,1),extend='both')\n",
    "if variation==None:\n",
    "    variation_title = 'constant'\n",
    "else:\n",
    "    variation_title = variation\n",
    "axes[0].set_title('2D field\\n'+title_string+' :'+variation_title)\n",
    "axes[0].set_xlabel('x-direction (units)')\n",
    "axes[0].set_ylabel('y-direction (units)')\n",
    "axes[0].set_aspect('equal', 'box')\n",
    "fig.colorbar(cont, ax=axes[0])\n",
    "\n",
    "#formatter = plt.FuncFormatter(lambda x, _: '{:.0e}'.format(x))\n",
    "formatter = mticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_powerlimits((-2,2))\n",
    "\n",
    "ax_twin1 = axes[1].twinx()\n",
    "ax_twin1.yaxis.set_major_formatter(formatter)\n",
    "ax_twin1.bar(bins_histogram1, counts_histogram1, width = widths_histogram1, color='yellowgreen', alpha=0.3)\n",
    "ax_twin1.set_ylabel('# pairwise observations')\n",
    "\n",
    "axes[1].plot(bins1, vario1, label='Variogram Curve',color='k')\n",
    "axes[1].set_title('Variogram (all points)')\n",
    "axes[1].set_xlabel('Spatial lag (units)')\n",
    "axes[1].set_ylabel('Semi-variogram $(arbitrary-unit^{2})$')\n",
    "\n",
    "plt.tight_layout()\n",
    "#pngname = 'vario_test_'+str(ii)+'.png'\n",
    "#print('saving to file: ',pngname)\n",
    "#plt.savefig(pngname,dpi=150)\n",
    "print('============\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4ca9f5-9907-4396-afee-a1efa61d4995",
   "metadata": {},
   "source": [
    "#### Dummy data with fractal features (B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b31cc-493e-451a-b798-b4ab4664c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mandelbrot(n_rows, n_columns, iterations, cx, cy):\n",
    "    x_cor = np.linspace(-2, 2, n_rows)\n",
    "    y_cor = np.linspace(-2, 2, n_columns)\n",
    "    x_len = len(x_cor)\n",
    "    y_len = len(y_cor)\n",
    "    output = np.zeros((x_len,y_len))\n",
    "    c = complex(cx, cy)\n",
    "    for i in range(x_len):\n",
    "        for j in range(y_len):\n",
    "            z = complex(x_cor[i], y_cor[j])\n",
    "            count = 0\n",
    "            for k in range(iterations):\n",
    "                z = (z * z) + c\n",
    "                count = count + 1\n",
    "                if (abs(z) > 4):\n",
    "                    break\n",
    "            output[i,j] = count\n",
    "        #print(int((i/x_len)*100),\"% completed\")\n",
    "\n",
    "    #print(output)\n",
    "    print(np.min(output),np.max(output))\n",
    "    #fig  = plt.figure(figsize = (7,7))\n",
    "    #cont = plt.contourf(output, levels=np.arange(np.min(output),np.max(output)+1,1),cmap='hot',extend='all')\n",
    "    #plt.colorbar(cont)\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.show()\n",
    "    return output\n",
    "    \n",
    "#mandelbrot(1000,1000,150,-0.42,0.6)\n",
    "#mandelbrot(1000,1000,150,-1.9,0.9)\n",
    "temperature_data = mandelbrot(200,200,150,-2.19,0.5)\n",
    "\n",
    "V, bins1, vario1  = plot_variogram_toy(temperature_data,10000,'uniform')\n",
    "bins_histogram1, counts_histogram1, widths_histogram1 = variogram_helper_functions.retrieve_histogram(V)\n",
    "print('-------\\n')\n",
    "V, bins2, vario2  = plot_conditional_variogram_toy(temperature_data,10000,lambda x: x > 3.0,'uniform')\n",
    "bins_histogram2, counts_histogram2, widths_histogram2 = variogram_helper_functions.retrieve_histogram(V)\n",
    "\n",
    "fig, axes    = plt.subplots(1,3,figsize=(14,5))\n",
    "cont         = axes[0].contourf(temperature_data,levels=np.arange(np.min(temperature_data),np.max(temperature_data)+0.5,0.5),extend='both')\n",
    "\n",
    "axes[0].set_title('2D fractal field')\n",
    "axes[0].set_xlabel('x-direction (units)')\n",
    "axes[0].set_ylabel('y-direction (units)')\n",
    "axes[0].set_aspect('equal', 'box')\n",
    "fig.colorbar(cont, ax=axes[0])\n",
    "\n",
    "#formatter = plt.FuncFormatter(lambda x, _: '{:.0e}'.format(x))\n",
    "formatter = mticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_powerlimits((-2,2))\n",
    "\n",
    "ax_twin1 = axes[1].twinx()\n",
    "ax_twin1.yaxis.set_major_formatter(formatter)\n",
    "ax_twin1.bar(bins_histogram1, counts_histogram1, width = widths_histogram1, color='yellowgreen', alpha=0.3)\n",
    "ax_twin1.set_ylabel('# pairwise observations')\n",
    "\n",
    "axes[1].plot(bins1, vario1, label='Variogram Curve',color='k')\n",
    "axes[1].set_title('Variogram (all points)')\n",
    "axes[1].set_xlabel('Spatial lag (units)')\n",
    "axes[1].set_ylabel('Semi-variogram $(arbitrary-unit^{2})$')\n",
    "\n",
    "\n",
    "ax_twin2 = axes[2].twinx()\n",
    "ax_twin2.yaxis.set_major_formatter(formatter)\n",
    "#ax_twin2.yaxis.offsetText.set_visible(False)\n",
    "#offset = ax_twin2.yaxis.get_major_formatter().get_offset()\n",
    "#ax_twin2.get_yaxis().get_major_formatter().set_scientific(True)\n",
    "ax_twin2.bar(bins_histogram2, counts_histogram2, width = widths_histogram2, color='yellowgreen', alpha=0.3)\n",
    "ax_twin2.set_ylabel('# pairwise observations')\n",
    "\n",
    "axes[2].plot(bins2, vario2, label='Variogram Curve',color='k')\n",
    "axes[2].set_title('Variogram only for \\npoints with values > 3')\n",
    "axes[2].set_xlabel('Spatial lag (units)')\n",
    "axes[2].set_ylabel('Semi-variogram $(arbitrary-unit^{2})$')\n",
    "\n",
    "plt.tight_layout()\n",
    "pngname = 'mandelbrot_variogram_uniform_classes.png'\n",
    "print('saving to file: ',pngname)\n",
    "plt.savefig(pngname,dpi=150)\n",
    "print('============\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e606b5-d353-4349-8754-968f5fdf83bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mandelbrot(n_rows, n_columns, iterations, cx, cy):\n",
    "    x_cor = np.linspace(-2, 2, n_rows)\n",
    "    y_cor = np.linspace(-2, 2, n_columns)\n",
    "    x_len = len(x_cor)\n",
    "    y_len = len(y_cor)\n",
    "    output = np.zeros((x_len,y_len))\n",
    "    c = complex(cx, cy)\n",
    "    for i in range(x_len):\n",
    "        for j in range(y_len):\n",
    "            z = complex(x_cor[i], y_cor[j])\n",
    "            count = 0\n",
    "            for k in range(iterations):\n",
    "                z = (z * z) + c\n",
    "                count = count + 1\n",
    "                if (abs(z) > 4):\n",
    "                    break\n",
    "            output[i,j] = count\n",
    "        #print(int((i/x_len)*100),\"% completed\")\n",
    "\n",
    "    #print(output)\n",
    "    print(np.min(output),np.max(output))\n",
    "    #fig  = plt.figure(figsize = (7,7))\n",
    "    #cont = plt.contourf(output, levels=np.arange(np.min(output),np.max(output)+1,1),cmap='hot',extend='all')\n",
    "    #plt.colorbar(cont)\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.show()\n",
    "    return output\n",
    "    \n",
    "#mandelbrot(1000,1000,150,-0.42,0.6)\n",
    "#mandelbrot(1000,1000,150,-1.9,0.9)\n",
    "temperature_data = mandelbrot(200,200,150,-2.19,0.5)\n",
    "\n",
    "V, bins1, vario1  = plot_variogram_toy(temperature_data,10000,'even')\n",
    "bins_histogram1, counts_histogram1, widths_histogram1 = variogram_helper_functions.retrieve_histogram(V)\n",
    "print('-------\\n')\n",
    "V, bins2, vario2  = plot_conditional_variogram_toy(temperature_data,10000,lambda x: x > 3.0,'even')\n",
    "bins_histogram2, counts_histogram2, widths_histogram2 = variogram_helper_functions.retrieve_histogram(V)\n",
    "\n",
    "fig, axes    = plt.subplots(1,3,figsize=(14,5))\n",
    "cont         = axes[0].contourf(temperature_data,levels=np.arange(np.min(temperature_data),np.max(temperature_data)+0.5,0.5),extend='both')\n",
    "\n",
    "axes[0].set_title('2D fractal field')\n",
    "axes[0].set_xlabel('x-direction (units)')\n",
    "axes[0].set_ylabel('y-direction (units)')\n",
    "axes[0].set_aspect('equal', 'box')\n",
    "fig.colorbar(cont, ax=axes[0])\n",
    "\n",
    "#formatter = plt.FuncFormatter(lambda x, _: '{:.0e}'.format(x))\n",
    "formatter = mticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_powerlimits((-2,2))\n",
    "\n",
    "ax_twin1 = axes[1].twinx()\n",
    "ax_twin1.yaxis.set_major_formatter(formatter)\n",
    "ax_twin1.bar(bins_histogram1, counts_histogram1, width = widths_histogram1, color='yellowgreen', alpha=0.3)\n",
    "ax_twin1.set_ylabel('# pairwise observations')\n",
    "\n",
    "axes[1].plot(bins1, vario1, label='Variogram Curve',color='k')\n",
    "axes[1].set_title('Variogram (all points)')\n",
    "axes[1].set_xlabel('Spatial lag (units)')\n",
    "axes[1].set_ylabel('Semi-variogram $(arbitrary-unit^{2})$')\n",
    "\n",
    "\n",
    "ax_twin2 = axes[2].twinx()\n",
    "ax_twin2.yaxis.set_major_formatter(formatter)\n",
    "#ax_twin2.yaxis.offsetText.set_visible(False)\n",
    "#offset = ax_twin2.yaxis.get_major_formatter().get_offset()\n",
    "#ax_twin2.get_yaxis().get_major_formatter().set_scientific(True)\n",
    "ax_twin2.bar(bins_histogram2, counts_histogram2, width = widths_histogram2, color='yellowgreen', alpha=0.3)\n",
    "ax_twin2.set_ylabel('# pairwise observations')\n",
    "\n",
    "axes[2].plot(bins2, vario2, label='Variogram Curve',color='k')\n",
    "axes[2].set_title('Variogram only for \\npoints with values > 3')\n",
    "axes[2].set_xlabel('Spatial lag (units)')\n",
    "axes[2].set_ylabel('Semi-variogram $(arbitrary-unit^{2})$')\n",
    "\n",
    "plt.tight_layout()\n",
    "pngname = 'mandelbrot_variogram_even_classes.png'\n",
    "print('saving to file: ',pngname)\n",
    "plt.savefig(pngname,dpi=150)\n",
    "print('============\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8befd-3d6e-46bd-8798-f330520cc75f",
   "metadata": {},
   "source": [
    "#### Plot Z-score from Moran's I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa336ea-1c9c-4c19-9526-5582438734b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from libpysal.weights.distance import DistanceBand\n",
    "import libpysal \n",
    "from esda.moran import Moran\n",
    "import variogram_helper_functions\n",
    "\n",
    "def compute_moran(DISTANCE_INTERVAL, COORDS, VALUES):\n",
    "    # Create binary spatial weights matrix based on distance interval\n",
    "    w = libpysal.weights.DistanceBand(COORDS, threshold=DISTANCE_INTERVAL, binary=True, silence_warnings=True)\n",
    "    # Compute Moran's I\n",
    "    moran = Moran(VALUES, w)\n",
    "    return DISTANCE_INTERVAL, moran.I, moran.EI, moran.VI_norm, moran.p_norm, moran.z_norm\n",
    "\n",
    "def plot_z_score_moran(DATA,NSAMPLES,DIST_INTERVALS):\n",
    "    y_dim, x_dim     = np.shape(DATA)\n",
    "    sample_coords_tuple = variogram_helper_functions.produce_random_coords(x_dim, y_dim, NSAMPLES, 'tuple')\n",
    "    nonnan_coords, nonnan_values = variogram_helper_functions.get_values_at_random_coords(DATA, sample_coords_tuple, 'tuple')\n",
    "    print('first 10 coords: ',nonnan_coords[0:10])\n",
    "    print('first 10 values: ',nonnan_values[0:10])\n",
    "\n",
    "    # Parallel computation of Moran's I for each distance interval\n",
    "    print('starting multiprocessing...')\n",
    "    with Pool(18) as pool:\n",
    "        moran_results = pool.starmap(compute_moran, [(DIST, nonnan_coords, nonnan_values) for DIST in DIST_INTERVALS])\n",
    "\n",
    "    # Unpack results\n",
    "    dist_values, moran_i_values, moran_ei_values, moran_vi_values, moran_p_values, moran_z_values = zip(*moran_results)\n",
    "\n",
    "    #print('# distances: ',len(distance_intervals))\n",
    "    #print('# moran_vals: ',len(moran_i_values))\n",
    "    # Plot Moran's I versus distance\n",
    "    #     fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n",
    "    #     ax = axes.flatten()\n",
    "    #     ax[0].plot(distance_intervals, moran_i_values, marker='o')\n",
    "    #     ax[0].set_xlabel('Spatial lag (km)')\n",
    "    #     ax[0].set_ylabel(\"Moran's I\")\n",
    "    #     ax[0].set_title(\"Moran's I vs Distance\")\n",
    "    #     ax[0].grid()\n",
    "\n",
    "    #     ax[1].plot(distance_intervals, moran_ei_values, marker='o')\n",
    "    #     ax[1].set_xlabel('Spatial lag (km)')\n",
    "    #     ax[1].set_ylabel(\"Moran EI\")\n",
    "    #     ax[1].set_title(\"Moran EI vs Distance\")\n",
    "    #     ax[1].grid()\n",
    "\n",
    "    #     ax[2].plot(distance_intervals, moran_p_values, marker='o')\n",
    "    #     ax[2].set_xlabel('Spatial lag (km)')\n",
    "    #     ax[2].set_ylabel(\"Moran p_norm\")\n",
    "    #     ax[2].set_title(\"Moran's p_norm vs Distance\")\n",
    "    #     ax[2].grid()\n",
    "\n",
    "    #     ax[3].plot(distance_intervals, moran_z_values, marker='o')\n",
    "    #     ax[3].set_xlabel('Spatial lag (km)')\n",
    "    #     ax[3].set_ylabel(\"Moran z_norm\")\n",
    "    #     ax[3].set_title(\"Moran z_norm vs Distance\")\n",
    "    #     ax[3].grid()\n",
    "\n",
    "    #     #plt.savefig('moran_testing.png', dpi=200)\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "    \n",
    "    df = pd.DataFrame(moran_results, columns=['distance', 'moran_I', 'moran_EI', 'moran_VI_norm', 'moran_p_norm','moran_z_norm'])\n",
    "    print(df)\n",
    "    #fig, axes = plt.subplots(1,1,figsize=(8, 8))\n",
    "    plt.plot(df.distance,df.moran_z_norm)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel('Spatial lag (km)')\n",
    "    ax.set_ylabel(\"Moran z_norm\")\n",
    "    ax.set_title(\"Moran z_norm vs Distance\")\n",
    "    plt.savefig('moran_z_norm_test5.png',dpi=150)\n",
    "    return df\n",
    "\n",
    "distance_intervals = np.arange(1, 100, 1)  \n",
    "plot_z_score_moran(temperature_data,5000,distance_intervals)\n",
    "\n",
    "# Convert the list of tuples to a pandas DataFrame\n",
    "#df = pd.DataFrame(moran_results, columns=['distance','moran_I', 'moran_EI', 'moran_VI_norm', 'moran_p_norm','moran_z_norm'])\n",
    "#plt.plot(distance_intervals,df.moran_z_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a33c0c-07b8-4987-8a9a-a406b8a99126",
   "metadata": {},
   "source": [
    "#### Plot PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468fc36-9dbd-4dbd-aaaa-401860652e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "def plot_PSD_np_array(DATA):\n",
    "    fig = plt.figure(figsize=(9,9))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    z = DATA - np.mean(DATA)\n",
    "    y_dim,x_dim = np.shape(z)\n",
    "    dx=1.0\n",
    "\n",
    "    print('        cropping the array')\n",
    "    height, width = z.shape\n",
    "    print('        original height of image: ',height)\n",
    "    print('        original width of image: ',width)\n",
    "    # Calculate the size of the square\n",
    "    size = min(width, height)\n",
    "\n",
    "    # Calculate the coordinates for cropping\n",
    "    left = (width - size) // 2\n",
    "    top = (height - size) // 2\n",
    "    right = (width + size) // 2\n",
    "    bottom = (height + size) // 2\n",
    "\n",
    "    # Crop the image using NumPy slicing\n",
    "    cropped_img_array = z[top:bottom, left:right]\n",
    "\n",
    "    # fourier spectrum part\n",
    "    image = cropped_img_array\n",
    "    print('        shape of input image: ',np.shape(image))\n",
    "    npix = image.shape[0]\n",
    "\n",
    "    fourier_image = np.fft.fftn(image)\n",
    "    fourier_amplitudes = np.abs(fourier_image)**2\n",
    "\n",
    "    kfreq = np.fft.fftfreq(npix) * npix\n",
    "    print('size of wavenumbers from np.fft.fftfreq: ',len(np.fft.fftfreq(npix)))\n",
    "    print('wavenumbers from np.fft.fftfreq: ',np.fft.fftfreq(npix))\n",
    "    print('min/max of wavenumbers from np.fft.fftfreq: ',min(np.fft.fftfreq(npix)),'/',max(np.fft.fftfreq(npix)))\n",
    "    print('size of wavenumbers from np.fft.fftfreq*npix: ',len(kfreq))\n",
    "    print('wavenumbers from kfreq: ',kfreq)\n",
    "    print('min/max of wavenumbers from kfreq: ',min(kfreq),'/',max(kfreq))\n",
    "    kfreq2D = np.meshgrid(kfreq, kfreq)\n",
    "    knrm = np.sqrt(kfreq2D[0]**2 + kfreq2D[1]**2)\n",
    "    #print('shape of knrm: ',np.shape(knrm))\n",
    "    #print('shape of fourier_amplitudes: ',np.shape(fourier_amplitudes))\n",
    "    knrm = knrm.flatten()\n",
    "    fourier_amplitudes = fourier_amplitudes.flatten()\n",
    "    #print('shape of knrm: ',np.shape(knrm))\n",
    "    #print('shape of fourier_amplitudes: ',np.shape(fourier_amplitudes))\n",
    "    kbins = np.arange(0.5, npix//2+1, 1.)\n",
    "    #print(kbins)\n",
    "    kvals = 0.5 * (kbins[1:] + kbins[:-1])\n",
    "    Abins, _, _ = stats.binned_statistic(knrm, fourier_amplitudes,\n",
    "                                         statistic = \"mean\",\n",
    "                                         bins = kbins)\n",
    "    Abins *= np.pi * (kbins[1:]**2 - kbins[:-1]**2)\n",
    "    wavelengths = npix*dx/kvals\n",
    "\n",
    "    ax1.semilogy(wavelengths, Abins)#  , color=color_dict[simulation], label=simulation)\n",
    "    ax1.set_xscale(\"log\")\n",
    "    ax1.invert_xaxis()  # Invert the x-axis\n",
    "    ax1.set_ylabel(r'power spectral density (arbitrary units)')# $(m^{3} s^{-2})$')\n",
    "    ax1.set_xlabel(r'wavelength (km)')\n",
    "    plt.legend(loc=('upper right'))\n",
    "    \n",
    "plot_PSD_np_array(temperature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2aa6c-372b-41bc-a4d3-04b8c0af1eed",
   "metadata": {},
   "source": [
    "### Drawing one sample for multiple variables for multiple simulations and save as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db066b9f-a719-4e3d-8d1f-c73a94632aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile save_variogram_multiprocessing.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as crs\n",
    "import random\n",
    "import skgstat as skg\n",
    "plt.style.use('ggplot')\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import read_vars_WRF_RAMS\n",
    "import variogram_helper_functions\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"font.family\"] = \"Roboto\"\n",
    "matplotlib.rcParams[\"font.sans-serif\"] = [\"Roboto\"]  # For non-unicode text\n",
    "#matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 18\n",
    "matplotlib.rcParams['axes.titlesize'] = 18\n",
    "matplotlib.rcParams['xtick.labelsize'] = 18\n",
    "matplotlib.rcParams['ytick.labelsize'] = 18\n",
    "matplotlib.rcParams['legend.fontsize'] = 18\n",
    "#matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    " \n",
    "def save_variogram(WHICH_TIME, VARIABLE, SIMULATION, CONDITION_INFO, SAMPLE_SIZE, DOMAIN, SAVE_CSV):\n",
    "    print('working on domain' ,domain)\n",
    "    print('working on ',VARIABLE)\n",
    "    print('working on simulation: ',SIMULATION)\n",
    "    \n",
    "    # provide grid spacings in km to multiply with the variogram bin distances\n",
    "    if DOMAIN=='1':\n",
    "        dx = 1.6 # km\n",
    "    if DOMAIN=='2':\n",
    "        dx=0.4   # km\n",
    "    if DOMAIN=='3':\n",
    "        dx=0.1   # km\n",
    "        \n",
    "    \n",
    "    # which model are we working on; need this for the read_RAMS_WRF_data_file\n",
    "    if SIMULATION[7]=='W':\n",
    "        model_name = 'WRF'\n",
    "        microphysics_scheme = SIMULATION[8]\n",
    "    elif SIMULATION[7]=='R':\n",
    "        model_name = 'RAMS'\n",
    "    else:\n",
    "        print('!!!!!issues with identifying model_name!!!!!')\n",
    "\n",
    "    print('        model name: ',model_name)\n",
    "\n",
    "    # grab the file needed\n",
    "    if model_name=='RAMS':   \n",
    "        selected_fil = variogram_helper_functions.find_RAMS_file(SIMULATION=SIMULATION,DOMAIN=DOMAIN,WHICH_TIME=WHICH_TIME)\n",
    "\n",
    "    if model_name=='WRF':\n",
    "        selected_fil =  variogram_helper_functions.find_WRF_file(SIMULATION=SIMULATION,DOMAIN=DOMAIN,WHICH_TIME=WHICH_TIME)\n",
    "\n",
    "    timestring = variogram_helper_functions.get_time_from_RAMS_file(selected_fil)[0]\n",
    "    #### MAIN PART ####\n",
    "    z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(selected_fil,VARIABLE[0],model_name,output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "    # read the file to get coordinates\n",
    "    y_dim, x_dim     = np.shape(z)\n",
    "\n",
    "    if CONDITION_INFO[0]=='environment':\n",
    "        print('getting random coordinates over ',CONDITION_INFO[0],' points with threshold ',CONDITION_INFO[1])\n",
    "        print('        getting total condensate for conditional variogram')\n",
    "        if VARIABLE[1]<0:\n",
    "            conditional_field, _, _, _ = read_vars_WRF_RAMS.read_variable(selected_fil,'QTC',model_name,output_height=False,interpolate=True,level=0,interptype='model')\n",
    "        else:\n",
    "            conditional_field, _, _, _ = read_vars_WRF_RAMS.read_variable(selected_fil,'QTC',model_name,output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "        print('        min, max for the condensate field is ',np.min(conditional_field),' ',np.max(conditional_field))\n",
    "        coords = variogram_helper_functions.produce_random_coords_conditional(SAMPLE_SIZE, conditional_field, CONDITION_STATEMENT=lambda x: x < CONDITION_INFO[1])\n",
    "    if CONDITION_INFO[0]=='storm': \n",
    "        print('getting random coordinates over ',CONDITION_INFO[0],' points with threshold ',CONDITION_INFO[1])\n",
    "        print('        getting total condensate for conditional variogram')\n",
    "        if VARIABLE[1]<0:\n",
    "            conditional_field, _, _, _ = read_vars_WRF_RAMS.read_variable(selected_fil,'QTC',model_name,output_height=False,interpolate=True,level=0,interptype='model')\n",
    "        else:\n",
    "            conditional_field, _, _, _ = read_vars_WRF_RAMS.read_variable(selected_fil,'QTC',model_name,output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "        print('        min, max for the condensate field is ',np.min(conditional_field),' ',np.max(conditional_field))\n",
    "        coords = variogram_helper_functions.produce_random_coords_conditional(SAMPLE_SIZE, conditional_field, CONDITION_STATEMENT=lambda x: x >= CONDITION_INFO[1])\n",
    "    if CONDITION_INFO[0]=='all':\n",
    "        print('getting random coordinates over ',CONDITION_INFO[0],' points')\n",
    "        coords = variogram_helper_functions.produce_random_coords(x_dim,y_dim,SAMPLE_SIZE)   \n",
    "\n",
    "    # produce a random sample of coordinates\n",
    "    nonnan_coords, nonnan_values = variogram_helper_functions.get_values_at_random_coords(z, coords)\n",
    "    # get the values of the field at the random coordinates\n",
    "    max_lag = np.sqrt(x_dim**2 + y_dim**2)/2.0# in grid points\n",
    "    num_lag_classses = int(max_lag*dx/5.0)\n",
    "    # create a variogram and save bin and variogram values in a matrix for saving\n",
    "    V , bins, exp_variogram = variogram_helper_functions.make_variogram(nonnan_coords, nonnan_values,num_lag_classses,MAXLAG=max_lag,DX=dx)\n",
    "    bins_middle_points, counts, widths = variogram_helper_functions.retrieve_histogram(V,DX=dx)\n",
    "    ##########################\n",
    "\n",
    "    if SAVE_CSV:\n",
    "        savecsv = '/home/isingh/code/variogram_data/'+SIMULATION+'/'+'G'+DOMAIN+'/CSVs'\n",
    "        if not os.path.exists(savecsv):\n",
    "            os.makedirs(savecsv)\n",
    "        if VARIABLE[2]:\n",
    "            data_file = savecsv+'/experimental_variogram_'+SIMULATION+'_G'+DOMAIN+'_'+CONDITION_INFO[0]+'_points_threshold_'+str(CONDITION_INFO[1])+'_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_'+z_time+'.csv'\n",
    "        else:\n",
    "            data_file = savecsv+'/experimental_variogram_'+SIMULATION+'_G'+DOMAIN+'_'+CONDITION_INFO[0]+'_points_threshold_'+str(CONDITION_INFO[1])+'_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_'+z_time+'.csv'\n",
    "\n",
    "        data_matrix = np.column_stack((bins, counts, widths, exp_variogram))\n",
    "        np.savetxt(data_file, data_matrix, delimiter=',', header='bins,counts,widths,exp_variogram', comments='')\n",
    "\n",
    "        print('        saving variogram data to ',data_file)\n",
    "        #print('    ------\\n')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "variables =[ ['QV', 0, 'model', '$q_{v}$', '$kg^{2}kg^{-2}$']     ,['THETAV', 0, 'model','${\\Theta}_{v}$', '$K^{2}$'],\\\n",
    "             ['QV', 750, 'pressure', '$q_{v}$', '$kg^{2}kg^{-2}$'],['THETAV', 750, 'pressure','${\\Theta}_{v}$', '$K^{2}$'],\\\n",
    "             ['W', 750, 'pressure', '$w$', '$m^{2}s^{-2}$']       ,['WSPD', 0, 'model', '$wspd$', '$m^{2}s^{-2}$'],\\\n",
    "             ['SHF', -999, None, '$SHF$', '$W^{2}m^{-4}$']        ,['LHF', -999, None, '$LHF$' , '$W^{2}m^{-4}$'],\\\n",
    "             ['ITC',-999, None, '$ITC$', '$mm^{2}$']]\n",
    "\n",
    "colors      = ['#000000','#377eb8', '#56B4E9','#ff7f00', '#4daf4a','#f781bf', '#a65628', '#984ea3','#999999', '#e41a1c', '#dede00']\n",
    "simulations = ['AUS1.1-R','DRC1.1-R','PHI1.1-R','USA1.1-R','WPO1.1-R','PHI2.1-R','BRA1.1-R','BRA1.2-R','RSA1.1-R','ARG1.1-R','ARG1.2-R']\n",
    "#simulations_wrf=['AUS1.1-WT','AUS1.1-WM','DRC1.1-WT','DRC1.1-WM','USA1.1-WT','USA1.1-WM','PHI1.1-WT','PHI1.1-WM','WPO1.1-WT','WPO1.1-WM']\n",
    "domain='1'\n",
    "thresholds = [0.0000001,0.000001,0.00001,0.0001]\n",
    "\n",
    "# for simulation in simulations:\n",
    "#     for variable in variables:\n",
    "#         for threshold in thresholds:\n",
    "#             for partition in ['storm','environment','all']:\n",
    "#                 save_variogram('middle', variable, simulation, [partition, threshold] , 15000, domain, True)\n",
    "#                 print('=====================================================================================\\n\\n\\n\\n')\n",
    "\n",
    "                \n",
    "# Running on the terminal in parallel\n",
    "argument = []\n",
    "\n",
    "\n",
    "# for variable in variables:\n",
    "#     argument = argument + [('middle', variable, simulations, 15000, domain, colors, False, True)]\n",
    "\n",
    "for simulation in simulations:\n",
    "    for variable in variables:\n",
    "        for threshold in thresholds:\n",
    "            for partition in ['storm','environment','all']:\n",
    "                #save_variogram('middle', variable, simulation, [partition, threshold] , 15000, domain, True)\n",
    "                #print('=====================================================================================\\n\\n\\n\\n')\n",
    "                argument = argument + [('middle', variable, simulation, [partition, threshold] , 15000, domain, True)]\n",
    "\n",
    "print('length of argument is: ',len(argument))\n",
    "\n",
    "\n",
    "# # ############################### FIRST OF ALL ################################\n",
    "cpu_count1 = 20 #cpu_count()\n",
    "print('number of cpus: ',cpu_count1)\n",
    "# # #############################################################################\n",
    "\n",
    "def main(FUNCTION, ARGUMENT):\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(processes = (cpu_count1-1)) as pool:\n",
    "        data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main(save_variogram, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3e53a-c429-444c-999b-737cd356d0e5",
   "metadata": {},
   "source": [
    "### Plot variograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b9fd7-6f93-44f5-a5f0-b9b545ea1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mticker\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_variogram_from_csv(CSV_FILES,EXPERIMENT_NAME,PLOT_COUNTS_BAR,SAVE_DIR):\n",
    "\n",
    "    units_dict = {'Tk':'$K$','QV':'$kg kg^{-1}$','RH':'percent','WSPD':'$m s^{-1}$','U':'$m s^{-1}$',\\\n",
    "              'V':'$m s^{-1}$','W':'$m s^{-1}$','MCAPE':'$J kg^{-1}$','MCIN':'$J kg^{-1}$','THETA':'$K$','QTC':'$kg kg^{-1}$',\\\n",
    "                  'SHF':'$W m^{-2}$', 'LHF':'$W m^{-2}$'}\n",
    "    \n",
    "    vario_dict = {'Tk':'$K^{2}$','THETAV':'$K^{2}$','QV':'$kg^{2} kg^{-2}$','RH':'percent^{2}','WSPD':'$m^{2} s^{-2}$','U':'$m^{2} s^{-2}$',\\\n",
    "              'V':'$m^{2} s^{-2}$','W':'$m^{2} s^{-2}$','MCAPE':'$J^{2} kg^{-2}$','MCIN':'$J2 kg^{-2}$','THETA':'$K^{2}$','QTC':'$kg^{2} kg^{-2}$',\\\n",
    "                  'SHF':'$W^{2} m^{-4}$', 'LHF':'$W^{2} m^{-4}$','ITC':'$mm^{2}$'}\n",
    "\n",
    "    fig, ax   = plt.subplots(1,1,figsize=(8,8))\n",
    "    \n",
    "    if PLOT_COUNTS_BAR:\n",
    "        formatter = mticker.ScalarFormatter(useMathText=True)\n",
    "        formatter.set_powerlimits((-2,2))\n",
    "\n",
    "        ax_twin1 = ax.twinx()\n",
    "        ax_twin1.yaxis.set_major_formatter(formatter)\n",
    "        \n",
    "    for csv_file in CSV_FILES:\n",
    "        csv_filename = Path(csv_file).stem\n",
    "        # print(csv_filename)\n",
    "        all_info = csv_filename.split('_')\n",
    "        simulation_name = all_info[2]\n",
    "        grid = all_info[3]\n",
    "        partition = all_info[4]\n",
    "        threshold = all_info[7]\n",
    "        variable_name = all_info[8]\n",
    "        levtype = all_info[10]\n",
    "        lev = all_info[12]\n",
    "        time = all_info[13]\n",
    "        # print(simulation_name)\n",
    "        # print(grid)\n",
    "        # print(partition)\n",
    "        # print(threshold)\n",
    "        # print(variable_name)\n",
    "        # print(levtype)\n",
    "        # print(lev)\n",
    "        # print(time)\n",
    "\n",
    "        df = pd.read_csv(csv_file)\n",
    "        line = ax.plot(df.bins,df.exp_variogram,label=simulation_name+' '+partition+' '+threshold)#, color=COLORS[ii],linestyle = linestyles[jj])\n",
    "        # print('------------')\n",
    "        \n",
    "        if PLOT_COUNTS_BAR:\n",
    "            ax_twin1.bar(df.bins, df.counts, width = df.widths, color='yellowgreen', alpha=0.1, fill= False, edgecolor=ax.get_lines()[-1].get_color())\n",
    "            ax_twin1.set_ylabel('# pairwise observations',fontsize=16)\n",
    "    \n",
    "\n",
    "    if levtype!='None':\n",
    "        if levtype=='pressure':\n",
    "            level_units = 'mb'\n",
    "        if levtype=='model': \n",
    "            level_units = ''\n",
    "            lev = int(int(lev)+1)\n",
    "        title_string = 'Variogram for '+variable_name+' at '+levtype+' level '+str(lev)+' '+level_units+' for '+grid+'\\nmid-simulation'#' + timestring + ' UTC'\n",
    "\n",
    "    else:\n",
    "        level_units = ''\n",
    "        title_string = 'Variogram for '+variable_name+' for '+grid+'\\nmid-simulation'#' + timestring + ' UTC'\n",
    "\n",
    "    ax.set_title(title_string,fontsize=16)\n",
    "    ax.set_xlabel('distance (km)',fontsize=16)\n",
    "    ax.set_ylabel('Variogram'+' ('+vario_dict[variable_name]+')',fontsize=16)\n",
    "    #plt.yscale(\"log\") \n",
    "    ax.legend(fontsize=14)\n",
    "    savepng = '/home/isingh/code/variogram_data/PNGs'\n",
    "    if not os.path.exists(savepng):\n",
    "        os.makedirs(savepng)\n",
    "    # for single simulations\n",
    "    if levtype!='None':\n",
    "        filename = 'experimental_variogram_'+EXPERIMENT_NAME+'_'+simulation_name+'_'+variable_name+'_levtype_'+levtype+'_lev_'+str(lev)+'_'+grid+'_mid-simulation.png'\n",
    "    else:\n",
    "        filename = 'experimental_variogram_'+EXPERIMENT_NAME+'_'+simulation_name+'_'+variable_name+'_'+grid+'_mid-simulation.png'\n",
    "    # for multiple simulations\n",
    "    print('saving to file: ',savepng+'/'+filename)\n",
    "    #plt.savefig(filename,dpi=150)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3799c27-665c-4a62-8efd-d4d98c7ebbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = ['AUS1.1-R','DRC1.1-R','PHI1.1-R','USA1.1-R','WPO1.1-R','PHI2.1-R','BRA1.1-R','BRA1.2-R','RSA1.1-R','ARG1.1-R','ARG1.2-R']\n",
    "\n",
    "for sim in simulations:\n",
    "    for var in ['LHF','SHF','WSPD','W','ITC','QV_levtype_model_lev_0','QV_levtype_pressure_lev_750','THETAV_levtype_model_lev_0','THETAV_levtype_pressure_lev_750']:\n",
    "        vario_files = sorted(glob.glob('/home/isingh/code/variogram_data/'+sim+'/G1/CSVs/*environment*_'+var+'_*.csv'))\n",
    "        #print(vario_files)\n",
    "        plot_variogram_from_csv(vario_files,'environment_points_thresholds',False,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209629ee-8c23-4198-8a63-89b304bd82cc",
   "metadata": {},
   "source": [
    "### Make plan view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d09bff-eb70-4576-a6e9-b9f04e606d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile make_plan_views_RAMS.py\n",
    "# make plan views of variables:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as crs\n",
    "import random\n",
    "import skgstat as skg\n",
    "import matplotlib.ticker as ticker\n",
    "import read_vars_WRF_RAMS\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import variogram_helper_functions\n",
    "\n",
    "\n",
    "simulations=['AUS1.1-R']#PHI1.1-R','PHI2.1-R','WPO1.1-R','BRA1.1-R','DRC1.1-R','AUS1.1-R']\n",
    "domain='1'\n",
    "variables = [['QV', 750, 'pressure', '$w_{750}^{2}', '$m s^{-1}$']]#['QTC', 750, 'pressure', '$Theta_{sfc}^{2} (K^{2})$'],['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$'],['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$']]\n",
    "\n",
    "for var in variables:\n",
    "    for sim in simulations:\n",
    "        variogram_helper_functions.make_plan_view('middle', var, sim, '1', 'BrBG', 15000, False, ['environment',1e-07, True])\n",
    "\n",
    "# print('working on domain' ,domain)\n",
    "# #Running on the terminal in parallel\n",
    "# argument = []\n",
    "# for var in variables:\n",
    "#     argument = argument + [('middle',var, simulations, domain)]\n",
    "\n",
    "# print('length of argument is: ',len(argument))\n",
    "\n",
    "\n",
    "# # # ############################### FIRST OF ALL ################################\n",
    "# cpu_count1 = 37 #cpu_count()\n",
    "# print('number of cpus: ',cpu_count1)\n",
    "# # # #############################################################################\n",
    "\n",
    "# def main(FUNCTION, ARGUMENT):\n",
    "#     start_time = time.perf_counter()\n",
    "#     with Pool(processes = (cpu_count1-1)) as pool:\n",
    "#         data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "#     finish_time = time.perf_counter()\n",
    "#     print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "#     #df_all = pd.concat(data, ignore_index=True)\n",
    "#     #thermo_indices_data_csv_file = csv_folder+'thermodynamic_indices_' + DOMAIN + '_comb_track_filt_01_02_50_02_sr5017_setpos.csv'\n",
    "#     #print('saving thermodynamic indices to the file: ',thermo_indices_data_csv_file)\n",
    "#     #df_all.to_csv(thermo_indices_data_csv_file)  # sounding data\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main(make_plan_view, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbc81cc-d7a8-4a9b-ad94-320c9618b731",
   "metadata": {},
   "source": [
    "### Variograms of forward simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99400aba-4d10-45ce-8630-8cf28138b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import variogram_helper_functions\n",
    "#163,174,181, 178, 87\n",
    "import xarray as xr\n",
    "radiometer_filename1 = 'a-L-2006-01-23-114000-g1_dz100_ARTS_7-9-20-33-30_c1_INCUS87.nc'\n",
    "radiometer_filename2 = 'a-L-2006-01-23-114000-g1_dz100_ARTS_7-9-20-33-30_c1_INCUS163.nc'\n",
    "radiometer_filename3 = 'a-L-2006-01-23-114000-g1_dz100_ARTS_7-9-20-33-30_c1_INCUS174.nc'\n",
    "radiometer_filename4 = 'a-L-2006-01-23-114000-g1_dz100_ARTS_7-9-20-33-30_c1_INCUS178.nc'\n",
    "radiometer_filename5 = 'a-L-2006-01-23-114000-g1_dz100_ARTS_7-9-20-33-30_c1_INCUS181.nc'\n",
    "ds1 = xr.open_dataset('/monsoon/MODEL/FORWARD_MODEL_DATA/V0/AUS1.1-R-V0/RADIOMETER/'+radiometer_filename1)\n",
    "ds2 = xr.open_dataset('/monsoon/MODEL/FORWARD_MODEL_DATA/V0/AUS1.1-R-V0/RADIOMETER/'+radiometer_filename2)\n",
    "ds3 = xr.open_dataset('/monsoon/MODEL/FORWARD_MODEL_DATA/V0/AUS1.1-R-V0/RADIOMETER/'+radiometer_filename3)\n",
    "ds4 = xr.open_dataset('/monsoon/MODEL/FORWARD_MODEL_DATA/V0/AUS1.1-R-V0/RADIOMETER/'+radiometer_filename4)\n",
    "ds5 = xr.open_dataset('/monsoon/MODEL/FORWARD_MODEL_DATA/V0/AUS1.1-R-V0/RADIOMETER/'+radiometer_filename5)\n",
    "\n",
    "fig = plt.figure()\n",
    "labels = ['87','163','174','178','181']\n",
    "for ii, z in enumerate([ds1.Tb.values,ds2.Tb.values,ds3.Tb.values,ds4.Tb.values,ds5.Tb.values]):\n",
    "    print(ii,' : ',labels[ii])\n",
    "    y_dim, x_dim     = np.shape(z)\n",
    "    coords = variogram_helper_functions.produce_random_coords(x_dim,y_dim,15000)  \n",
    "    # produce a random sample of coordinates\n",
    "    nonnan_coords, nonnan_values = variogram_helper_functions.get_values_at_random_coords(z, coords)\n",
    "    # get the values of the field at the random coordinates\n",
    "    max_lag = np.sqrt(x_dim**2 + y_dim**2)/2.0# in grid points\n",
    "    num_lag_classses = int(max_lag*1.6/5.0)\n",
    "    # create a variogram and save bin and variogram values in a matrix for saving\n",
    "    _ , bins, exp_variogram, matrix_for_saving = variogram_helper_functions.make_variogram(nonnan_coords, nonnan_values,num_lag_classses,MAXLAG=max_lag,DX=1.6)\n",
    "    ##########################\n",
    "    plt.plot(bins,exp_variogram,label=labels[ii])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f5563-d290-43c6-a0f7-e5fed30cc99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiometer_filename1 = 'a-L-2006-01-23-114000-g1_dz100_ARTS_7-9-20-33-30_c1_INCUS87_FP.nc'\n",
    "radiometer_filename2 = 'a-L-2006-01-23-114000-g1_dz100_ARTS_7-9-20-33-30_c1_INCUS163_FP.nc'\n",
    "radiometer_filename3 = 'a-L-2006-01-23-114000-g1_dz100_ARTS_7-9-20-33-30_c1_INCUS174_FP.nc'\n",
    "radiometer_filename4 = 'a-L-2006-01-23-114000-g1_dz100_ARTS_7-9-20-33-30_c1_INCUS178_FP.nc'\n",
    "radiometer_filename5 = 'a-L-2006-01-23-114000-g1_dz100_ARTS_7-9-20-33-30_c1_INCUS181_FP.nc'\n",
    "ds1 = xr.open_dataset('/monsoon/MODEL/FORWARD_MODEL_DATA/V0/AUS1.1-R-V0/RADIOMETER/'+radiometer_filename1)\n",
    "ds2 = xr.open_dataset('/monsoon/MODEL/FORWARD_MODEL_DATA/V0/AUS1.1-R-V0/RADIOMETER/'+radiometer_filename2)\n",
    "ds3 = xr.open_dataset('/monsoon/MODEL/FORWARD_MODEL_DATA/V0/AUS1.1-R-V0/RADIOMETER/'+radiometer_filename3)\n",
    "ds4 = xr.open_dataset('/monsoon/MODEL/FORWARD_MODEL_DATA/V0/AUS1.1-R-V0/RADIOMETER/'+radiometer_filename4)\n",
    "ds5 = xr.open_dataset('/monsoon/MODEL/FORWARD_MODEL_DATA/V0/AUS1.1-R-V0/RADIOMETER/'+radiometer_filename5)\n",
    "\n",
    "fig = plt.figure()\n",
    "labels = ['87','163','174','178','181']\n",
    "for ii, z in enumerate([ds1.Tb.values,ds2.Tb.values,ds3.Tb.values,ds4.Tb.values,ds5.Tb.values]):\n",
    "    print(ii,' : ',labels[ii])\n",
    "    y_dim, x_dim     = np.shape(z)\n",
    "    coords = variogram_helper_functions.produce_random_coords(x_dim,y_dim,15000)  \n",
    "    # produce a random sample of coordinates\n",
    "    nonnan_coords, nonnan_values = variogram_helper_functions.get_values_at_random_coords(z, coords)\n",
    "    # get the values of the field at the random coordinates\n",
    "    max_lag = np.sqrt(x_dim**2 + y_dim**2)/2.0# in grid points\n",
    "    num_lag_classses = int(max_lag*1.6/5.0)\n",
    "    # create a variogram and save bin and variogram values in a matrix for saving\n",
    "    _ , bins, exp_variogram, matrix_for_saving = variogram_helper_functions.make_variogram(nonnan_coords, nonnan_values,num_lag_classses,MAXLAG=max_lag,DX=1.6)\n",
    "    ##########################\n",
    "    plt.plot(bins,exp_variogram,label=labels[ii])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dbe8ab-e2a0-4b97-af47-e25db1819543",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/monsoon/MODEL/FORWARD_MODEL_DATA/V0/AUS1.1-R-V0/GEO_IR/'+'a-A-2006-01-23-133000-g1_ahi_himawari8.nc')\n",
    "fig = plt.figure()\n",
    "wavelengths = ['6.2','6.9','7.3','11.2']\n",
    "for ii, wavelength in enumerate(wavelengths):\n",
    "    z = ds.Tb.sel(wavelength = float(wavelength)).values\n",
    "    print(ii,' : ',wavelengths[ii])\n",
    "    y_dim, x_dim     = np.shape(z)\n",
    "    coords = variogram_helper_functions.produce_random_coords(x_dim,y_dim,5000)  \n",
    "    # produce a random sample of coordinates\n",
    "    nonnan_coords, nonnan_values = variogram_helper_functions.get_values_at_random_coords(z, coords)\n",
    "    # get the values of the field at the random coordinates\n",
    "    max_lag = np.sqrt(x_dim**2 + y_dim**2)/2.0# in grid points\n",
    "    num_lag_classses = int(max_lag*1.6/5.0)\n",
    "    # create a variogram and save bin and variogram values in a matrix for saving\n",
    "    _ , bins, exp_variogram, matrix_for_saving = variogram_helper_functions.make_variogram(nonnan_coords, nonnan_values,num_lag_classses,MAXLAG=max_lag,DX=1.6)\n",
    "    ##########################\n",
    "    plt.plot(bins,exp_variogram,label=wavelengths[ii])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d721835-2ac5-49c8-91a4-f6fe65f91d71",
   "metadata": {},
   "source": [
    "### One sample variograms for nested domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6376e-dedf-47cf-93df-318002d059d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile save_coincident_variograms_multiprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as crs\n",
    "import random\n",
    "import skgstat as skg\n",
    "plt.style.use('ggplot')\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import read_vars_WRF_RAMS\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_coincident_variograms(WHICH_TIME, VARIABLE, SIMULATIONS, SAMPLE_SIZE, DOMAINS, COINCIDENT, NSAMPLES, COLORS, SAVE_NPY, PLOT):\n",
    "    \n",
    "    print('working on ',VARIABLE,'\\n')\n",
    "\n",
    "    for ii,simulation in enumerate(SIMULATIONS): \n",
    "        print('  <<working on simulation: ',simulation,'>>\\n')\n",
    "        if PLOT:\n",
    "            fig    = plt.figure(figsize=(8,8))\n",
    "        for DOMAIN in DOMAINS:\n",
    "            print('    <<working on domain: ',DOMAIN,'>>\\n')\n",
    "            if DOMAIN==1:\n",
    "                dx = 1.6\n",
    "            if DOMAIN==2:\n",
    "                dx=0.4\n",
    "            if DOMAIN==3:\n",
    "                dx=0.1\n",
    "            \n",
    "            if simulation=='PHI2.1-R':\n",
    "                rams_g3_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G'+'3'+'/out_30s/Lite/'+'a-L-*g3.h5'))# CSU machine\n",
    "            else:\n",
    "                rams_g3_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G'+'3'+'/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "            print('        total # files = ',len(rams_g3_files))\n",
    "            print('        first file is ',rams_g3_files[0])\n",
    "            print('        last file is ',rams_g3_files[-1])\n",
    "\n",
    "            if WHICH_TIME=='start':\n",
    "                rams_g3_fil    = rams_g3_files[0]\n",
    "                print('        choosing the start file: ',rams_g3_fil)\n",
    "            if WHICH_TIME=='middle':\n",
    "                rams_g3_fil    = rams_g3_files[int(len(rams_g3_files)/2)]\n",
    "                print('        choosing the middle file: ',rams_g3_fil)\n",
    "            if WHICH_TIME=='end':\n",
    "                rams_g3_fil    = rams_g3_files[-1]\n",
    "                print('        choosing the end file: ',rams_g3_fil)\n",
    "\n",
    "            if DOMAIN==1 or DOMAIN ==2:\n",
    "                print('searching for domain ',DOMAIN,' file for the same time in the directory: ',\\\n",
    "                      '/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G'+str(DOMAIN)+'/out/')\n",
    "                g3_time = get_time_from_RAMS_file(rams_g3_fil)[2]\n",
    "                print('        time in G3 file ',g3_time)\n",
    "                if simulation=='PHI2.1-R':\n",
    "                    rams_g1_or_g2_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G3/out_30s/Lite/'+'a-L-*g'+str(DOMAIN)+'.h5'))# CSU machine\n",
    "                else:\n",
    "                    rams_g1_or_g2_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G3/out_30s/'+'a-L-*g'+str(DOMAIN)+'.h5'))# CSU machine\n",
    "                print('        found total ',len(rams_g1_or_g2_files),' files for domain ',DOMAIN,' for this simulation')\n",
    "                list_of_times = [get_time_from_RAMS_file(fil)[2] for fil in rams_g1_or_g2_files]\n",
    "                ind_g1_or_g2_file_index = find_closest_datetime_index(list_of_times, g3_time)\n",
    "                rams_larger_grid_file = rams_g1_or_g2_files[ind_g1_or_g2_file_index]\n",
    "                print('        found the file ',rams_larger_grid_file)\n",
    "\n",
    "\n",
    "            if DOMAIN<3:\n",
    "                print('Domain intersection portion...')\n",
    "                z, z_name, z_units, z_time = grab_intersection_gbig_gsmall(VARIABLE,rams_larger_grid_file,rams_g3_fil)\n",
    "            else:\n",
    "                z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(rams_g3_fil,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "\n",
    "            # read the file to get coordinates\n",
    "            y_dim, x_dim     = np.shape(z)\n",
    "            coords = produce_random_coords(x_dim,y_dim,SAMPLE_SIZE)                           \n",
    "            # produce a random sample of coordinates\n",
    "            nonnan_coords, nonnan_values = get_values_at_random_coords(z, coords)\n",
    "            # get the values of the field at the random coordinates\n",
    "            max_lag = np.sqrt(x_dim**2 + y_dim**2)/2.0# in grid points\n",
    "            # create a variogram and save bin and variogram values in a matrix for saving\n",
    "            _ , bins, exp_variogram, matrix_for_saving = variogram_helper_functions.make_variogram(nonnan_coords, nonnan_values,100,MAXLAG=max_lag,DX=dx)\n",
    "\n",
    "            if SAVE_NPY:\n",
    "                if VARIABLE[2]:\n",
    "                    data_file = 'experimental_variogram_RAMS_coincident_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_'+z_time+'_'+simulation+'_1_sample_no_mask_d0'+str(DOMAIN)+'.npy'\n",
    "                else:\n",
    "                    data_file = 'experimental_variogram_RAMS_coincident_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_'+z_time+'_'+simulation+'_1_sample_no_mask_d0'+str(DOMAIN)+'.npy'\n",
    "\n",
    "                with open(data_file, 'wb') as f:\n",
    "                    np.save(f, matrix_for_saving)\n",
    "                    np.save(f, matrix_for_saving)\n",
    "\n",
    "                print('        saving variogram data to ',data_file)\n",
    "                print('        ------\\n')\n",
    "\n",
    "            if DOMAIN==1:\n",
    "                linestyle = '-'\n",
    "            if DOMAIN==2:\n",
    "                linestyle = '--'\n",
    "            if DOMAIN==3:\n",
    "                linestyle = ':'\n",
    "\n",
    "            # Add lines to the plot\n",
    "            if PLOT:\n",
    "                plt.plot(bins[bins<150],exp_variogram[bins<150],label=simulation+' d0'+str(DOMAIN), color=COLORS[ii],linestyle=linestyle)\n",
    "\n",
    "        # Add annotations/title and save the plot\n",
    "        if PLOT:\n",
    "            if VARIABLE[2]:\n",
    "                title_string = 'Variogram for '+VARIABLE[0]+' at '+VARIABLE[2]+' level '+str(int(VARIABLE[1]))+'\\nat mid-simulation'\n",
    "            else:\n",
    "                title_string = 'Variogram for '+VARIABLE[0]+'\\nat mid-simulation'    \n",
    "            plt.title(title_string)\n",
    "            plt.xlabel('distance (km)')\n",
    "            plt.ylabel(VARIABLE[3])\n",
    "            plt.legend()\n",
    "            if VARIABLE[2]:\n",
    "                filename = 'experimental_variogram_RAMS_'+simulation+'_coincident_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_1_sample_no_mask_mid-simulation.png'\n",
    "            else:\n",
    "                filename = 'experimental_variogram_RAMS_'+simulation+'_coincident_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_1_sample_no_mask_mid-simulation.png'\n",
    "            print('        saving plot to file: ',filename)\n",
    "            plt.savefig(filename,dpi=150)\n",
    "            #print('\\n\\n')\n",
    "        print('------------------------------------------------------------------------\\n')\n",
    "\n",
    "# PARAMETERS\n",
    "simulations=['BRA1.1-R','PHI2.1-R','DRC1.1-R','PHI1.1-R','WPO1.1-R','BRA1.1-R','USA1.1-R','AUS1.1-R']\n",
    "domains=[1,2,3]\n",
    "nsamples=1\n",
    "sample_size = 15000\n",
    "variables = [['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$'], ['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$'],\\\n",
    "              ['W', 750, 'pressure', '$W_{750}^{2} (m^{2}s^{-2})$'],['MCAPE', -999, None, '$MCAPE^{2} (J^{2}kg^{-2})$'] ]             \n",
    "# [['TOP_SOIL_MOISTURE', -999, None, '$SM^{2} (m^{3} m^{3})$'], ['LHF', -999, None, '$LHF^{2} (Wm^{-2})$'],\\\n",
    "#              ['SHF', -999, None, '$SHF^{2} (Wm^{-2})$'],\\\n",
    "#              ['Tk', 0, 'model', '$T_{sfc}^{2} (K^{2})$']             , ['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$'], ['RH', 0, 'model', '$RH_{sfc}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 0, 'model', '$U_{sfc}^{2} (m^{2}s^{-2})$']        , ['V', 0, 'model', '$V_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 0, 'model', '$WSPD_{sfc}^{2} (m^{2}s^{-2})$']  , ['W', 0, 'model', '$W_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['MCAPE', -999, None, '$MCAPE^{2} (J^{2}kg^{-2})$']     , ['MCIN', -999, None, '$MCIN^{2} (J^{2}kg^{-2})$'], \\\n",
    "#              ['Tk', 750, 'pressure', '$T_{750}^{2} (K^{2})$']        , ['THETA', 750, 'pressure', '$Theta_{750}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 750, 'pressure', '$Qvapor_{750}^{2} (kg^{2}kg^{-2})$'], ['RH', 750, 'pressure', '$RH_{750}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 750, 'pressure', '$U_{750}^{2} (m^{2}s^{-2})$']   , ['V', 750, 'pressure', '$V_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 750, 'pressure', '$WSPD_{750}^{2} (m^{2}s^{-2})$'], ['W', 750, 'pressure', '$W_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['Tk', 500, 'pressure', '$T_{500}^{2} (K^{2})$']        , ['THETA', 500, 'pressure', '$Theta_{500}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 500, 'pressure', '$Qvapor_{500}^{2} (kg^{2}kg^{-2})$'], ['RH', 500, 'pressure', '$RH_{500}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 500, 'pressure', '$U_{500}^{2} (m^{2}s^{-2})$']   , ['V', 500, 'pressure', '$V_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 500, 'pressure', '$WSPD_{500}^{2} (m^{2}s^{-2})$'], ['W', 500, 'pressure', '$W_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['Tk', 200, 'pressure', '$T_{200}^{2} (K^{2})$']        , ['THETA', 200, 'pressure', '$Theta_{200}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 200, 'pressure', '$Qvapor_{200}^{2} (kg^{2}kg^{-2})$'], ['RH', 200, 'pressure', '$RH_{200}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 200, 'pressure', '$U_{200}^{2} (m^{2}s^{-2})$']   , ['V', 200, 'pressure', '$V_{200}^{2} (m^{2}s^{-2})$'], \\\n",
    "#              ['WSPD', 200, 'pressure', '$WSPD_{200}^{2} (m^{2}s^{-2})$'], ['W', 200, 'pressure', '$W_{200}^{2} (m^{2}s^{-2})$']]\n",
    "colors    =  ['#000000','#000000','#E69F00','#56B4E9','#009E73','#0072B2','#D55E00','#CC79A7']\n",
    "\n",
    "# for variable in variables:\n",
    "#     save_coincident_variograms('middle', variable, simulations, 15000, domains, False, 1, colors, True)\n",
    "#     print('=====================================================================================\\n\\n\\n\\n')\n",
    "          \n",
    "          \n",
    "#Running on the terminal in parallel\n",
    "argument = []\n",
    "for variable in variables:\n",
    "    argument = argument + [('middle', variable, simulations, 15000, domains, False, 1, colors, False, True)]\n",
    "\n",
    "print('length of argument is: ',len(argument))\n",
    "\n",
    "\n",
    "# # ############################### FIRST OF ALL ################################\n",
    "cpu_count1 = 18 #cpu_count()\n",
    "print('number of cpus: ',cpu_count1)\n",
    "# # #############################################################################\n",
    "\n",
    "def main(FUNCTION, ARGUMENT):\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(processes = (cpu_count1-1)) as pool:\n",
    "        data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "    #df_all = pd.concat(data, ignore_index=True)\n",
    "    #thermo_indices_data_csv_file = csv_folder+'thermodynamic_indices_' + DOMAIN + '_comb_track_filt_01_02_50_02_sr5017_setpos.csv'\n",
    "    #print('saving thermodynamic indices to the file: ',thermo_indices_data_csv_file)\n",
    "    #df_all.to_csv(thermo_indices_data_csv_file)  # sounding data\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main(save_coincident_variograms, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb968c8d-c841-42ec-acba-264c15f25eb5",
   "metadata": {},
   "source": [
    "### One sample variograms for nested, RESAMPLED domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8230a0-6ffd-4c93-a382-6cbcd08983cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile save_resampled_coincident_variograms_multiprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as crs\n",
    "import random\n",
    "import skgstat as skg\n",
    "plt.style.use('ggplot')\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import read_vars_WRF_RAMS\n",
    "from pathlib import Path\n",
    "import variogram_helper_functions\n",
    "\n",
    "\n",
    "\n",
    "def save_resampled_coincident_variograms(WHICH_TIME, VARIABLE, SIMULATIONS, SAMPLE_SIZE, DOMAINS, COINCIDENT, NSAMPLES, COLORS, PLOT):\n",
    "    \n",
    "    print('working on ',VARIABLE,'\\n')\n",
    "\n",
    "    for ii,simulation in enumerate(SIMULATIONS): \n",
    "        print('  <<working on simulation: ',simulation,'>>\\n')\n",
    "        if PLOT:\n",
    "            fig    = plt.figure(figsize=(8,8))\n",
    "        for DOMAIN in DOMAINS:\n",
    "            print('    <<working on domain: ',DOMAIN,'>>\\n')\n",
    "            if DOMAIN==1:\n",
    "                dx=1.6\n",
    "            if DOMAIN==2:\n",
    "                dx=1.6\n",
    "            if DOMAIN==3:\n",
    "                dx=1.6\n",
    "            \n",
    "            if simulation=='PHI2.1-R':\n",
    "                rams_g3_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G'+'3'+'/out_30s/Lite/'+'a-L-*g3.h5'))# CSU machine\n",
    "            else:\n",
    "                rams_g3_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G'+'3'+'/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "            print('        total # files = ',len(rams_g3_files))\n",
    "            print('        first file is ',rams_g3_files[0])\n",
    "            print('        last file is ',rams_g3_files[-1])\n",
    "\n",
    "            if WHICH_TIME=='start':\n",
    "                rams_g3_fil    = rams_g3_files[0]\n",
    "                print('        choosing the start file: ',rams_g3_fil)\n",
    "            if WHICH_TIME=='middle':\n",
    "                rams_g3_fil    = rams_g3_files[int(len(rams_g3_files)/2)]\n",
    "                print('        choosing the middle file: ',rams_g3_fil)\n",
    "            if WHICH_TIME=='end':\n",
    "                rams_g3_fil    = rams_g3_files[-1]\n",
    "                print('        choosing the end file: ',rams_g3_fil)\n",
    "\n",
    "            if DOMAIN==1 or DOMAIN ==2:\n",
    "                print('searching for domain ',DOMAIN,' file for the same time in the directory: ',\\\n",
    "                      '/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G'+str(DOMAIN)+'/out/')\n",
    "                g3_time = get_time_from_RAMS_file(rams_g3_fil)[2]\n",
    "                print('        time in G3 file ',g3_time)\n",
    "                if simulation=='PHI2.1-R':\n",
    "                    rams_g1_or_g2_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G3/out_30s/Lite/'+'a-L-*g'+str(DOMAIN)+'.h5'))# CSU machine\n",
    "                else:\n",
    "                    rams_g1_or_g2_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G3/out_30s/'+'a-L-*g'+str(DOMAIN)+'.h5'))# CSU machine\n",
    "                print('        found total ',len(rams_g1_or_g2_files),' files for domain ',DOMAIN,' for this simulation')\n",
    "                list_of_times = [get_time_from_RAMS_file(fil)[2] for fil in rams_g1_or_g2_files]\n",
    "                ind_g1_or_g2_file_index = find_closest_datetime_index(list_of_times, g3_time)\n",
    "                rams_larger_grid_file = rams_g1_or_g2_files[ind_g1_or_g2_file_index]\n",
    "                print('        found the file ',rams_larger_grid_file)\n",
    "\n",
    "\n",
    "            if DOMAIN<3:\n",
    "                print('Domain intersection portion...')\n",
    "                z, z_name, z_units, z_time = grab_intersection_gbig_gsmall(VARIABLE,rams_larger_grid_file,rams_g3_fil)\n",
    "            else:\n",
    "                z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(rams_g3_fil,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "\n",
    "            \n",
    "            if DOMAIN==2:\n",
    "                # Reshape and average to reduce size by 4 in each dimension\n",
    "                print('Resampling the domain 2 array...')\n",
    "                new_shape = (z.shape[0] // 4, z.shape[1] // 4)\n",
    "                z_temp = z[:new_shape[0]*4, :new_shape[1]*4].reshape(new_shape[0], 4, new_shape[1], 4)\n",
    "                print('        shape of the temporary reshaped array is ',z_temp.shape)\n",
    "                z = np.nanmean(z_temp,axis=(1, 3))\n",
    "                print('        shape of the smaller array is ',z.shape)\n",
    "            if DOMAIN==3:\n",
    "                print('        Resampling the domain 3 array...')\n",
    "                new_shape = (z.shape[0] // 16, z.shape[1] // 16)\n",
    "                z_temp = z[:new_shape[0]*16, :new_shape[1]*16].reshape(new_shape[0], 16, new_shape[1], 16)\n",
    "                print('        shape of the temporary reshaped array is ',z_temp.shape)\n",
    "                z = np.nanmean(z_temp, axis=(1, 3))\n",
    "                print('        shape of the smaller array is ',z.shape)\n",
    "                \n",
    "            # read the file to get coordinates\n",
    "            y_dim, x_dim     = np.shape(z)\n",
    "            coords = produce_random_coords(x_dim,y_dim,SAMPLE_SIZE)                           \n",
    "            # produce a random sample of coordinates\n",
    "            nonnan_coords, nonnan_values = get_values_at_random_coords(z, coords)\n",
    "            # get the values of the field at the random coordinates\n",
    "            bins, exp_variogram, matrix_for_saving = make_variogram(nonnan_coords, nonnan_values, 200, dx)\n",
    "            # create a variogram and save bin and variogram values in a matrix for saving\n",
    "\n",
    "            if VARIABLE[2]:\n",
    "                data_file = 'experimental_resampled_variogram_RAMS_coincident_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_'+z_time+'_'+simulation+'_1_sample_no_mask_d0'+str(DOMAIN)+'.npy'\n",
    "            else:\n",
    "                data_file = 'experimental_resampled_variogram_RAMS_coincident_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_'+z_time+'_'+simulation+'_1_sample_no_mask_d0'+str(DOMAIN)+'.npy'\n",
    "\n",
    "            with open(data_file, 'wb') as f:\n",
    "                np.save(f, matrix_for_saving)\n",
    "\n",
    "            print('        saving variogram data to ',data_file)\n",
    "            print('        ------\\n')\n",
    "\n",
    "            if DOMAIN==1:\n",
    "                linestyle = '-'\n",
    "            if DOMAIN==2:\n",
    "                linestyle = '--'\n",
    "            if DOMAIN==3:\n",
    "                linestyle = ':'\n",
    "\n",
    "            if PLOT:\n",
    "                plt.plot(bins[bins<150],exp_variogram[bins<150],label=simulation+' d0'+str(DOMAIN), color=COLORS[ii],linestyle=linestyle)\n",
    "\n",
    "        if PLOT:\n",
    "            if VARIABLE[2]:\n",
    "                title_string = 'Variogram (resampled) for '+VARIABLE[0]+' at '+VARIABLE[2]+' level '+str(int(VARIABLE[1]))+'\\nat mid-simulation'\n",
    "            else:\n",
    "                title_string = 'Variogram (resampled) for '+VARIABLE[0]+'\\nat mid-simulation'    \n",
    "            plt.title(title_string)\n",
    "            plt.xlabel('distance (km)')\n",
    "            plt.ylabel(VARIABLE[3])\n",
    "            plt.legend()\n",
    "            if VARIABLE[2]:\n",
    "                filename = 'experimental_resampled_variogram_RAMS_'+simulation+'_coincident_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_1_sample_no_mask_mid-simulation.png'\n",
    "            else:\n",
    "                filename = 'experimental_resampled_variogram_RAMS_'+simulation+'_coincident_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_1_sample_no_mask_mid-simulation.png'\n",
    "            print('        saving plot to file: ',filename)\n",
    "            plt.savefig(filename,dpi=150)\n",
    "            #print('\\n\\n')\n",
    "        print('------------------------------------------------------------------------\\n')\n",
    "\n",
    "# PARAMETERS\n",
    "simulations=['BRA1.1-R','PHI2.1-R','DRC1.1-R','PHI1.1-R','WPO1.1-R','USA1.1-R','AUS1.1-R']\n",
    "domains=[1,2,3]\n",
    "nsamples=1\n",
    "sample_size = 15000\n",
    "variables = [['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$'], ['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$'],\\\n",
    "             ['W', 750, 'pressure', '$W_{750}^{2} (m^{2}s^{-2})$']]#,['MCAPE', -999, None, '$MCAPE^{2} (J^{2}kg^{-2})$'] ]             \n",
    "# [['TOP_SOIL_MOISTURE', -999, None, '$SM^{2} (m^{3} m^{3})$'], ['LHF', -999, None, '$LHF^{2} (Wm^{-2})$'],\\\n",
    "#              ['SHF', -999, None, '$SHF^{2} (Wm^{-2})$'],\\\n",
    "#              ['Tk', 0, 'model', '$T_{sfc}^{2} (K^{2})$']             , ['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$'], ['RH', 0, 'model', '$RH_{sfc}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 0, 'model', '$U_{sfc}^{2} (m^{2}s^{-2})$']        , ['V', 0, 'model', '$V_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 0, 'model', '$WSPD_{sfc}^{2} (m^{2}s^{-2})$']  , ['W', 0, 'model', '$W_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['MCAPE', -999, None, '$MCAPE^{2} (J^{2}kg^{-2})$']     , ['MCIN', -999, None, '$MCIN^{2} (J^{2}kg^{-2})$'], \\\n",
    "#              ['Tk', 750, 'pressure', '$T_{750}^{2} (K^{2})$']        , ['THETA', 750, 'pressure', '$Theta_{750}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 750, 'pressure', '$Qvapor_{750}^{2} (kg^{2}kg^{-2})$'], ['RH', 750, 'pressure', '$RH_{750}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 750, 'pressure', '$U_{750}^{2} (m^{2}s^{-2})$']   , ['V', 750, 'pressure', '$V_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 750, 'pressure', '$WSPD_{750}^{2} (m^{2}s^{-2})$'], ['W', 750, 'pressure', '$W_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['Tk', 500, 'pressure', '$T_{500}^{2} (K^{2})$']        , ['THETA', 500, 'pressure', '$Theta_{500}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 500, 'pressure', '$Qvapor_{500}^{2} (kg^{2}kg^{-2})$'], ['RH', 500, 'pressure', '$RH_{500}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 500, 'pressure', '$U_{500}^{2} (m^{2}s^{-2})$']   , ['V', 500, 'pressure', '$V_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 500, 'pressure', '$WSPD_{500}^{2} (m^{2}s^{-2})$'], ['W', 500, 'pressure', '$W_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['Tk', 200, 'pressure', '$T_{200}^{2} (K^{2})$']        , ['THETA', 200, 'pressure', '$Theta_{200}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 200, 'pressure', '$Qvapor_{200}^{2} (kg^{2}kg^{-2})$'], ['RH', 200, 'pressure', '$RH_{200}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 200, 'pressure', '$U_{200}^{2} (m^{2}s^{-2})$']   , ['V', 200, 'pressure', '$V_{200}^{2} (m^{2}s^{-2})$'], \\\n",
    "#              ['WSPD', 200, 'pressure', '$WSPD_{200}^{2} (m^{2}s^{-2})$'], ['W', 200, 'pressure', '$W_{200}^{2} (m^{2}s^{-2})$']]\n",
    "colors    =  ['#000000','#E69F00','#56B4E9','#009E73','#F0E442','#0072B2','#D55E00','#CC79A7']\n",
    "\n",
    "# for variable in variables:\n",
    "#     save_resampled_coincident_variograms('middle', variable, simulations, 15000, domains, False, 1, colors, True)\n",
    "#     print('=====================================================================================\\n\\n\\n\\n')\n",
    "          \n",
    "          \n",
    "#Running on the terminal in parallel\n",
    "argument = []\n",
    "for variable in variables:\n",
    "    argument = argument + [('middle', variable, simulations, 15000, domains, False, 1, colors, True)]\n",
    "\n",
    "print('length of argument is: ',len(argument))\n",
    "\n",
    "# # ############################### FIRST OF ALL ################################\n",
    "cpu_count1 = 18 #cpu_count()\n",
    "print('number of cpus: ',cpu_count1)\n",
    "# # #############################################################################\n",
    "\n",
    "def main(FUNCTION, ARGUMENT):\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(processes = (cpu_count1-1)) as pool:\n",
    "        data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "      \n",
    "if __name__ == \"__main__\":\n",
    "    main(save_resampled_coincident_variograms, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe25b0-570f-4830-8e42-2b667de9f445",
   "metadata": {},
   "source": [
    "### Save spatial autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b293ec-3a68-4a98-baba-f1ee3de62c51",
   "metadata": {},
   "source": [
    "#### Testing/Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f813d4-ab46-4c02-a01f-8a788c7551a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx=0.1\n",
    "temperature_data = np.random.rand(150, 150)  # Example random temperature field data\n",
    "array = np.zeros((150, 150))\n",
    "\n",
    "# Define the center and radius of the circle\n",
    "center_x1 = 120\n",
    "center_y1 = 60\n",
    "radius1 = 7.5\n",
    "\n",
    "center_x2 = 70\n",
    "center_y2 = 100\n",
    "radius2 = 10\n",
    "\n",
    "# Generate indices for the circle\n",
    "y_indices, x_indices = np.ogrid[:150, :150]\n",
    "#circle_mask1 = (x_indices - center_x1)**2 + (y_indices - center_y1)**2 <= radius1**2\n",
    "circle_mask2 = (x_indices - center_x2)**2 + (y_indices - center_y2)**2 <= radius2**2\n",
    "\n",
    "# Set the values inside the circle to 1\n",
    "#array[circle_mask1] = 1\n",
    "array[circle_mask2] = 2\n",
    "temperature_data = temperature_data  + array\n",
    "plt.imshow(temperature_data)\n",
    "plt.savefig('circle.png',dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524787f-8bf7-471b-9f2b-9861811998c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from libpysal.weights.distance import DistanceBand\n",
    "import libpysal \n",
    "from esda.moran import Moran\n",
    "import random\n",
    "\n",
    "# Generate or load your 2D field data of temperature\n",
    "# Here, I'm assuming you have a 2D array named 'temperature_data'\n",
    "# Replace this with your actual temperature field data\n",
    "dx=0.1\n",
    "temperature_data = np.random.rand(150, 150)  # Example random temperature field data\n",
    "array = np.zeros((150, 150))\n",
    "\n",
    "# Define the center and radius of the circle\n",
    "center_x1 = 120\n",
    "center_y1 = 60\n",
    "radius1 = 7.5\n",
    "\n",
    "center_x2 = 70\n",
    "center_y2 = 100\n",
    "radius2 = 10\n",
    "\n",
    "# Generate indices for the circle\n",
    "y_indices, x_indices = np.ogrid[:150, :150]\n",
    "#circle_mask1 = (x_indices - center_x1)**2 + (y_indices - center_y1)**2 <= radius1**2\n",
    "circle_mask2 = (x_indices - center_x2)**2 + (y_indices - center_y2)**2 <= radius2**2\n",
    "\n",
    "# Set the values inside the circle to 1\n",
    "#array[circle_mask1] = 1\n",
    "array[circle_mask2] = 2\n",
    "temperature_data = temperature_data  + array\n",
    "\n",
    "\n",
    "y_dim, x_dim     = np.shape(temperature_data)\n",
    "print('        shape of the arrays is ',y_dim,'x',x_dim)\n",
    "x      = np.arange(0,x_dim)#*dx\n",
    "y      = np.arange(0,y_dim)#*dx\n",
    "# # full coordinate arrays\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "coords_tuples_2d = np.vstack(([yy.T], [xx.T])).T\n",
    "print('        shape of combined coords matrix: ',np.shape(coords_tuples_2d))\n",
    "coords_all_list = coords_tuples_2d.reshape(-1, 2).tolist()\n",
    "print('        shape of 1d list of coords: ',np.shape(coords_all_list))\n",
    "tuples_all_tuple =  [tuple(sublist) for sublist in coords_all_list]\n",
    "#print(tuples_all_tuple[0:10])\n",
    "#print(type(tuples_all_tuple[0]))\n",
    "\n",
    "sample_coords_tuple = random.sample(tuples_all_tuple,5000)\n",
    "print(sample_coords_tuple[0:10])\n",
    "print('        get field values from these points...')\n",
    "values = np.fromiter((temperature_data[c[0], c[1]] for c in sample_coords_tuple), dtype=float)\n",
    "modified_sample_coords_tuple = [tuple(dx * element for element in tup) for tup in sample_coords_tuple]\n",
    "print(modified_sample_coords_tuple[0:10])\n",
    "#w=libpysal.weights.DistanceBand(sample_coords_tuple,threshold=11.2,binary=False)\n",
    "#w=libpysal.weights.DistanceBand(sample_coords_tuple,threshold=11.2,binary=False,alpha=-2.)\n",
    "distance_intervals = np.arange(0.1,14,0.5)  # Define distance intervals to compute Moran's I\n",
    "moran_i_values  = []\n",
    "moran_ei_values = []\n",
    "moran_p_values  = []\n",
    "moran_z_values  = []\n",
    "moran_vi_values = []\n",
    "for distance_interval in distance_intervals:\n",
    "    print('distance interval : ',distance_interval)\n",
    "    # Create binary spatial weights matrix based on distance interval\n",
    "    w=libpysal.weights.DistanceBand(modified_sample_coords_tuple,threshold=distance_interval,binary=True,silence_warnings=True)\n",
    "    #print(len(w.neighbors))\n",
    "    print('# islands: ',len(w.islands))\n",
    "    print('# max_neighbors: ',(w.max_neighbors))\n",
    "    print('# mean_neighbors: ',(w.mean_neighbors))\n",
    "    print('# min_neighbors: ',(w.min_neighbors))\n",
    "    #print('neighbors : ')\n",
    "    #print(w.neighbors)\n",
    "    #print('weights : ')\n",
    "    #print(w.weights)\n",
    "    #w =DistanceBand (data = temperature_data, threshold = distance_interval)\n",
    "    #w_interval = DistanceBand.from_array(temperature_data, threshold=distance_interval, binary=True)\n",
    "    # Compute Moran's I\n",
    "    moran = Moran(values, w)\n",
    "    moran_i_values.append(moran.I)\n",
    "    moran_ei_values.append(moran.EI)\n",
    "    moran_vi_values.append(moran.VI_norm)\n",
    "    moran_p_values.append(moran.p_norm)\n",
    "    moran_z_values.append(moran.z_norm)\n",
    "    print('------------------\\n\\n')\n",
    "\n",
    "# Plot Moran's I versus distance\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n",
    "ax = axes.flatten()\n",
    "ax[0].plot(distance_intervals, moran_i_values, marker='o')\n",
    "ax[0].set_xlabel('Spatial lag (km)')\n",
    "ax[0].set_ylabel(\"Moran's I\")\n",
    "ax[0].set_title(\"Moran's I vs Distance\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(distance_intervals, moran_ei_values, marker='o')\n",
    "ax[1].set_xlabel('Spatial lag (km)')\n",
    "ax[1].set_ylabel(\"Moran EI\")\n",
    "ax[1].set_title(\"Moran EI vs Distance\")\n",
    "ax[1].grid()\n",
    "\n",
    "ax[2].plot(distance_intervals, moran_p_values, marker='o')\n",
    "ax[2].set_xlabel('Spatial lag (km)')\n",
    "ax[2].set_ylabel(\"Moran p_norm\")\n",
    "ax[2].set_title(\"Moran's p_norm vs Distance\")\n",
    "ax[2].grid()\n",
    "\n",
    "ax[3].plot(distance_intervals, moran_z_values, marker='o')\n",
    "ax[3].set_xlabel('Spatial lag (km)')\n",
    "ax[3].set_ylabel(\"Moran z_norm\")\n",
    "ax[3].set_title(\"Moran z_norm vs Distance\")\n",
    "ax[3].grid()\n",
    "plt.savefig('moran_testing.png',dpi=200)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f8b48-785d-48bd-bc74-34a8dd3362e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile save_spatial_autocorr_multiprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as crs\n",
    "import random\n",
    "import skgstat as skg\n",
    "plt.style.use('ggplot')\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import read_vars_WRF_RAMS\n",
    "\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "   \n",
    "def read_head(headfile,h5file):\n",
    "        # Function that reads header files from RAMS\n",
    "\n",
    "        # Inputs:\n",
    "        #   headfile: header file including full path in str format\n",
    "        #   h5file: h5 datafile including full path in str format\n",
    "\n",
    "        # Returns:\n",
    "        #   zmn: height levels for momentum values (i.e., grid box upper and lower levels)\n",
    "        #   ztn: height levels for thermodynaic values (i.e., grid box centers)\n",
    "        #   nx:: the number of x points for the domain associated with the h5file\n",
    "        #   ny: the number of y points for the domain associated with the h5file\n",
    "        #   npa: the number of surface patches\n",
    "\n",
    "\n",
    "        dom_num = h5file[h5file.index('.h5')-1] # Find index of .h5 to determine position showing which nest domain to use\n",
    "\n",
    "        with open(headfile) as f:\n",
    "            contents = f.readlines()\n",
    "\n",
    "        idx_zmn = contents.index('__zmn0'+dom_num+'\\n')\n",
    "        nz_m = int(contents[idx_zmn+1])\n",
    "        zmn = np.zeros(nz_m)\n",
    "        for i in np.arange(0,nz_m):\n",
    "            zmn[i] =  float(contents[idx_zmn+2+i])\n",
    "\n",
    "        idx_ztn = contents.index('__ztn0'+dom_num+'\\n')\n",
    "        nz_t = int(contents[idx_ztn+1])\n",
    "        ztn = np.zeros(nz_t)\n",
    "        for i in np.arange(0,nz_t):\n",
    "            ztn[i] =  float(contents[idx_ztn+2+i])\n",
    "\n",
    "        ztop = np.max(ztn) # Model domain top (m)\n",
    "\n",
    "        # Grad the size of the horizontal grid spacing\n",
    "        idx_dxy = contents.index('__deltaxn\\n')\n",
    "        dxy = float(contents[idx_dxy+1+int(dom_num)].strip())\n",
    "\n",
    "        idx_npatch = contents.index('__npatch\\n')\n",
    "        npa = int(contents[idx_npatch+2])\n",
    "\n",
    "        idx_ny = contents.index('__nnyp\\n')\n",
    "        idx_nx = contents.index('__nnxp\\n')\n",
    "        ny = np.ones(int(contents[idx_ny+1]))\n",
    "        nx = np.ones(int(contents[idx_ny+1]))\n",
    "        for i in np.arange(0,len(ny)):\n",
    "            nx[i] = int(contents[idx_nx+2+i])\n",
    "            ny[i] = int(contents[idx_ny+2+i])\n",
    "\n",
    "        ny_out = ny[int(dom_num)-1]\n",
    "        nx_out = nx[int(dom_num)-1]\n",
    "\n",
    "        return zmn, ztn, nx_out, ny_out, dxy, npa \n",
    "\n",
    "simulations=['ARG1.1-R_old','PHI1.1-R','PHI2.1-R','WPO1.1-R','BRA1.1-R','USA1.1-R','DRC1.1-R','AUS1.1-R']\n",
    "domain='3'\n",
    "nsamples=1\n",
    "sample_size = 15000\n",
    "variables = [['Tk', 0, 'model', '$T_{sfc}^{2} (K^{2})$']             , ['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$'],\\\n",
    "             ['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$'], ['RH', 0, 'model', '$RH_{sfc}^{2} (percent^{2})$'],\\\n",
    "             ['U', 0, 'model', '$U_{sfc}^{2} (m^{2}s^{-2})$']        , ['V', 0, 'model', '$V_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "             ['WSPD', 0, 'model', '$WSPD_{sfc}^{2} (m^{2}s^{-2})$']  , ['W', 0, 'model', '$W_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "             ['MCAPE', -999, None, '$MCAPE^{2} (J^{2}kg^{-2})$']     , ['MCIN', -999, None, '$MCIN^{2} (J^{2}kg^{-2})$'], \\\n",
    "             ['Tk', 750, 'pressure', '$T_{750}^{2} (K^{2})$']        , ['THETA', 750, 'pressure', '$Theta_{750}^{2} (K^{2})$'],\\\n",
    "             ['QV', 750, 'pressure', '$Qvapor_{750}^{2} (kg^{2}kg^{-2})$'], ['RH', 750, 'pressure', '$RH_{750}^{2} (percent^{2})$'],\\\n",
    "             ['U', 750, 'pressure', '$U_{750}^{2} (m^{2}s^{-2})$']   , ['V', 750, 'pressure', '$V_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "             ['WSPD', 750, 'pressure', '$WSPD_{750}^{2} (m^{2}s^{-2})$'], ['W', 750, 'pressure', '$W_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "             ['Tk', 500, 'pressure', '$T_{500}^{2} (K^{2})$']        , ['THETA', 500, 'pressure', '$Theta_{500}^{2} (K^{2})$'],\\\n",
    "             ['QV', 500, 'pressure', '$Qvapor_{500}^{2} (kg^{2}kg^{-2})$'], ['RH', 500, 'pressure', '$RH_{500}^{2} (percent^{2})$'],\\\n",
    "             ['U', 500, 'pressure', '$U_{500}^{2} (m^{2}s^{-2})$']   , ['V', 500, 'pressure', '$V_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "             ['WSPD', 500, 'pressure', '$WSPD_{500}^{2} (m^{2}s^{-2})$'], ['W', 500, 'pressure', '$W_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "             ['Tk', 200, 'pressure', '$T_{200}^{2} (K^{2})$']        , ['THETA', 200, 'pressure', '$Theta_{200}^{2} (K^{2})$'],\\\n",
    "             ['QV', 200, 'pressure', '$Qvapor_{200}^{2} (kg^{2}kg^{-2})$'], ['RH', 200, 'pressure', '$RH_{200}^{2} (percent^{2})$'],\\\n",
    "             ['U', 200, 'pressure', '$U_{200}^{2} (m^{2}s^{-2})$']   , ['V', 200, 'pressure', '$V_{200}^{2} (m^{2}s^{-2})$'], \\\n",
    "             ['WSPD', 200, 'pressure', '$WSPD_{200}^{2} (m^{2}s^{-2})$'], ['W', 200, 'pressure', '$W_{200}^{2} (m^{2}s^{-2})$']]\n",
    "\n",
    "colors    =  ['#000000','#E69F00','#56B4E9','#009E73','#F0E442','#0072B2','#D55E00','#CC79A7']\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from libpysal.weights.distance import DistanceBand\n",
    "import libpysal \n",
    "from esda.moran import Moran\n",
    "import random\n",
    "\n",
    "# Generate or load your 2D field data of temperature\n",
    "# Here, I'm assuming you have a 2D array named 'temperature_data'\n",
    "# Replace this with your actual temperature field data\n",
    "dx=0.1\n",
    "temperature_data = np.random.rand(150, 150)  # Example random temperature field data\n",
    "array = np.zeros((150, 150))\n",
    "\n",
    "# Define the center and radius of the circle\n",
    "center_x1 = 120\n",
    "center_y1 = 60\n",
    "radius1 = 7.5\n",
    "\n",
    "center_x2 = 70\n",
    "center_y2 = 100\n",
    "radius2 = 10\n",
    "\n",
    "# Generate indices for the circle\n",
    "y_indices, x_indices = np.ogrid[:150, :150]\n",
    "#circle_mask1 = (x_indices - center_x1)**2 + (y_indices - center_y1)**2 <= radius1**2\n",
    "circle_mask2 = (x_indices - center_x2)**2 + (y_indices - center_y2)**2 <= radius2**2\n",
    "\n",
    "# Set the values inside the circle to 1\n",
    "#array[circle_mask1] = 1\n",
    "array[circle_mask2] = 2\n",
    "temperature_data = temperature_data  + array\n",
    "\n",
    "\n",
    "y_dim, x_dim     = np.shape(temperature_data)\n",
    "print('        shape of the arrays is ',y_dim,'x',x_dim)\n",
    "x      = np.arange(0,x_dim)#*dx\n",
    "y      = np.arange(0,y_dim)#*dx\n",
    "# # full coordinate arrays\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "coords_tuples_2d = np.vstack(([yy.T], [xx.T])).T\n",
    "print('        shape of combined coords matrix: ',np.shape(coords_tuples_2d))\n",
    "coords_all_list = coords_tuples_2d.reshape(-1, 2).tolist()\n",
    "print('        shape of 1d list of coords: ',np.shape(coords_all_list))\n",
    "tuples_all_tuple =  [tuple(sublist) for sublist in coords_all_list]\n",
    "#print(tuples_all_tuple[0:10])\n",
    "#print(type(tuples_all_tuple[0]))\n",
    "\n",
    "sample_coords_tuple = random.sample(tuples_all_tuple,5000)\n",
    "print(sample_coords_tuple[0:10])\n",
    "print('        get field values from these points...')\n",
    "values = np.fromiter((temperature_data[c[0], c[1]] for c in sample_coords_tuple), dtype=float)\n",
    "modified_sample_coords_tuple = [tuple(dx * element for element in tup) for tup in sample_coords_tuple]\n",
    "print(modified_sample_coords_tuple[0:10])\n",
    "#w=libpysal.weights.DistanceBand(sample_coords_tuple,threshold=11.2,binary=False)\n",
    "#w=libpysal.weights.DistanceBand(sample_coords_tuple,threshold=11.2,binary=False,alpha=-2.)\n",
    "distance_intervals = np.arange(0.1,14,1.0)  # Define distance intervals to compute Moran's I\n",
    "moran_i_values  = []\n",
    "moran_ei_values = []\n",
    "moran_p_values  = []\n",
    "moran_z_values  = []\n",
    "moran_vi_values = []\n",
    "for distance_interval in distance_intervals:\n",
    "    print('distance interval : ',distance_interval)\n",
    "    # Create binary spatial weights matrix based on distance interval\n",
    "    w=libpysal.weights.DistanceBand(modified_sample_coords_tuple,threshold=distance_interval,binary=True,silence_warnings=True)\n",
    "    #print(len(w.neighbors))\n",
    "    print('# islands: ',len(w.islands))\n",
    "    print('# max_neighbors: ',(w.max_neighbors))\n",
    "    print('# mean_neighbors: ',(w.mean_neighbors))\n",
    "    print('# min_neighbors: ',(w.min_neighbors))\n",
    "    #print('neighbors : ')\n",
    "    #print(w.neighbors)\n",
    "    #print('weights : ')\n",
    "    #print(w.weights)\n",
    "    #w =DistanceBand (data = temperature_data, threshold = distance_interval)\n",
    "    #w_interval = DistanceBand.from_array(temperature_data, threshold=distance_interval, binary=True)\n",
    "    # Compute Moran's I\n",
    "    moran = Moran(values, w)\n",
    "    moran_i_values.append(moran.I)\n",
    "    moran_ei_values.append(moran.EI)\n",
    "    moran_vi_values.append(moran.VI_norm)\n",
    "    moran_p_values.append(moran.p_norm)\n",
    "    moran_z_values.append(moran.z_norm)\n",
    "    print('------------------\\n\\n')\n",
    "\n",
    "# Plot Moran's I versus distance\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n",
    "ax = axes.flatten()\n",
    "ax[0].plot(distance_intervals, moran_i_values, marker='o')\n",
    "ax[0].set_xlabel('Spatial lag (km)')\n",
    "ax[0].set_ylabel(\"Moran's I\")\n",
    "ax[0].set_title(\"Moran's I vs Distance\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(distance_intervals, moran_ei_values, marker='o')\n",
    "ax[1].set_xlabel('Spatial lag (km)')\n",
    "ax[1].set_ylabel(\"Moran EI\")\n",
    "ax[1].set_title(\"Moran EI vs Distance\")\n",
    "ax[1].grid()\n",
    "\n",
    "ax[2].plot(distance_intervals, moran_p_values, marker='o')\n",
    "ax[2].set_xlabel('Spatial lag (km)')\n",
    "ax[2].set_ylabel(\"Moran p_norm\")\n",
    "ax[2].set_title(\"Moran's p_norm vs Distance\")\n",
    "ax[2].grid()\n",
    "\n",
    "ax[3].plot(distance_intervals, moran_z_values, marker='o')\n",
    "ax[3].set_xlabel('Spatial lag (km)')\n",
    "ax[3].set_ylabel(\"Moran z_norm\")\n",
    "ax[3].set_title(\"Moran z_norm vs Distance\")\n",
    "ax[3].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.grid(True)\n",
    "#plt.show()\n",
    "#values_all = z_temp.flatten()# reshape(-1,0)\n",
    "#print('        shape of flattened values array : ',np.shape(values_all))\n",
    "\n",
    "def save_autocorr(WHICH_TIME, VARIABLE, SIMULATIONS, SAMPLE_SIZE, DOMAIN, NSAMPLES, COLORS, PLOT):\n",
    "    if DOMAIN=='1':\n",
    "        dx = 1.6\n",
    "    if DOMAIN=='2':\n",
    "        dx=0.4\n",
    "    if DOMAIN=='3':\n",
    "        dx=0.1\n",
    "    print('working on ',VARIABLE,'\\n')\n",
    "    if PLOT:\n",
    "        fig    = plt.figure(figsize=(8,8))\n",
    "    for ii,simulation in enumerate(SIMULATIONS):   \n",
    "        print('    working on simulation: ',simulation)\n",
    "        if DOMAIN=='1' or DOMAIN =='2':\n",
    "            rams_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+DOMAIN+'/out/'+'a-A-*g'+DOMAIN+'.h5'))# CSU machine\n",
    "        if DOMAIN=='3':\n",
    "            rams_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+DOMAIN+'/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "        print('        total # files = ',len(rams_files))\n",
    "        print('        first file is ',rams_files[0])\n",
    "        print('        last file is ',rams_files[-1])\n",
    "        if WHICH_TIME=='start':\n",
    "            rams_fil    = rams_files[0]\n",
    "        if WHICH_TIME=='middle':\n",
    "            rams_fil    = rams_files[int(len(rams_files)/2)]\n",
    "        if WHICH_TIME=='end':\n",
    "            rams_fil    = rams_files[-1]\n",
    "        print('        choosing the middle file: ',rams_fil)\n",
    "\n",
    "        da     = xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "        z_temp = np.da['TOPT'].values\n",
    "        y_dim, x_dim     = np.shape(z_temp)\n",
    "        print('        shape of the arrays is ',y_dim,'x',x_dim)\n",
    "        x      = np.arange(0,x_dim)\n",
    "        y      = np.arange(0,y_dim)\n",
    "        # # full coordinate arrays\n",
    "        xx, yy = np.meshgrid(x, y)\n",
    "        coords_tuples_2d = np.vstack(([yy.T], [xx.T])).T\n",
    "        print('        shape of combined coords matrix: ',np.shape(coords_tuples_2d))\n",
    "        coords_all = coords_tuples_2d.reshape(-1, 2).tolist()\n",
    "        print('        shape of 1d list of coords: ',np.shape(coords_all))\n",
    "        values_all = z_temp.flatten()# reshape(-1,0)\n",
    "        print('        shape of flattened values array : ',np.shape(values_all))\n",
    "\n",
    "        z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(rams_fil,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "        print('        got the data... min = ',np.nanmin(z),' max = ',np.nanmax(z))\n",
    "        print('        percentage of nans is ',np.count_nonzero(np.isnan(z))/len(z.flatten()))\n",
    "        print('        choosing '+str(SAMPLE_SIZE)+' random points...')\n",
    "        coords = random.sample(coords_all,SAMPLE_SIZE)\n",
    "        print('        get field values from these points...')\n",
    "        values = np.fromiter((z[c[0], c[1]] for c in coords), dtype=float)\n",
    "        # Remove nan values\n",
    "        print('        Removing nan values and the corresponding coordinates...')\n",
    "        nan_mask = ~np.isnan(values)\n",
    "        print('        # non-nan values',np.count_nonzero(nan_mask))\n",
    "        values   = values[nan_mask]\n",
    "        sampled_coords_array = np.array(coords)\n",
    "        coords   = sampled_coords_array[nan_mask].tolist()\n",
    "        print('        final shape of coords is ',np.shape(coords))\n",
    "        print('        final shape of values is ',np.shape(values))\n",
    "        V        = skg.Variogram(coords, values,n_lags=200,bin_func='even')\n",
    "        print('        creating variogram...')\n",
    "        \n",
    "        bins = V.bins*dx # convert from ineteger coordinates to physical coordinates (km)\n",
    "        exp_variograms =  V.experimental\n",
    "\n",
    "        matrix_for_saving = np.array([bins,exp_variograms]).T\n",
    "        \n",
    "        if VARIABLE[2]:\n",
    "            data_file = 'experimental_variogram_RAMS_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_'+z_time+'_'+simulation+'_1_sample_no_mask_d0'+DOMAIN+'.npy'\n",
    "        else:\n",
    "            data_file = 'experimental_variogram_RAMS_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_'+z_time+'_'+simulation+'_1_sample_no_mask_d0'+DOMAIN+'.npy'\n",
    "\n",
    "        with open(data_file, 'wb') as f:\n",
    "            np.save(f, matrix_for_saving)\n",
    "            np.save(f, matrix_for_saving)\n",
    "\n",
    "        print('        saving variogram data to ',data_file)\n",
    "        print('    ------\\n')\n",
    "        \n",
    "        if PLOT:\n",
    "            plt.plot(bins,exp_variograms,label=simulation, color=COLORS[ii])\n",
    "        \n",
    "    if PLOT:\n",
    "        if VARIABLE[2]:\n",
    "            title_string = 'Variogram for '+VARIABLE[0]+' at '+VARIABLE[2]+' level '+str(int(VARIABLE[1]))+' for d0'+DOMAIN+'\\nmid-simulation'\n",
    "        else:\n",
    "            title_string = 'Variogram for '+VARIABLE[0]+' for d0'+DOMAIN+'\\nmid-simulation'    \n",
    "        plt.title(title_string)\n",
    "        plt.xlabel('distance (km)')\n",
    "        plt.ylabel(VARIABLE[3])\n",
    "        plt.legend()\n",
    "        if VARIABLE[2]:\n",
    "            filename = 'experimental_variogram_8_simulations_RAMS_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_1_sample_no_mask_d0'+DOMAIN+'_mid-simulation.png'\n",
    "        else:\n",
    "            filename = 'experimental_variogram_8_simulations_RAMS_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_1_sample_no_mask_d0'+DOMAIN+'_mid-simulation.png'\n",
    "        print('saving to file: ',filename)\n",
    "        plt.savefig(filename,dpi=150)\n",
    "        print('\\n\\n')\n",
    "\n",
    "print('working on domain' ,domain)\n",
    "#Running on the terminal in parallel\n",
    "argument = []\n",
    "for var in variables:\n",
    "    argument = argument + [('end',var, simulations, sample_size, domain, nsamples, colors, True)]\n",
    "\n",
    "print('length of argument is: ',len(argument))\n",
    "\n",
    "\n",
    "# # ############################### FIRST OF ALL ################################\n",
    "cpu_count1 = 37 #cpu_count()\n",
    "print('number of cpus: ',cpu_count1)\n",
    "# # #############################################################################\n",
    "\n",
    "def main(FUNCTION, ARGUMENT):\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(processes = (cpu_count1-1)) as pool:\n",
    "        data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "    #df_all = pd.concat(data, ignore_index=True)\n",
    "    #thermo_indices_data_csv_file = csv_folder+'thermodynamic_indices_' + DOMAIN + '_comb_track_filt_01_02_50_02_sr5017_setpos.csv'\n",
    "    #print('saving thermodynamic indices to the file: ',thermo_indices_data_csv_file)\n",
    "    #df_all.to_csv(thermo_indices_data_csv_file)  # sounding data\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main(save_variogram, argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc103a-067a-4638-bfbc-c1e45502755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for autocorr calculation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as crs\n",
    "import random\n",
    "import skgstat as skg\n",
    "plt.style.use('ggplot')\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import read_vars_WRF_RAMS\n",
    "\n",
    "z_temp = np.random.rand(150, 150) \n",
    "y_dim, x_dim     = np.shape(z_temp)\n",
    "print('        shape of the arrays is ',y_dim,'x',x_dim)\n",
    "x      = np.arange(0,x_dim)\n",
    "y      = np.arange(0,y_dim)\n",
    "# # full coordinate arrays\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "coords_tuples_2d = np.vstack(([yy.T], [xx.T])).T\n",
    "print('        shape of combined coords matrix: ',np.shape(coords_tuples_2d))\n",
    "coords_all = coords_tuples_2d.reshape(-1, 2).tolist()\n",
    "print('        shape of 1d list of coords: ',np.shape(coords_all))\n",
    "values_all = z_temp.flatten()# reshape(-1,0)\n",
    "print('        shape of flattened values array : ',np.shape(values_all))\n",
    "z = z_temp # z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(rams_fil,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "print('        got the data... min = ',np.nanmin(z),' max = ',np.nanmax(z))\n",
    "print('        percentage of nans is ',np.count_nonzero(np.isnan(z))/len(z.flatten()))\n",
    "print('        choosing '+str(1000)+' random points...')\n",
    "coords = random.sample(coords_all,1000)\n",
    "print('        get field values from these points...')\n",
    "values = np.fromiter((z[c[0], c[1]] for c in coords), dtype=float)\n",
    "# Remove nan values\n",
    "print('        Removing nan values and the corresponding coordinates...')\n",
    "nan_mask = ~np.isnan(values)\n",
    "print('        # non-nan values',np.count_nonzero(nan_mask))\n",
    "values   = values[nan_mask]\n",
    "sampled_coords_array = np.array(coords)\n",
    "coords   = sampled_coords_array[nan_mask].tolist()\n",
    "print('        final shape of coords is ',np.shape(coords))\n",
    "print('first 10 coords are: ',coords[0:10])\n",
    "print('        final shape of values is ',np.shape(values))\n",
    "V        = skg.Variogram(coords, values,n_lags=80,bin_func='even')\n",
    "print('        creating variogram...')\n",
    "\n",
    "bins = V.bins # convert from integer coordinates to physical coordinates (km)\n",
    "exp_variograms =  V.experimental\n",
    "print('bins are : ',bins, 'with shape ',np.shape(bins))\n",
    "print('--------\\n')\n",
    "print('experimental variogram is :',exp_variograms ,' with shape ',np.shape(exp_variograms))\n",
    "print('--------\\n')\n",
    "print('empirical variogram: ',V.get_empirical())\n",
    "print('--------\\n')\n",
    "print('maxlag: ',V.maxlag)\n",
    "print('--------\\n')\n",
    "#print('fit bounds: ',V.__get_fit_bounds())\n",
    "print('--------\\n')\n",
    "print('data: ',V.data()) # theoretical model \n",
    "print('--------\\n')\n",
    "print('model_residuals: ',V.model_residuals) # theoretical model \n",
    "print('--------\\n')\n",
    "print('parameters: range, sill, nugget are ',V.parameters)\n",
    "print('----plot----\\n')\n",
    "V.plot()\n",
    "print('---scattergram-----\\n')\n",
    "#V.scattergram()\n",
    "print('---distance_difference_plot-----\\n')\n",
    "V.distance_difference_plot()\n",
    "print('--------\\n')\n",
    "print(' all pairwise differences shape :',np.shape(V.pairwise_diffs))\n",
    "print('--------\\n')\n",
    "print(' all distance shape :',np.shape(V.distance))\n",
    "print('--------\\n')\n",
    "print(' _calc_groups :',V._calc_groups())\n",
    "print('--------\\n')\n",
    "print(' lag_classes :',V.lag_classes())\n",
    "print('--------\\n') \n",
    "print(' values shape :',np.shape (V.values))\n",
    "print('--------\\n') \n",
    "print(' _diff :',np.shape (V._diff))\n",
    "print('--------\\n') \n",
    "print(' lag_groups :',V.lag_groups(), ' with shape: ',np.shape(V.lag_groups()))\n",
    "print('min lag group: ',min(V.lag_groups()), 'max lag group: ',max(V.lag_groups()))\n",
    "print('unique lag groups ',np.unique(V.lag_groups()))\n",
    "print('--------\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d363e4-d1ac-4de2-be69-eaf92183f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "blah = pdist(np.column_stack((V.values, np.zeros(len(V.values)))),metric=\"euclidean\")\n",
    "print(np.shape(blah))\n",
    "print(blah[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bff8a4-4c11-40ab-b94c-c5e9ec8f4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate or load your 2D field data\n",
    "# Here, I'm assuming you have a 2D array named 'field_data'\n",
    "# Replace this with your actual field data\n",
    "field_data = np.random.rand(50, 50)  # Example random field data\n",
    "\n",
    "# Define the maximum lag distance to consider\n",
    "maxlag = min(field_data.shape) // 2\n",
    "\n",
    "# Calculate spatial autocorrelation for different lags\n",
    "spatial_autocorr = []\n",
    "\n",
    "for lag in range(1, maxlag + 1):\n",
    "    autocorr = np.corrcoef(field_data[:-lag, :-lag].flatten(), field_data[lag:, lag:].flatten())[0, 1]\n",
    "    spatial_autocorr.append(autocorr)\n",
    "\n",
    "# Plot spatial autocorrelation with lag\n",
    "plt.plot(range(1, maxlag + 1), spatial_autocorr, marker='o')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Spatial Autocorrelation')\n",
    "plt.title('Spatial Autocorrelation vs Lag')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25833718-0859-4b69-b3a6-b274bf900817",
   "metadata": {},
   "source": [
    "### Plot variograms from saved .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7903dec-f369-4dd8-911a-61121d454831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "simulations=['PHI1.1-R','PHI2.1-R','WPO1.1-R','BRA1.1-R','USA1.1-R','DRC1.1-R','AUS1.1-R']\n",
    "variables = [['Tk', 0, 'model', '$T_{sfc}^{2} (K^{2})$']             , ['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$'],\\\n",
    "             ['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$'], ['RH', 0, 'model', '$RH_{sfc}^{2} (percent^{2})$'],\\\n",
    "             ['U', 0, 'model', '$U_{sfc}^{2} (m^{2}s^{-2})$']        , ['V', 0, 'model', '$V_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "             ['WSPD', 0, 'model', '$WSPD_{sfc}^{2} (m^{2}s^{-2})$']  , ['W', 0, 'model', '$W_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "             ['MCAPE', -999, None, '$MCAPE^{2} (J^{2}kg^{-2})$']     , ['MCIN', -999, None, '$MCIN^{2} (J^{2}kg^{-2})$'], \\\n",
    "             ['Tk', 750, 'pressure', '$T_{750}^{2} (K^{2})$']        , ['THETA', 750, 'pressure', '$Theta_{750}^{2} (K^{2})$'],\\\n",
    "             ['QV', 750, 'pressure', '$Qvapor_{750}^{2} (kg^{2}kg^{-2})$'], ['RH', 750, 'pressure', '$RH_{750}^{2} (percent^{2})$'],\\\n",
    "             ['U', 750, 'pressure', '$U_{750}^{2} (m^{2}s^{-2})$']   , ['V', 750, 'pressure', '$V_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "             ['WSPD', 750, 'pressure', '$WSPD_{750}^{2} (m^{2}s^{-2})$'], ['W', 750, 'pressure', '$W_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "             ['Tk', 500, 'pressure', '$T_{500}^{2} (K^{2})$']        , ['THETA', 500, 'pressure', '$Theta_{500}^{2} (K^{2})$'],\\\n",
    "             ['QV', 500, 'pressure', '$Qvapor_{500}^{2} (kg^{2}kg^{-2})$'], ['RH', 500, 'pressure', '$RH_{500}^{2} (percent^{2})$'],\\\n",
    "             ['U', 500, 'pressure', '$U_{500}^{2} (m^{2}s^{-2})$']   , ['V', 500, 'pressure', '$V_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "             ['WSPD', 500, 'pressure', '$WSPD_{500}^{2} (m^{2}s^{-2})$'], ['W', 500, 'pressure', '$W_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "             ['Tk', 200, 'pressure', '$T_{200}^{2} (K^{2})$']        , ['THETA', 200, 'pressure', '$Theta_{200}^{2} (K^{2})$'],\\\n",
    "             ['QV', 200, 'pressure', '$Qvapor_{200}^{2} (kg^{2}kg^{-2})$'], ['RH', 200, 'pressure', '$RH_{200}^{2} (percent^{2})$'],\\\n",
    "             ['U', 200, 'pressure', '$U_{200}^{2} (m^{2}s^{-2})$']   , ['V', 200, 'pressure', '$V_{200}^{2} (m^{2}s^{-2})$'], \\\n",
    "             ['WSPD', 200, 'pressure', '$WSPD_{200}^{2} (m^{2}s^{-2})$'], ['W', 200, 'pressure', '$W_{200}^{2} (m^{2}s^{-2})$']]\n",
    "colors    = ['#7F7F7F','#E69F00','#56B4E9','#009E73','#F0E442','#0072B2','#D55E00','#CC79A7']\n",
    "color_dict = {'ARG1.1-R_old':'#000000',\\\n",
    "              'PHI1.1-R':'#E69F00',\\\n",
    "              'PHI2.1-R':'#56B4E9',\\\n",
    "              'WPO1.1-R':'#009E73',\\\n",
    "              'BRA1.1-R':'#7F7F7F',\\\n",
    "              'USA1.1-R':'#0072B2',\\\n",
    "              'DRC1.1-R':'#D55E00',\\\n",
    "              'AUS1.1-R':'#CC79A7'}\n",
    "linestyles = ['-', '--', ':']\n",
    "domain='3'\n",
    "\n",
    "for variable in variables:\n",
    "    print('variable: ',variable[0],variable[1],variable[2])\n",
    "    #print('simulation: ',simulation)\n",
    "    if variable[2]:\n",
    "        data_files = glob.glob('./variogram_data/experimental_variogram_RAMS_'+variable[0]+'_levtype_'+variable[2]+'_lev_'+str(int(variable[1]))+'_*_1_sample_no_mask_d0'+domain+'.npy')\n",
    "    else:\n",
    "        data_files =  glob.glob('./variogram_data/experimental_variogram_RAMS_'+variable[0]+'_levtype_'+'None'+'_lev_'+'None_*_1_sample_no_mask_d0'+domain+'.npy')\n",
    "    #print(len(data_files),'\\n')\n",
    "    #print(data_files)\n",
    "    #print('---------\\n')\n",
    "    #if len(data_files)<8:\n",
    "    #    print(variable)#,': ',len(data_files))\n",
    "    plt.figure(figsize=(11,8))\n",
    "    ax = plt.gca()\n",
    "    #temp_list = []\n",
    "    for ii, fil in enumerate(sorted(data_files)):\n",
    "        fil_name = Path(fil).name\n",
    "        #print(fil_name)\n",
    "        simulation_name = fil_name.split('_')[9]\n",
    "        timestep        = fil_name.split('_')[8]\n",
    "        timestep_pd     = pd.to_datetime(timestep,format='%Y%m%d%H%M%S')\n",
    "        print('simulation_name: ',simulation_name)\n",
    "        print('time: ',timestep_pd)\n",
    "        vario_data = np.load(fil)\n",
    "        bins = vario_data[:,0][vario_data[:,0]<=220.]\n",
    "        exp_vario = vario_data[:,1][vario_data[:,0]<=220.]\n",
    "        plt.plot(bins,exp_vario,label=simulation_name+' '+timestep_pd.strftime('%H:%M:%S'),color=color_dict[simulation_name],linestyle=linestyles[ii % len(linestyles)])\n",
    "    if variable[2]:\n",
    "        title_string = 'Variogram for '+variable[0]+' at '+variable[2]+' level '+str(int(variable[1]))+' for d0'+domain#+'\\nmid-simulation'\n",
    "    else:\n",
    "        title_string = 'Variogram for '+variable[0]+' for d0'+domain#+'\\nmid-simulation'    \n",
    "    plt.title(title_string)\n",
    "    plt.xlabel('distance (km)')\n",
    "    plt.ylabel(variable[3])\n",
    "    #plt.legend(prop = { \"size\": 9 })\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    if variable[2]:\n",
    "            filename = 'experimental_variogram_7_simulations_RAMS_'+variable[0]+'_levtype_'+variable[2]+'_lev_'+str(int(variable[1]))+'_1_sample_no_mask_d0'+domain+'.png'\n",
    "    else:\n",
    "            filename = 'experimental_variogram_7_simulations_RAMS_'+variable[0]+'_levtype_'+'None'+'_lev_'+'None'+'_1_sample_no_mask_d0'+domain+'.png'\n",
    "    print('saving to file: ',filename)\n",
    "    plt.savefig(filename,dpi=150,bbox_inches='tight')\n",
    "    #print(sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+'3'+'/out_30s/'+'a-L-*g3.h5')))\n",
    "    print('---------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8dbe0-7826-4bed-9d9f-8f589f484603",
   "metadata": {},
   "source": [
    "## Variograms with masked domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc5d0bb-2638-4f5d-82ec-74ebfedfaddb",
   "metadata": {},
   "source": [
    "### Create and save storm/environment masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0980f7-1c7b-4e4f-bd52-6158cd335ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile save_masks_for_variogram_multiprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as crs\n",
    "import random\n",
    "plt.style.use('ggplot')\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import read_vars_WRF_RAMS\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "   \n",
    "def read_head(headfile,h5file):\n",
    "        # Function that reads header files from RAMS\n",
    "\n",
    "        # Inputs:\n",
    "        #   headfile: header file including full path in str format\n",
    "        #   h5file: h5 datafile including full path in str format\n",
    "\n",
    "        # Returns:\n",
    "        #   zmn: height levels for momentum values (i.e., grid box upper and lower levels)\n",
    "        #   ztn: height levels for thermodynaic values (i.e., grid box centers)\n",
    "        #   nx:: the number of x points for the domain associated with the h5file\n",
    "        #   ny: the number of y points for the domain associated with the h5file\n",
    "        #   npa: the number of surface patches\n",
    "\n",
    "\n",
    "        dom_num = h5file[h5file.index('.h5')-1] # Find index of .h5 to determine position showing which nest domain to use\n",
    "\n",
    "        with open(headfile) as f:\n",
    "            contents = f.readlines()\n",
    "\n",
    "        idx_zmn = contents.index('__zmn0'+dom_num+'\\n')\n",
    "        nz_m = int(contents[idx_zmn+1])\n",
    "        zmn = np.zeros(nz_m)\n",
    "        for i in np.arange(0,nz_m):\n",
    "            zmn[i] =  float(contents[idx_zmn+2+i])\n",
    "\n",
    "        idx_ztn = contents.index('__ztn0'+dom_num+'\\n')\n",
    "        nz_t = int(contents[idx_ztn+1])\n",
    "        ztn = np.zeros(nz_t)\n",
    "        for i in np.arange(0,nz_t):\n",
    "            ztn[i] =  float(contents[idx_ztn+2+i])\n",
    "\n",
    "        ztop = np.max(ztn) # Model domain top (m)\n",
    "\n",
    "        # Grad the size of the horizontal grid spacing\n",
    "        idx_dxy = contents.index('__deltaxn\\n')\n",
    "        dxy = float(contents[idx_dxy+1+int(dom_num)].strip())\n",
    "\n",
    "        idx_npatch = contents.index('__npatch\\n')\n",
    "        npa = int(contents[idx_npatch+2])\n",
    "\n",
    "        idx_ny = contents.index('__nnyp\\n')\n",
    "        idx_nx = contents.index('__nnxp\\n')\n",
    "        ny = np.ones(int(contents[idx_ny+1]))\n",
    "        nx = np.ones(int(contents[idx_ny+1]))\n",
    "        for i in np.arange(0,len(ny)):\n",
    "            nx[i] = int(contents[idx_nx+2+i])\n",
    "            ny[i] = int(contents[idx_ny+2+i])\n",
    "\n",
    "        ny_out = ny[int(dom_num)-1]\n",
    "        nx_out = nx[int(dom_num)-1]\n",
    "\n",
    "        return zmn, ztn, nx_out, ny_out, dxy, npa \n",
    "\n",
    "simulations=['USA1.1-R','ARG1.1-R_old','PHI1.1-R','PHI2.1-R','WPO1.1-R','BRA1.1-R','DRC1.1-R','AUS1.1-R']\n",
    "domain='3'\n",
    "\n",
    "def save_storm_mask_ITC(simulation_name):\n",
    "    Cp=1004.\n",
    "    Rd=287.0\n",
    "    p00 = 100000.0\n",
    "    path = '/monsoon/MODEL/LES_MODEL_DATA/'+simulation_name+'/G3/out_30s/'\n",
    "    # Grab all the rams files \n",
    "    h5filepath = path+'a-L*g3.h5'\n",
    "    h5files1 = sorted(glob.glob(h5filepath))\n",
    "    hefilepath = path+'a-L*head.txt'\n",
    "    hefiles1 = sorted(glob.glob(hefilepath))\n",
    "    print('    first file: ',h5files1[0])\n",
    "    print('    last file: ',h5files1[-1])\n",
    "    middle_file = h5files1[int(len(h5files1)/2)]\n",
    "    timestr = get_time_from_RAMS_file(middle_file)[1]\n",
    "    da=xr.open_dataset(middle_file,engine='h5netcdf', phony_dims='sort')\n",
    "    domain_z_dim,domain_y_dim,domain_x_dim=np.shape(da.WP)\n",
    "    print('    ',domain_z_dim)\n",
    "    print('    ',domain_y_dim)\n",
    "    print('    ',domain_x_dim)\n",
    "\n",
    "    zm, zt, nx, ny, dxy, npa = read_head(hefiles1[0],h5files1[0])\n",
    "\n",
    "    print('    calculating ITC...')\n",
    "    total_condensate = da['RTP']-da['RV']\n",
    "    # Load variables needed to calculate density\n",
    "    th = da['THETA']\n",
    "    nx = np.shape(th)[2]\n",
    "    ny = np.shape(th)[1]\n",
    "    pi = da['PI']\n",
    "    rv = da['RV']\n",
    "    # Convert RAMS native variables to temperature and pressure\n",
    "    pres = np.power((pi/Cp),Cp/Rd)*p00\n",
    "    temp = th*(pi/Cp)\n",
    "    del(th,pi)\n",
    "    # Calculate atmospheric density\n",
    "    dens = pres/(Rd*temp*(1+0.61*rv))\n",
    "    del(pres,temp,rv)\n",
    "    # Difference in heights (dz)    \n",
    "    diff_zt_3D = np.tile(np.diff(zt),(int(ny),int(nx),1))\n",
    "    diff_zt_3D = np.moveaxis(diff_zt_3D,2,0)\n",
    "    itc                       = np.nansum(total_condensate[1:,:,:]*dens[1:,:,:]*diff_zt_3D,axis=0) \n",
    "    output_var                = itc/997.0*1000 # integrated total frozen condensate in mm\n",
    "    output_var_mod = np.where(output_var > 2, 1 , np.nan)\n",
    "    plt.imshow(output_var_mod,cmap='GnBu')\n",
    "    plt.colorbar()\n",
    "    print('    done estimating ITC for ',simulation_name)\n",
    "    output_var_mask = np.where(output_var > 2.0, 1.0 , np.nan)\n",
    "    storm_mask_filename = 'storm_mask_'+simulation_name+'_'+timestr+'.npy'\n",
    "    print('    saving the storm mask to ',storm_mask_filename)\n",
    "    with open(storm_mask_filename, 'wb') as f:\n",
    "        np.save(f, output_var_mask)\n",
    "    return\n",
    "    \n",
    "def save_storm_mask_surface_precip_w(WHICH_TIME, SIMULATIONS, DOMAIN, PLOT):\n",
    "    print('creating a storm mask')\n",
    "    if DOMAIN=='1':\n",
    "        dx = 1.6\n",
    "    if DOMAIN=='2':\n",
    "        dx=0.4\n",
    "    if DOMAIN=='3':\n",
    "        dx=0.1\n",
    "    \n",
    "    for ii,simulation in enumerate(SIMULATIONS):   \n",
    "        print('    working on simulation: ',simulation)\n",
    "        if DOMAIN=='1' or DOMAIN =='2':\n",
    "            rams_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+DOMAIN+'/out/'+'a-A-*g'+DOMAIN+'.h5'))# CSU machine\n",
    "        if DOMAIN=='3':\n",
    "            rams_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+DOMAIN+'/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "        print('        total # files = ',len(rams_files))\n",
    "        print('        first file is ',rams_files[0])\n",
    "        print('        last file is ',rams_files[-1])\n",
    "        if WHICH_TIME=='start':\n",
    "            rams_fil    = rams_files[0]\n",
    "        if WHICH_TIME=='middle':\n",
    "            rams_fil    = rams_files[int(len(rams_files)/2)]\n",
    "        if WHICH_TIME=='end':\n",
    "            rams_fil    = rams_files[-1]\n",
    "        print('        choosing the middle file: ',rams_fil)\n",
    "       \n",
    "        da     = xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "        precip = da['PCPRR'].values  # kgm**-2\n",
    "        #print('        shape of precip rate is ',np.shape(z))\n",
    "        w      = da['WP'].max(dim='phony_dim_3')\n",
    "        print('        precip rate min = ',np.nanmin(precip),' max = ',np.nanmax(precip))\n",
    "        #print('        max w is ',np.nanmax(w))\n",
    "        # create a mask\n",
    "        output_var_mask = np.where(precip > 0.0001, 1.0 , np.nan)#*np.where(qtc > 0.0001, 1.0 , np.nan)\n",
    "        storm_mask_filename = 'storm_mask_precip_'+simulation+'_'+get_time_from_RAMS_file(rams_fil)[1]+'.npy'\n",
    "        print('        saving the storm mask to ',storm_mask_filename)\n",
    "        with open(storm_mask_filename, 'wb') as f:\n",
    "            np.save(f, output_var_mask)\n",
    "         \n",
    "        if PLOT:\n",
    "            fig    = plt.figure(figsize=(8,8))\n",
    "            mask_contours = plt.imshow(output_var_mask)#,cmap='GnBu')\n",
    "            plt.contour(w,levels=np.arange(5,60,10),colors='k')\n",
    "            timestep_string = get_time_from_RAMS_file(rams_fil)[0]\n",
    "            #timestep_string     = pd.to_datetime(z_time,format='%Y%m%d%H%M%S').strftime('%Y-%m-%d %H:%M:%S')\n",
    "            plt.title('storm_mask_surface_precip'+simulation+'\\n'+timestep_string)\n",
    "            plt.colorbar(mask_contours)\n",
    "            plt.savefig('storm_mask_surface_precip'+simulation+'_'+get_time_from_RAMS_file(rams_fil)[1]+'.png')\n",
    "\n",
    "def save_storm_mask_upper_level_precip_w_qtc(WHICH_TIME, LEVEL, LEVEL_TYPE, SIMULATIONS, DOMAIN, PLOT, MASK_CRITERIA):\n",
    "    print('creating a storm mask for ',LEVEL_TYPE,' level ',LEVEL)\n",
    "    if DOMAIN=='1':\n",
    "        dx = 1.6\n",
    "    if DOMAIN=='2':\n",
    "        dx=0.4\n",
    "    if DOMAIN=='3':\n",
    "        dx=0.1\n",
    "    \n",
    "    for ii,simulation in enumerate(SIMULATIONS):   \n",
    "        print('    working on simulation: ',simulation)\n",
    "        if DOMAIN=='1' or DOMAIN =='2':\n",
    "            rams_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+DOMAIN+'/out/'+'a-A-*g'+DOMAIN+'.h5'))# CSU machine\n",
    "        if DOMAIN=='3':\n",
    "            rams_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+DOMAIN+'/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "        print('        total # files = ',len(rams_files))\n",
    "        print('        first file is ',rams_files[0])\n",
    "        print('        last file is ',rams_files[-1])\n",
    "        if WHICH_TIME=='start':\n",
    "            rams_fil    = rams_files[0]\n",
    "        if WHICH_TIME=='middle':\n",
    "            rams_fil    = rams_files[int(len(rams_files)/2)]\n",
    "        if WHICH_TIME=='end':\n",
    "            rams_fil    = rams_files[-1]\n",
    "        print('        choosing the ',WHICH_TIME,' file: ',rams_fil)\n",
    "       \n",
    "        w, w_name, w_units, w_time = read_vars_WRF_RAMS.read_variable(rams_fil,'W','RAMS',output_height=False,interpolate=True,level=LEVEL,interptype=LEVEL_TYPE)\n",
    "        qtc, qtc_name, qtc_units, qtc_time = read_vars_WRF_RAMS.read_variable(rams_fil,'QTC','RAMS',output_height=False,interpolate=True,level=LEVEL,interptype=LEVEL_TYPE)\n",
    "        #precip, precip_name, precip_units, precip_time = read_vars_WRF_RAMS.read_variable(rams_fil,'PCP_RATE_3D','RAMS',output_height=False,interpolate=True,level=LEVEL,interptype=LEVEL_TYPE)\n",
    "        print('        w min = ',np.nanmin(w),' max = ',np.nanmax(w))\n",
    "        print('        qtc min = ',np.nanmin(qtc),' max = ',np.nanmax(qtc))\n",
    "        #print('        precip min = ',np.nanmin(precip),' max = ',np.nanmax(precip))\n",
    "        # create a mask\n",
    "        \n",
    "        if MASK_CRITERIA=='qtc_0.00001_w_2':\n",
    "            output_var_mask = np.where(qtc > 0.00001, 1.0 , np.nan)*np.where(w > 2.0, 1.0 , np.nan)\n",
    "            storm_mask_filename = 'storm_mask_'+MASK_CRITERIA+'_'+LEVEL_TYPE+'_level_'+str(int(LEVEL))+'_'+simulation+'_'+get_time_from_RAMS_file(rams_fil)[1]+'.npy'\n",
    "            print('        saving the storm mask to ',storm_mask_filename)\n",
    "            with open(storm_mask_filename, 'wb') as f:\n",
    "                np.save(f, output_var_mask)\n",
    "        if MASK_CRITERIA=='qtc_0.00001_w_1':\n",
    "            output_var_mask = np.where(qtc > 0.00001, 1.0 , np.nan)*np.where(w > 1.0, 1.0 , np.nan)\n",
    "            storm_mask_filename = 'storm_mask_'+MASK_CRITERIA+'_'+LEVEL_TYPE+'_level_'+str(int(LEVEL))+'_'+simulation+'_'+get_time_from_RAMS_file(rams_fil)[1]+'.npy'\n",
    "            print('        saving the storm mask to ',storm_mask_filename)\n",
    "            with open(storm_mask_filename, 'wb') as g:\n",
    "                np.save(g, output_var_mask)\n",
    "        if MASK_CRITERIA=='qtc_0.00001':\n",
    "            output_var_mask = np.where(qtc > 0.00001, 1.0 , np.nan)\n",
    "            storm_mask_filename = 'storm_mask_'+MASK_CRITERIA+'_'+LEVEL_TYPE+'_level_'+str(int(LEVEL))+'_'+simulation+'_'+get_time_from_RAMS_file(rams_fil)[1]+'.npy'\n",
    "            print('        saving the storm mask to ',storm_mask_filename)\n",
    "            with open(storm_mask_filename, 'wb') as h:\n",
    "                np.save(h, output_var_mask)\n",
    "\n",
    "        if PLOT:\n",
    "            fig    = plt.figure(figsize=(8,8))\n",
    "            mask_contours = plt.imshow(output_var_mask)#,cmap='GnBu')\n",
    "            plt.contour(w,levels=np.arange(5,60,10),colors='k')\n",
    "            timestep_string = get_time_from_RAMS_file(rams_fil)[0]\n",
    "            #timestep_string     = pd.to_datetime(z_time,format='%Y%m%d%H%M%S').strftime('%Y-%m-%d %H:%M:%S')\n",
    "            plt.title('storm_mask: '+MASK_CRITERIA+' ;'+simulation+'\\n'+timestep_string)\n",
    "            plt.savefig('storm_mask_'+MASK_CRITERIA+'_'+simulation+'_'+w_time+'.png',dpi=150)\n",
    "            plt.colorbar(mask_contours)\n",
    " \n",
    "#save_storm_mask_surface_precip_w('end',simulations,domain,False)\n",
    "\n",
    "# for pressure_lev in [750,500,200]:\n",
    "#     #for mask_type in ['qtc_0.00001_w_2','qtc_0.00001_w_1','qtc_0.00001']:\n",
    "#     for mask_criteria in ['qtc_0.00001']:\n",
    "#         for sim_time in ['start','end']:\n",
    "#             save_storm_mask_upper_level_precip_w_qtc('start', pressure_lev, 'pressure', simulations, domain, True, mask_criteria)\n",
    "#             print('-----------\\n\\n')\n",
    "\n",
    "#save_storm_mask_surface_precip_w('middle', simulations, domain, True)\n",
    "# sims=['RSA1.1-R']\n",
    "# for sim in sims:\n",
    "#     print('working on simulation: ',sim)\n",
    "#     save_storm_mask(sim)\n",
    "\n",
    "#save_variogram_masked_domain('not_near_storm','middle',variables[0], simulations, sample_size, domain, nsamples, colors, True)                                        \n",
    "                                         \n",
    "print('working on domain' ,domain)\n",
    "#Running on the terminal in parallel\n",
    "argument = []\n",
    "for sim_time in ['start','middle','end']:\n",
    "    argument = argument + [(sim_time, simulations, domain, False)]\n",
    "\n",
    "print('length of argument is: ',len(argument))\n",
    "\n",
    "\n",
    "# # ############################### FIRST OF ALL ################################\n",
    "cpu_count1 = 37 #cpu_count()\n",
    "print('number of cpus: ',cpu_count1)\n",
    "# # #############################################################################\n",
    "\n",
    "def main(FUNCTION, ARGUMENT):\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(processes = (cpu_count1-1)) as pool:\n",
    "        data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "    #df_all = pd.concat(data, ignore_index=True)\n",
    "    #thermo_indices_data_csv_file = csv_folder+'thermodynamic_indices_' + DOMAIN + '_comb_track_filt_01_02_50_02_sr5017_setpos.csv'\n",
    "    #print('saving thermodynamic indices to the file: ',thermo_indices_data_csv_file)\n",
    "    #df_all.to_csv(thermo_indices_data_csv_file)  # sounding data\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main(save_storm_mask_surface_precip_w, argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9216e57-4a75-433d-bebc-ca31868fdee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot masks\n",
    "temp_file_array = np.load('storm_mask_BRA1.1-R_20140331190000.npy')\n",
    "mask            = np.where((temp_file_array < 1.1) & (temp_file_array > 0.9), np.nan , 1.0)\n",
    "plt.imshow(temp_file_array)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8102e-4bd4-4712-aa20-003e37c9868d",
   "metadata": {},
   "source": [
    "### Create variograms for masked domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d290e6-3487-4a2c-af45-e3a2a4f66eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile save_variogram_masked_domain_multiprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as crs\n",
    "import random\n",
    "import skgstat as skg\n",
    "plt.style.use('ggplot')\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import read_vars_WRF_RAMS\n",
    "\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "   \n",
    "def read_head(headfile,h5file):\n",
    "        # Function that reads header files from RAMS\n",
    "\n",
    "        # Inputs:\n",
    "        #   headfile: header file including full path in str format\n",
    "        #   h5file: h5 datafile including full path in str format\n",
    "\n",
    "        # Returns:\n",
    "        #   zmn: height levels for momentum values (i.e., grid box upper and lower levels)\n",
    "        #   ztn: height levels for thermodynaic values (i.e., grid box centers)\n",
    "        #   nx:: the number of x points for the domain associated with the h5file\n",
    "        #   ny: the number of y points for the domain associated with the h5file\n",
    "        #   npa: the number of surface patches\n",
    "\n",
    "\n",
    "        dom_num = h5file[h5file.index('.h5')-1] # Find index of .h5 to determine position showing which nest domain to use\n",
    "\n",
    "        with open(headfile) as f:\n",
    "            contents = f.readlines()\n",
    "\n",
    "        idx_zmn = contents.index('__zmn0'+dom_num+'\\n')\n",
    "        nz_m = int(contents[idx_zmn+1])\n",
    "        zmn = np.zeros(nz_m)\n",
    "        for i in np.arange(0,nz_m):\n",
    "            zmn[i] =  float(contents[idx_zmn+2+i])\n",
    "\n",
    "        idx_ztn = contents.index('__ztn0'+dom_num+'\\n')\n",
    "        nz_t = int(contents[idx_ztn+1])\n",
    "        ztn = np.zeros(nz_t)\n",
    "        for i in np.arange(0,nz_t):\n",
    "            ztn[i] =  float(contents[idx_ztn+2+i])\n",
    "\n",
    "        ztop = np.max(ztn) # Model domain top (m)\n",
    "\n",
    "        # Grad the size of the horizontal grid spacing\n",
    "        idx_dxy = contents.index('__deltaxn\\n')\n",
    "        dxy = float(contents[idx_dxy+1+int(dom_num)].strip())\n",
    "\n",
    "        idx_npatch = contents.index('__npatch\\n')\n",
    "        npa = int(contents[idx_npatch+2])\n",
    "\n",
    "        idx_ny = contents.index('__nnyp\\n')\n",
    "        idx_nx = contents.index('__nnxp\\n')\n",
    "        ny = np.ones(int(contents[idx_ny+1]))\n",
    "        nx = np.ones(int(contents[idx_ny+1]))\n",
    "        for i in np.arange(0,len(ny)):\n",
    "            nx[i] = int(contents[idx_nx+2+i])\n",
    "            ny[i] = int(contents[idx_ny+2+i])\n",
    "\n",
    "        ny_out = ny[int(dom_num)-1]\n",
    "        nx_out = nx[int(dom_num)-1]\n",
    "\n",
    "        return zmn, ztn, nx_out, ny_out, dxy, npa \n",
    "\n",
    "simulations=['ARG1.1-R_old','PHI1.1-R','PHI2.1-R','WPO1.1-R','BRA1.1-R','USA1.1-R','DRC1.1-R','AUS1.1-R']\n",
    "domain='3'\n",
    "nsamples=1\n",
    "sample_size = 15000\n",
    "variables = [['Tk', 0, 'model', '$T_{sfc}^{2} (K^{2})$','precip']             , ['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$','precip'],\\\n",
    "             ['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$','precip'], ['RH', 0, 'model', '$RH_{sfc}^{2} (percent^{2})$','precip'],\\\n",
    "             ['U', 0, 'model', '$U_{sfc}^{2} (m^{2}s^{-2})$','precip']        , ['V', 0, 'model', '$V_{sfc}^{2} (m^{2}s^{-2})$','precip'],\\\n",
    "             ['WSPD', 0, 'model', '$WSPD_{sfc}^{2} (m^{2}s^{-2})$','precip']  , ['W', 0, 'model', '$W_{sfc}^{2} (m^{2}s^{-2})$','precip'],\\\n",
    "             ['MCAPE', -999, None, '$MCAPE^{2} (J^{2}kg^{-2})$','precip']     , ['MCIN', -999, None, '$MCIN^{2} (J^{2}kg^{-2})$','precip'], \\\n",
    "             ['Tk', 750, 'pressure', '$T_{750}^{2} (K^{2})$','qtc_0.00001_w_1']        , ['THETA', 750, 'pressure', '$Theta_{750}^{2} (K^{2})$','qtc_0.00001_w_1'],\\\n",
    "             ['QV', 750, 'pressure', '$Qvapor_{750}^{2} (kg^{2}kg^{-2})$','qtc_0.00001_w_1'], ['RH', 750, 'pressure', '$RH_{750}^{2} (percent^{2})$','qtc_0.00001_w_1'],\\\n",
    "             ['U', 750, 'pressure', '$U_{750}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1']   , ['V', 750, 'pressure', '$V_{750}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'],\\\n",
    "             ['WSPD', 750, 'pressure', '$WSPD_{750}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'], ['W', 750, 'pressure', '$W_{750}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'],\\\n",
    "             ['Tk', 500, 'pressure', '$T_{500}^{2} (K^{2})$','qtc_0.00001_w_1']        , ['THETA', 500, 'pressure', '$Theta_{500}^{2} (K^{2})$','qtc_0.00001_w_1'],\\\n",
    "             ['QV', 500, 'pressure', '$Qvapor_{500}^{2} (kg^{2}kg^{-2})$','qtc_0.00001_w_1'], ['RH', 500, 'pressure', '$RH_{500}^{2} (percent^{2})$','qtc_0.00001_w_1'],\\\n",
    "             ['U', 500, 'pressure', '$U_{500}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1']   , ['V', 500, 'pressure', '$V_{500}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'],\\\n",
    "             ['WSPD', 500, 'pressure', '$WSPD_{500}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'], ['W', 500, 'pressure', '$W_{500}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'],\\\n",
    "             ['Tk', 200, 'pressure', '$T_{200}^{2} (K^{2})$','qtc_0.00001_w_1']        , ['THETA', 200, 'pressure', '$Theta_{200}^{2} (K^{2})$','qtc_0.00001_w_1'],\\\n",
    "             ['QV', 200, 'pressure', '$Qvapor_{200}^{2} (kg^{2}kg^{-2})$','qtc_0.00001_w_1'], ['RH', 200, 'pressure', '$RH_{200}^{2} (percent^{2})$','qtc_0.00001_w_1'],\\\n",
    "             ['U', 200, 'pressure', '$U_{200}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1']   , ['V', 200, 'pressure', '$V_{200}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'], \\\n",
    "             ['WSPD', 200, 'pressure', '$WSPD_{200}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'], ['W', 200, 'pressure', '$W_{200}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1']]\n",
    "\n",
    "colors    =  ['#000000','#E69F00','#56B4E9','#009E73','#F0E442','#0072B2','#D55E00','#CC79A7']\n",
    "\n",
    "def save_variogram_masked_domain(MASK_AREA,WHICH_TIME, VARIABLE, SIMULATIONS, SAMPLE_SIZE, DOMAIN, NSAMPLES, COLORS, PLOT):\n",
    "    if DOMAIN=='1':\n",
    "        dx = 1.6\n",
    "    if DOMAIN=='2':\n",
    "        dx=0.4\n",
    "    if DOMAIN=='3':\n",
    "        dx=0.1\n",
    "        \n",
    "    print('working on ',VARIABLE,'\\n')\n",
    "    MASK_TYPE= 'precip' #VARIABLE[4]\n",
    "    print('mask type is ',MASK_TYPE)\n",
    "    \n",
    "    if PLOT:\n",
    "        fig    = plt.figure(figsize=(8,8))\n",
    "        \n",
    "    for ii,simulation in enumerate(SIMULATIONS):   \n",
    "        print('    working on simulation: ',simulation)\n",
    "                \n",
    "        if DOMAIN=='1' or DOMAIN =='2':\n",
    "            rams_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+DOMAIN+'/out/'+'a-A-*g'+DOMAIN+'.h5'))# CSU machine\n",
    "        if DOMAIN=='3':\n",
    "            rams_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+DOMAIN+'/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "        print('        total # files = ',len(rams_files))\n",
    "        print('        first file is ',rams_files[0])\n",
    "        print('        last file is ',rams_files[-1])\n",
    "        if WHICH_TIME=='start':\n",
    "            rams_fil    = rams_files[0]\n",
    "        if WHICH_TIME=='middle':\n",
    "            rams_fil    = rams_files[int(len(rams_files)/2)]\n",
    "        if WHICH_TIME=='end':\n",
    "            rams_fil    = rams_files[-1]\n",
    "        print('        choosing the ',WHICH_TIME,' file: ',rams_fil)\n",
    "        \n",
    "        ######################################################\n",
    "        # if (VARIABLE[1]<0) or (VARIABLE[1]==0):\n",
    "        #     mask_search_string = 'storm_mask_'+MASK_TYPE+'_'+simulation+'_'+get_time_from_RAMS_file(rams_fil)[1]+'.npy'\n",
    "        #     print(        'mask search string: ',mask_search_string)\n",
    "        #     mask_name = sorted(glob.glob(mask_search_string))\n",
    "        # else:\n",
    "        #     mask_search_string = 'storm_mask_'+MASK_TYPE+'_'+VARIABLE[2]+'_level_'+str(VARIABLE[1])+'_'+simulation+'_'+get_time_from_RAMS_file(rams_fil)[1]+'.npy'\n",
    "        #     print('        mask search string: ',mask_search_string)\n",
    "        #     mask_name = sorted(glob.glob(mask_search_string))\n",
    "        \n",
    "        mask_search_string = 'storm_mask_'+MASK_TYPE+'_'+simulation+'_'+get_time_from_RAMS_file(rams_fil)[1]+'.npy'\n",
    "        mask_name = sorted(glob.glob(mask_search_string))\n",
    "        print('        <<< found mask file>>>: ',mask_name)\n",
    "\n",
    "        if MASK_AREA=='near_storm':\n",
    "            mask    = np.load(mask_name[0])\n",
    "        elif MASK_AREA=='not_near_storm':\n",
    "            mask    = np.load(mask_name[0])\n",
    "            mask    = np.where((mask < 1.1) & (mask > 0.9), np.nan , 1.0)\n",
    "        else:\n",
    "            print('please provide a corect value of area type')\n",
    "        ###############################################################\n",
    "        \n",
    "\n",
    "        da     = xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "        z_temp = da['TOPT'].values\n",
    "        y_dim, x_dim     = np.shape(z_temp)\n",
    "        print('        shape of the arrays is ',y_dim,'x',x_dim)\n",
    "        x      = np.arange(0,x_dim)\n",
    "        y      = np.arange(0,y_dim)\n",
    "        # # full coordinate arrays\n",
    "        xx, yy = np.meshgrid(x, y)*mask\n",
    "        coords_tuples_2d = np.vstack(([yy.T], [xx.T])).T\n",
    "        print('        shape of combined coords matrix: ',np.shape(coords_tuples_2d))\n",
    "        coords_all_np = coords_tuples_2d.reshape(-1, 2)#.tolist()\n",
    "        print('        shape of 1d list of coords: ',np.shape(coords_all_np))\n",
    "        # Create a boolean mask for rows with NaN values\n",
    "        nan_rows_mask = np.any(np.isnan(coords_all_np), axis=1)\n",
    "        # Use the mask to select rows without NaN values\n",
    "        coords_all_np = coords_all_np[~nan_rows_mask]\n",
    "        coords_all    = coords_all_np.tolist()\n",
    "        #print('first 10 coords are: ',coords_all[0:11])\n",
    "\n",
    "        z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(rams_fil,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "        print('        got the data... min = ',np.nanmin(z),' max = ',np.nanmax(z))\n",
    "        print('        percentage of nans is ',np.count_nonzero(np.isnan(z))/len(z.flatten()))\n",
    "        print('        choosing ',min(SAMPLE_SIZE,len(coords_all)),' random points...')\n",
    "        coords = random.sample(coords_all,min(SAMPLE_SIZE,len(coords_all)))\n",
    "        #print('first 10 randomly selected coords are: ',coords[0:11])\n",
    "        print('        get field values from these points...')\n",
    "        values = np.fromiter((z[int(c[0]), int(c[1])] for c in coords), dtype=float)\n",
    "        # Remove nan values\n",
    "        print('        Removing nan values and the corresponding coordinates...')\n",
    "        nan_mask = ~np.isnan(values)\n",
    "        print('        # non-nan values',np.count_nonzero(nan_mask))\n",
    "        values   = values[nan_mask]\n",
    "        sampled_coords_array = np.array(coords)\n",
    "        coords   = sampled_coords_array[nan_mask].tolist()\n",
    "        print('        final shape of coords is ',np.shape(coords))\n",
    "        print('        final shape of values is ',np.shape(values))\n",
    "        V        = skg.Variogram(coords, values,n_lags=200,bin_func='even')\n",
    "        print('        creating variogram...')\n",
    "        \n",
    "        bins = V.bins*dx # convert from ineteger coordinates to physical coordinates (km)\n",
    "        exp_variograms =  V.experimental\n",
    "\n",
    "        matrix_for_saving = np.array([bins,exp_variograms]).T\n",
    "        \n",
    "        if VARIABLE[2]:\n",
    "            data_file = 'experimental_variogram_RAMS_mask_'+MASK_AREA+'_criteria_'+MASK_TYPE+'_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_'+z_time+'_'+simulation+'_1_sample_d0'+DOMAIN+'.npy'\n",
    "        else:\n",
    "            data_file = 'experimental_variogram_RAMS_mask_'+MASK_AREA+'_criteria_'+MASK_TYPE+'_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_'+z_time+'_'+simulation+'_1_sample_d0'+DOMAIN+'.npy'\n",
    "\n",
    "        with open(data_file, 'wb') as f:\n",
    "            np.save(f, matrix_for_saving)\n",
    "            np.save(f, matrix_for_saving)\n",
    "\n",
    "        print('        saving variogram data to ',data_file)\n",
    "        print('    ------\\n')\n",
    "        \n",
    "        if PLOT:\n",
    "            plt.plot(bins,exp_variograms,label=simulation, color=COLORS[ii])\n",
    "        \n",
    "    if PLOT:\n",
    "        if VARIABLE[2]:\n",
    "            title_string = 'Variogram for masked ('+MASK_AREA+'; '+MASK_TYPE+') '+VARIABLE[0]+' at '+VARIABLE[2]+' level '+str(int(VARIABLE[1]))+' for d0'+DOMAIN+'\\nmid-simulation'\n",
    "        else:\n",
    "            title_string = 'Variogram for masked ('+MASK_AREA+'; '+MASK_TYPE+') '+VARIABLE[0]+' for d0'+DOMAIN+'\\nmid-simulation'    \n",
    "        plt.title(title_string)\n",
    "        plt.xlabel('distance (km)')\n",
    "        plt.ylabel(VARIABLE[3])\n",
    "        plt.legend()\n",
    "        if VARIABLE[2]:\n",
    "            filename = 'experimental_variogram_8_simulations_RAMS_mask_'+MASK_AREA+'_criteria_'+MASK_TYPE+'_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_1_sample_d0'+DOMAIN+'_mid-simulation.png'\n",
    "        else:\n",
    "            filename = 'experimental_variogram_8_simulations_RAMS_mask_'+MASK_AREA+'_criteria_'+MASK_TYPE+'_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_1_sample_d0'+DOMAIN+'_mid-simulation.png'\n",
    "        print('saving to file: ',filename)\n",
    "        plt.savefig(filename,dpi=150)\n",
    "        print('\\n\\n')\n",
    "\n",
    "                                         \n",
    "#save_variogram_masked_domain('not_near_storm','middle',variables[0], simulations, sample_size, domain, nsamples, colors, False)                                        \n",
    "                                         \n",
    "print('working on domain' ,domain)\n",
    "#Running on the terminal in parallel\n",
    "argument = []\n",
    "for var in variables:\n",
    "    for sim_time in ['start','end','middle']:\n",
    "        argument = argument + [('not_near_storm',sim_time,var, simulations, sample_size, domain, nsamples, colors, False)]\n",
    "\n",
    "print('length of argument is: ',len(argument))\n",
    "\n",
    "\n",
    "# # ############################### FIRST OF ALL ################################\n",
    "cpu_count1 = 37 #cpu_count()\n",
    "print('number of cpus: ',cpu_count1)\n",
    "# # #############################################################################\n",
    "\n",
    "def main(FUNCTION, ARGUMENT):\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(processes = (cpu_count1-1)) as pool:\n",
    "        data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "    #df_all = pd.concat(data, ignore_index=True)\n",
    "    #thermo_indices_data_csv_file = csv_folder+'thermodynamic_indices_' + DOMAIN + '_comb_track_filt_01_02_50_02_sr5017_setpos.csv'\n",
    "    #print('saving thermodynamic indices to the file: ',thermo_indices_data_csv_file)\n",
    "    #df_all.to_csv(thermo_indices_data_csv_file)  # sounding data\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main(save_variogram_masked_domain, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801df0f-35cb-4925-94ec-fb6781f62e65",
   "metadata": {},
   "source": [
    "### Plot variograms from masked domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2069fbc9-2218-4728-9870-05c0e3257793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "plt.style.use('ggplot')\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "simulations=['PHI1.1-R','PHI2.1-R','WPO1.1-R','BRA1.1-R','USA1.1-R','DRC1.1-R','AUS1.1-R']\n",
    "variables = [['Tk', 0, 'model', '$T_{sfc}^{2} (K^{2})$','precip']             , ['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$','precip'],\\\n",
    "             ['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$','precip'], ['RH', 0, 'model', '$RH_{sfc}^{2} (percent^{2})$','precip'],\\\n",
    "             ['U', 0, 'model', '$U_{sfc}^{2} (m^{2}s^{-2})$','precip']        , ['V', 0, 'model', '$V_{sfc}^{2} (m^{2}s^{-2})$','precip'],\\\n",
    "             ['WSPD', 0, 'model', '$WSPD_{sfc}^{2} (m^{2}s^{-2})$','precip']  , ['W', 0, 'model', '$W_{sfc}^{2} (m^{2}s^{-2})$','precip'],\\\n",
    "             ['MCAPE', -999, None, '$MCAPE^{2} (J^{2}kg^{-2})$','precip']     , ['MCIN', -999, None, '$MCIN^{2} (J^{2}kg^{-2})$','precip'], \\\n",
    "             ['Tk', 750, 'pressure', '$T_{750}^{2} (K^{2})$','qtc_0.00001_w_1']        , ['THETA', 750, 'pressure', '$Theta_{750}^{2} (K^{2})$','qtc_0.00001_w_1'],\\\n",
    "             ['QV', 750, 'pressure', '$Qvapor_{750}^{2} (kg^{2}kg^{-2})$','qtc_0.00001_w_1'], ['RH', 750, 'pressure', '$RH_{750}^{2} (percent^{2})$','qtc_0.00001_w_1'],\\\n",
    "             ['U', 750, 'pressure', '$U_{750}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1']   , ['V', 750, 'pressure', '$V_{750}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'],\\\n",
    "             ['WSPD', 750, 'pressure', '$WSPD_{750}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'], ['W', 750, 'pressure', '$W_{750}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'],\\\n",
    "             ['Tk', 500, 'pressure', '$T_{500}^{2} (K^{2})$','qtc_0.00001_w_1']        , ['THETA', 500, 'pressure', '$Theta_{500}^{2} (K^{2})$','qtc_0.00001_w_1'],\\\n",
    "             ['QV', 500, 'pressure', '$Qvapor_{500}^{2} (kg^{2}kg^{-2})$','qtc_0.00001_w_1'], ['RH', 500, 'pressure', '$RH_{500}^{2} (percent^{2})$','qtc_0.00001_w_1'],\\\n",
    "             ['U', 500, 'pressure', '$U_{500}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1']   , ['V', 500, 'pressure', '$V_{500}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'],\\\n",
    "             ['WSPD', 500, 'pressure', '$WSPD_{500}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'], ['W', 500, 'pressure', '$W_{500}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'],\\\n",
    "             ['Tk', 200, 'pressure', '$T_{200}^{2} (K^{2})$','qtc_0.00001_w_1']        , ['THETA', 200, 'pressure', '$Theta_{200}^{2} (K^{2})$','qtc_0.00001_w_1'],\\\n",
    "             ['QV', 200, 'pressure', '$Qvapor_{200}^{2} (kg^{2}kg^{-2})$','qtc_0.00001_w_1'], ['RH', 200, 'pressure', '$RH_{200}^{2} (percent^{2})$','qtc_0.00001_w_1'],\\\n",
    "             ['U', 200, 'pressure', '$U_{200}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1']   , ['V', 200, 'pressure', '$V_{200}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'], \\\n",
    "             ['WSPD', 200, 'pressure', '$WSPD_{200}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1'], ['W', 200, 'pressure', '$W_{200}^{2} (m^{2}s^{-2})$','qtc_0.00001_w_1']]\n",
    "\n",
    "\n",
    "colors    = ['#7F7F7F','#E69F00','#56B4E9','#009E73','#F0E442','#0072B2','#D55E00','#CC79A7']\n",
    "\n",
    "color_dict = {'ARG1.1-R_old':'#000000',\\\n",
    "              'ARG1.1-R':'#000000',\\\n",
    "              'PHI1.1-R':'#E69F00',\\\n",
    "              'PHI2.1-R':'#56B4E9',\\\n",
    "              'WPO1.1-R':'#009E73',\\\n",
    "              'BRA1.1-R':'#7F7F7F',\\\n",
    "              'USA1.1-R':'#0072B2',\\\n",
    "              'DRC1.1-R':'#D55E00',\\\n",
    "              'AUS1.1-R':'#CC79A7'}\n",
    "linestyles = ['-', '--', ':']\n",
    "domain='3'\n",
    "mask_type_dict = {'not_near_storm':'environment','near_storm':'storm'}\n",
    "# the key is the type of the mask, the value is the corresponding title world\n",
    "mask_type='not_near_storm'\n",
    "\n",
    "for variable in variables:\n",
    "    print('variable: ',variable[0],variable[1],variable[2])\n",
    "    #criteria = variable[4]\n",
    "    criteria = 'precip'\n",
    "    \n",
    "    #print('simulation: ',simulation)\n",
    "    if variable[2]:\n",
    "        data_files = glob.glob('experimental_variogram_RAMS_mask_'+mask_type+'_criteria_'+criteria+'_'+variable[0]+'_levtype_'+variable[2]+'_lev_'+str(int(variable[1]))+'_*_1_sample_d0'+domain+'.npy')\n",
    "    else:\n",
    "        data_files =  glob.glob('experimental_variogram_RAMS_mask_'+mask_type+'_criteria_'+criteria+'_'+variable[0]+'_levtype_'+'None'+'_lev_'+'None_*_1_sample_d0'+domain+'.npy')\n",
    "    print(len(data_files),' files\\n')\n",
    "    print(data_files)\n",
    "    print('---------\\n')\n",
    "    #if len(data_files)<8:\n",
    "    #    print(variable)#,': ',len(data_files))\n",
    "    plt.figure(figsize=(11,8))\n",
    "    ax = plt.gca()\n",
    "    #temp_list = []\n",
    "    \n",
    "    for ii, fil in enumerate(sorted(data_files)):\n",
    "        fil_name = Path(fil).name\n",
    "        print(fil_name.split('_'))\n",
    "        simulation_name = fil_name.split('_')[15]\n",
    "        timestep        = fil_name.split('_')[14]\n",
    "        timestep_pd     = pd.to_datetime(timestep,format='%Y%m%d%H%M%S')\n",
    "        print('simulation_name: ',simulation_name)\n",
    "        print('time: ',timestep_pd)\n",
    "        vario_data = np.load(fil)\n",
    "        bins = vario_data[:,0][vario_data[:,0]<=100.]\n",
    "        exp_vario = vario_data[:,1][vario_data[:,0]<=100.]\n",
    "\n",
    "        def find_inflection_points(y,smooth):\n",
    "            if smooth:\n",
    "                from scipy.signal import savgol_filter\n",
    "                from scipy.ndimage import gaussian_filter1d\n",
    "                # Apply Savitzky-Golay filter for smoothing\n",
    "                #window_size = 20  # Adjust the window size based on your data\n",
    "                #order = 4  # Adjust the polynomial order based on your data\n",
    "                sigma = 3\n",
    "                #variogram_curve = savgol_filter(y, window_size, order)\n",
    "                y = gaussian_filter1d(y,sigma)\n",
    "                \n",
    "            # Calculate the second derivative\n",
    "            second_derivative = np.gradient(np.gradient(y))\n",
    "            # Find peaks in the negative second derivative (indicating concave down regions)\n",
    "            inflection_points = find_peaks(-second_derivative)[0]\n",
    "            return inflection_points\n",
    "        \n",
    "        # Find inflection points\n",
    "        #inflection_points = find_inflection_points(exp_vario,True)\n",
    "        #plt.scatter(bins[inflection_points], exp_vario[inflection_points],color=color_dict[simulation_name])\n",
    "        plt.plot(bins,exp_vario,label=simulation_name+' '+timestep_pd.strftime('%H:%M:%S'),color=color_dict[simulation_name],linestyle=linestyles[ii % len(linestyles)])\n",
    "\n",
    "    if variable[2]:\n",
    "        title_string = 'Variogram for masked ('+mask_type_dict[mask_type]+') '+variable[0]+' at '+variable[2]+' level '+str(int(variable[1]))+' for d0'+domain#+'\\nmid-simulation'\n",
    "    else:\n",
    "        title_string = 'Variogram for masked ('+mask_type_dict[mask_type]+') '+variable[0]+' for d0'+domain#+'\\nmid-simulation'    \n",
    "    plt.title(title_string)\n",
    "    plt.xlabel('distance (km)')\n",
    "    plt.ylabel(variable[3])\n",
    "    #plt.legend(prop = { \"size\": 9 })\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    if variable[1]>=0:\n",
    "            filename = 'experimental_variograms_simulations_RAMS_mask_'+mask_type+'_criteria_'+criteria+'_'+variable[0]+'_levtype_'+variable[2]+'_lev_'+str(int(variable[1]))+'_1_sample_no_mask_d0'+domain+'.png'\n",
    "    else:\n",
    "            filename = 'experimental_variograms_simulations_RAMS_mask_'+mask_type+'_criteria_'+criteria+'_'+variable[0]+'_levtype_'+'None'+'_lev_'+'None'+'_1_sample_no_mask_d0'+domain+'.png'\n",
    "    print('saving to file: ',filename)\n",
    "    plt.savefig(filename,dpi=150,bbox_inches='tight')\n",
    "    #print(sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+'3'+'/out_30s/'+'a-L-*g3.h5')))\n",
    "    print('---------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f561ed0-e48b-4732-9844-3ecb7b3baf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## OLD CODE #############################################\n",
    "from skgstat.plotting import backend\n",
    "from pprint import pprint\n",
    "import random\n",
    "import skgstat as skg\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def create_variograms_for_masked_array(simulations,variables,ylabels,nsamples,sample_size,nbins,area_type='near_storm'):\n",
    "    for ii, var in enumerate(variables):\n",
    "        print('working on ',var,'\\n')\n",
    "        fig    = plt.figure(figsize=(8,8))\n",
    "        for simulation in simulations:   \n",
    "            print('    working on simulation: ',simulation)\n",
    "            rams_files_g3=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G3/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "            print('        total # files = ',len(rams_files_g3))\n",
    "            print('        first file is ',rams_files_g3[0])\n",
    "            print('        last file is ',rams_files_g3[-1])\n",
    "            rams_fil     = rams_files_g3[int(len(rams_files_g3)/2)]\n",
    "            print('        choosing the middle file: ',rams_fil)\n",
    "            timestr = get_time_from_RAMS_file(rams_fil)[1]\n",
    "            mask_name = 'storm_mask_'+simulation+'_'+timestr+'.npy'\n",
    "            print('        using storm mask file: ',mask_name)\n",
    "            \n",
    "            if area_type=='near_storm':\n",
    "                mask    = np.load(mask_name)\n",
    "            elif area_type=='not_near_storm':\n",
    "                mask    = np.load(mask_name)\n",
    "                mask    = np.where((mask < 1.1) & (mask > 0.9), np.nan , 1.0)\n",
    "            else:\n",
    "                print('please provide a corect value of area type')\n",
    "            da     = xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "            z_temp = da['TOPT'].values\n",
    "            y_dim, x_dim = np.shape(z_temp)\n",
    "            print('        shape of the arrays is ',y_dim,'x',x_dim)\n",
    "            x      = np.arange(0,x_dim)\n",
    "            y      = np.arange(0,y_dim)\n",
    "            # # full coordinate arrays\n",
    "            xx, yy = np.meshgrid(x, y)*mask\n",
    "            coords_tuples_2d = np.vstack(([yy.T], [xx.T])).T\n",
    "            print('        shape of combined coords matrix: ',np.shape(coords_tuples_2d))\n",
    "            coords_all_np = coords_tuples_2d.reshape(-1, 2)#.tolist()\n",
    "            print('        shape of flattened combined coords matrix: ',np.shape(coords_all_np))\n",
    "            # Create a boolean mask for rows with NaN values\n",
    "            nan_rows_mask = np.any(np.isnan(coords_all_np), axis=1)\n",
    "            # Use the mask to select rows without NaN values\n",
    "            coords_all_np = coords_all_np[~nan_rows_mask]\n",
    "            coords_all    = coords_all_np.tolist()\n",
    "            print('        shape of flattened combined coords list with nans removed: ',np.shape(coords_all))\n",
    "            values_all = z_temp.flatten()# reshape(-1,0)\n",
    "            print('        shape of flattened values array : ',np.shape(values_all))\n",
    "            \n",
    "            if var=='WSPD':\n",
    "                print('        calculating WSPD')\n",
    "                z      = (da['UP'][0,:,:].values**2 + da['VP'][0,:,:].values**2)\n",
    "            else:\n",
    "                z      = da[var][0,:,:].values\n",
    "            print('        choosing ',sample_size,' random points...')\n",
    "            coords = random.sample(coords_all,sample_size)\n",
    "            print('        shape of '+str(sample_size)+' random selected coords :',np.shape(coords))\n",
    "            print('        get field values from these points...')\n",
    "            values = np.fromiter((z[int(c[0]), int(c[1])] for c in coords), dtype=float)\n",
    "            V      = skg.Variogram(coords, values,n_lags=nbins,bin_func='even')\n",
    "            #print('        Variogram properties: ')\n",
    "            #pprint(V.describe())\n",
    "            #print('\\n')\n",
    "            print('        plotting experimental variogram...')\n",
    "            plt.plot(V.bins/10.0, V.experimental,label=simulation)\n",
    "            print('    ------\\n')\n",
    "\n",
    "        plt.title('Variogram for a sample of size '+str(sample_size)+' for surface '+var)\n",
    "        plt.xlabel('distance (km)')\n",
    "        plt.ylabel(ylabels[ii])\n",
    "        plt.legend(loc='upper right')\n",
    "        # l, b, h, w = .15, .72, .1, .2\n",
    "        # axins = fig.add_axes([l, b, w, h])\n",
    "        # axins.hist(output_variogram.distance/10.0,bins=nbins) \n",
    "        # axins.set_yticks([])\n",
    "        # axins.set_xticks([50,150,250],labels=['50','150','250'])\n",
    "        # axins.set_xticklabels(axins.get_xticklabels(), rotation=0, fontsize=8)\n",
    "        # axins.set_xlabel('distance (km)',fontsize=8)\n",
    "        # axins.set_title('#pairs by distance',fontsize=8)\n",
    "        filename = 'experimental_variogram_all_simulations_RAMS_'+var+'_1_sample_d03_'+area_type+'.png'\n",
    "        print('saving to file: ',filename)\n",
    "        plt.savefig(filename,dpi=150)\n",
    "        print('\\n\\n')\n",
    "        return V\n",
    "        \n",
    "# output_variogram = create_variograms_for_masked_array(sims,varnames,ylabs,nsamp,20000,200,'near_storm')\n",
    "# #output_variogram = create_variograms_for_masked_array(sims,varnames,ylabs,nsamp,10000,200,'not_near_storm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef2249-0a41-416f-8d9b-6dc133522195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backend('plotly')\n",
    "from skgstat.plotting import backend\n",
    "from pprint import pprint\n",
    "import random\n",
    "import skgstat as skg\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def create_histogram_variopairs(simulations,variables,nsamples,sample_size,nbins):\n",
    "    for ii, var in enumerate(variables):\n",
    "        print('working on ',var,'\\n')\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(10, 8))\n",
    "        flattened_axes = axes.reshape(-1)\n",
    "        for jj, simulation in enumerate(simulations):   \n",
    "            print('    working on simulation: ',simulation)\n",
    "            rams_files_g3=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G3/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "            print('        total # files = ',len(rams_files_g3))\n",
    "            print('        first file is ',rams_files_g3[0])\n",
    "            print('        last file is ',rams_files_g3[-1])\n",
    "            rams_fil     = rams_files_g3[int(len(rams_files_g3)/2)]\n",
    "            print('        choosing the middle file: ',rams_fil)\n",
    "            timestr = get_time_from_RAMS_file(rams_fil)[1]\n",
    "            # mask_name = 'storm_mask_'+simulation+'_'+timestr+'.npy'\n",
    "            # print('        using storm mask file: ',mask_name)\n",
    "            # if area_type=='near_storm':\n",
    "            #     mask    = np.load(mask_name)\n",
    "            # elif area_type=='not_near_storm':\n",
    "            #     mask    = np.load(mask_name)\n",
    "            #     mask    = np.where((mask < 1.1) & (mask > 0.9), np.nan , 1.0)\n",
    "            # else:\n",
    "            #     print('please provide a corect value of area type')\n",
    "            da     = xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "            z_temp = da['TOPT'].values\n",
    "            y_dim, x_dim = np.shape(z_temp)\n",
    "            print('        shape of the arrays is ',y_dim,'x',x_dim)\n",
    "            x      = np.arange(0,x_dim)\n",
    "            y      = np.arange(0,y_dim)\n",
    "            # # full coordinate arrays\n",
    "            xx, yy = np.meshgrid(x, y) #*mask\n",
    "            coords_tuples_2d = np.vstack(([yy.T], [xx.T])).T\n",
    "            print('        shape of combined coords matrix: ',np.shape(coords_tuples_2d))\n",
    "            coords_all_np = coords_tuples_2d.reshape(-1, 2)#.tolist()\n",
    "            print('        shape of flattened combined coords matrix: ',np.shape(coords_all_np))\n",
    "            # Create a boolean mask for rows with NaN values\n",
    "            nan_rows_mask = np.any(np.isnan(coords_all_np), axis=1)\n",
    "            # Use the mask to select rows without NaN values\n",
    "            coords_all_np = coords_all_np[~nan_rows_mask]\n",
    "            coords_all    = coords_all_np.tolist()\n",
    "            print('        shape of flattened combined coords list with nans removed: ',np.shape(coords_all))\n",
    "            values_all = z_temp.flatten()# reshape(-1,0)\n",
    "            print('        shape of flattened values array : ',np.shape(values_all))\n",
    "            \n",
    "            if var=='WSPD':\n",
    "                print('        calculating WSPD')\n",
    "                z      = (da['UP'][0,:,:].values**2 + da['VP'][0,:,:].values**2)\n",
    "            else:\n",
    "                z      = da[var][0,:,:].values\n",
    "            print('        choosing ',sample_size,' random points...')\n",
    "            coords = random.sample(coords_all,sample_size)\n",
    "            print('        shape of '+str(sample_size)+' random selected coords :',np.shape(coords))\n",
    "            print('        get field values from these points...')\n",
    "            values = np.fromiter((z[int(c[0]), int(c[1])] for c in coords), dtype=float)\n",
    "            #V      = skg.Variogram(coords, values,n_lags=nbins,bin_func='even')\n",
    "            V      = skg.Variogram(coords, values,n_lags=nbins,bin_func='uniform')\n",
    "            print('        Variogram properties: ')\n",
    "            pprint(V.describe())\n",
    "            print('\\n')\n",
    "            #print('        plotting experimental variogram...')\n",
    "            #plt.plot(V.bins/10.0, V.experimental,label=simulation)\n",
    "\n",
    "            flattened_axes[jj].hist(V.distance/10.0,bins=nbins,color = \"skyblue\") \n",
    "            #flattened_axes[jj].set_yticks([])\n",
    "            flattened_axes[jj].set_xticks([50,150,250],labels=['50','150','250'])\n",
    "            flattened_axes[jj].set_xticklabels(flattened_axes[jj].get_xticklabels(), rotation=0, fontsize=8)\n",
    "            flattened_axes[jj].set_xlabel('distance (km)',fontsize=8)\n",
    "            flattened_axes[jj].set_title(simulation+'\\n'+get_time_from_RAMS_file(rams_fil)[0],fontsize=9)\n",
    "            if jj==0 or jj==4:\n",
    "                flattened_axes[jj].set_ylabel('#pairs',fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Histograms')\n",
    "        filename = 'histogram_pairs_RAMS_all_sims.png'\n",
    "        print('saving to file: ',filename)\n",
    "        #plt.savefig(filename,dpi=150)\n",
    "        print('\\n\\n')\n",
    "        #return V\n",
    "        \n",
    "sims=['DRC1.1-R','AUS1.1-R']#,'PHI1.1-R','PHI2.1-R','WPO1.1-R','BRA1.1-R','USA1.1-R','RSA1.1-R']\n",
    "nsamp=1\n",
    "varnames = ['RV']\n",
    "# ylabs=['$RV^{2} (kg^{2}kg^{-2})$']\n",
    "# masked_array_list = []\n",
    "        \n",
    "#plot_variable(sims,varnames)\n",
    "create_histogram_variopairs(sims,varnames,nsamp,10000,100)\n",
    "# output_variogram = create_variograms_for_masked_array(sims,varnames,ylabs,nsamp,20000,200,'near_storm')\n",
    "# #output_variogram = create_variograms_for_masked_array(sims,varnames,ylabs,nsamp,10000,200,'not_near_storm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f5523-d1a2-4488-815b-933783bb79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_variogram = create_variograms_for_masked_array(sims,varnames,ylabs,nsamp,20000,200,'not_near_storm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8023f-5fdf-4c81-9271-4298da4bb687",
   "metadata": {},
   "source": [
    "## Data analysis with variograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f06d9e-3bb9-4632-bb67-e1538adaf717",
   "metadata": {},
   "source": [
    "### k-mean analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550df1c5-0955-4101-9c95-41eeee132752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "simulations=['PHI1.1-R','PHI2.1-R','WPO1.1-R','BRA1.1-R','USA1.1-R','DRC1.1-R','AUS1.1-R']\n",
    "variables = [['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$']]\n",
    "#              ['Tk', 0, 'model', '$T_{sfc}^{2} (K^{2})$']             , ['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$'], ['RH', 0, 'model', '$RH_{sfc}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 0, 'model', '$U_{sfc}^{2} (m^{2}s^{-2})$']        , ['V', 0, 'model', '$V_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 0, 'model', '$WSPD_{sfc}^{2} (m^{2}s^{-2})$']  , ['W', 0, 'model', '$W_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['MCAPE', -999, None, '$MCAPE^{2} (J^{2}kg^{-2})$']     , ['MCIN', -999, None, '$MCIN^{2} (J^{2}kg^{-2})$'], \\\n",
    "#              ['Tk', 750, 'pressure', '$T_{750}^{2} (K^{2})$']        , ['THETA', 750, 'pressure', '$Theta_{750}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 750, 'pressure', '$Qvapor_{750}^{2} (kg^{2}kg^{-2})$'], ['RH', 750, 'pressure', '$RH_{750}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 750, 'pressure', '$U_{750}^{2} (m^{2}s^{-2})$']   , ['V', 750, 'pressure', '$V_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 750, 'pressure', '$WSPD_{750}^{2} (m^{2}s^{-2})$'], ['W', 750, 'pressure', '$W_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['Tk', 500, 'pressure', '$T_{500}^{2} (K^{2})$']        , ['THETA', 500, 'pressure', '$Theta_{500}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 500, 'pressure', '$Qvapor_{500}^{2} (kg^{2}kg^{-2})$'], ['RH', 500, 'pressure', '$RH_{500}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 500, 'pressure', '$U_{500}^{2} (m^{2}s^{-2})$']   , ['V', 500, 'pressure', '$V_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 500, 'pressure', '$WSPD_{500}^{2} (m^{2}s^{-2})$'], ['W', 500, 'pressure', '$W_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['Tk', 200, 'pressure', '$T_{200}^{2} (K^{2})$']        , ['THETA', 200, 'pressure', '$Theta_{200}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 200, 'pressure', '$Qvapor_{200}^{2} (kg^{2}kg^{-2})$'], ['RH', 200, 'pressure', '$RH_{200}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 200, 'pressure', '$U_{200}^{2} (m^{2}s^{-2})$']   , ['V', 200, 'pressure', '$V_{200}^{2} (m^{2}s^{-2})$'], \\\n",
    "#              ['WSPD', 200, 'pressure', '$WSPD_{200}^{2} (m^{2}s^{-2})$'], ['W', 200, 'pressure', '$W_{200}^{2} (m^{2}s^{-2})$']]\n",
    "# colors    = ['#7F7F7F','#E69F00','#56B4E9','#009E73','#F0E442','#0072B2','#D55E00','#CC79A7']\n",
    "color_dict = {'ARG1.1-R_old':'#000000',\\\n",
    "              'PHI1.1-R':'#E69F00',\\\n",
    "              'PHI2.1-R':'#56B4E9',\\\n",
    "              'WPO1.1-R':'#009E73',\\\n",
    "              'BRA1.1-R':'#7F7F7F',\\\n",
    "              'USA1.1-R':'#0072B2',\\\n",
    "              'DRC1.1-R':'#D55E00',\\\n",
    "              'AUS1.1-R':'#CC79A7'}\n",
    "linestyles = ['-', '--', ':']\n",
    "domain='3'\n",
    "\n",
    "for variable in variables:\n",
    "    print('variable: ',variable[0],variable[1],variable[2])\n",
    "    #print('simulation: ',simulation)\n",
    "    if variable[2]:\n",
    "        data_files = glob.glob('./variogram_data/experimental_variogram_RAMS_'+variable[0]+'_levtype_'+variable[2]+'_lev_'+str(int(variable[1]))+'_*_1_sample_no_mask_d0'+domain+'.npy')\n",
    "    else:\n",
    "        data_files =  glob.glob('./variogram_data/experimental_variogram_RAMS_'+variable[0]+'_levtype_'+'None'+'_lev_'+'None_*_1_sample_no_mask_d0'+domain+'.npy')\n",
    "    print(len(data_files),'\\n')\n",
    "    print(sorted(data_files))\n",
    "    #print('---------\\n')\n",
    "    #if len(data_files)<8:\n",
    "    #    print(variable)#,': ',len(data_files))\n",
    "    #plt.figure(figsize=(11,8))\n",
    "    #ax = plt.gca()\n",
    "    temp_list = []\n",
    "   \n",
    "    for ii, fil in enumerate(sorted(data_files)[1::3]):\n",
    "        print(ii)\n",
    "        fil_name = Path(fil).name\n",
    "        #print(fil_name)\n",
    "        simulation_name = fil_name.split('_')[9]\n",
    "        timestep        = fil_name.split('_')[8]\n",
    "        timestep_pd     = pd.to_datetime(timestep,format='%Y%m%d%H%M%S')\n",
    "        print('simulation_name: ',simulation_name)\n",
    "        print('time: ',timestep_pd)\n",
    "        vario_data = np.load(fil)\n",
    "        bins = vario_data[:,0]#[vario_data[:,0]<=220.]\n",
    "        exp_vario = vario_data[:,1]#[vario_data[:,0]<=220.]\n",
    "        print(np.shape(exp_vario))\n",
    "        temp_list.append(exp_vario)\n",
    "        plt.plot(bins,exp_vario,label=simulation_name+' '+timestep_pd.strftime('%H:%M:%S'),color=color_dict[simulation_name])#,linestyle=linestyles[ii % len(linestyles)])\n",
    "    print(np.shape(temp_list))\n",
    "    if variable[2]:\n",
    "        title_string = 'Variogram for '+variable[0]+' at '+variable[2]+' level '+str(int(variable[1]))+' for d0'+domain#+'\\nmid-simulation'\n",
    "    else:\n",
    "        title_string = 'Variogram for '+variable[0]+' for d0'+domain#+'\\nmid-simulation'    \n",
    "    print(title_string)\n",
    "    #plt.title(title_string)\n",
    "    #plt.xlabel('distance (km)')\n",
    "    #plt.ylabel(variable[3])\n",
    "    #plt.legend(prop = { \"size\": 9 })\n",
    "    #ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    #if variable[2]:\n",
    "    #        filename = 'experimental_variogram_7_simulations_RAMS_'+variable[0]+'_levtype_'+variable[2]+'_lev_'+str(int(variable[1]))+'_1_sample_no_mask_d0'+domain+'.png'\n",
    "    #else:\n",
    "    #        filename = 'experimental_variogram_7_simulations_RAMS_'+variable[0]+'_levtype_'+'None'+'_lev_'+'None'+'_1_sample_no_mask_d0'+domain+'.png'\n",
    "    #print('saving to file: ',filename)\n",
    "    #plt.savefig(filename,dpi=150,bbox_inches='tight')\n",
    "    #print(sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G'+'3'+'/out_30s/'+'a-L-*g3.h5')))\n",
    "    print('---------------\\n')\n",
    "    \n",
    "# Generate or load your variogram data\n",
    "# Here, I'm assuming you have 5 variograms stored in a 2D NumPy array, each with 200 bins\n",
    "\n",
    "variograms = np.array(temp_list)  # Replace this with your variogram data\n",
    "\n",
    "# Perform k-means clustering on each variogram\n",
    "num_clusters = 3  # You can adjust the number of clusters as needed\n",
    "cluster_labels = []\n",
    "\n",
    "# for variogram in variograms:\n",
    "#     kmeans = KMeans(n_clusters=num_clusters)\n",
    "#     kmeans.fit(variogram.reshape(-1, 1))\n",
    "#     cluster_labels.append(kmeans.labels_)\n",
    "\n",
    "\n",
    "# Perform k-means clustering\n",
    "num_clusters = 3  # You can adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(variograms)\n",
    "\n",
    "\n",
    "# Get the cluster labels and centroids\n",
    "cluster_labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Print the cluster labels and centroids\n",
    "print(\"Cluster Labels:\", cluster_labels)\n",
    "print(\"Centroids:\", np.shape(centroids))\n",
    "\n",
    "for ii, centroid in enumerate(centroids):\n",
    "    plt.plot(centroid,label='cluster label '+str(cluster_labels[ii]),linestyle='--')\n",
    "    \n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "# Initialize lists to store inertias and number of clusters\n",
    "inertias = []\n",
    "num_clusters_range = range(1, 7)  # Range of number of clusters to try\n",
    "\n",
    "# Iterate over different numbers of clusters\n",
    "for num_clusters in num_clusters_range:\n",
    "    # Perform k-means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(variograms)\n",
    "    \n",
    "    # Append the inertia to the list\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot inertia versus number of clusters\n",
    "plt.plot(num_clusters_range, inertias, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Inertia vs Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Plot the clustered variograms\n",
    "# plt.figure(figsize=(12, 10))\n",
    "\n",
    "# for i, variogram in enumerate(variograms):\n",
    "#     plt.subplot(3, 3, i+1)\n",
    "#     for j in range(num_clusters):\n",
    "#         plt.plot(variogram[cluster_labels[i] == j], label=f'Cluster {j+1}')\n",
    "#     plt.title(f'Variogram {i+1}')\n",
    "#     plt.xlabel('Bin')\n",
    "#     plt.ylabel('Variogram Value')\n",
    "#     plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0846dd-9969-404b-aab5-f78d122a35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate or load your vectors data\n",
    "# Here, I'm assuming you have 10 vectors of size 19 stored in a 2D NumPy array\n",
    "vectors_data = np.random.rand(10, 19)  # Replace this with your vector data\n",
    "\n",
    "# Perform k-means clustering\n",
    "num_clusters = 3  # You can adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(vectors_data)\n",
    "\n",
    "# Get the cluster labels and centroids\n",
    "cluster_labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Print the cluster labels and centroids\n",
    "print(\"Cluster Labels:\", cluster_labels)\n",
    "print(\"Centroids:\", centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5576e64c-6f47-44df-81c5-87a075d052d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variograms = np.random.rand(5, 200)  # Replace this with your variogram data\n",
    "\n",
    "# Perform k-means clustering on each variogram\n",
    "num_clusters = 3  # You can adjust the number of clusters as needed\n",
    "cluster_labels = []\n",
    "\n",
    "for variogram in variograms:\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(variogram.reshape(-1, 1))\n",
    "    cluster_labels.append(kmeans.labels_)\n",
    "\n",
    "# Plot the clustered variograms\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, variogram in enumerate(variograms):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    for j in range(num_clusters):\n",
    "        plt.plot(variogram[cluster_labels[i] == j], label=f'Cluster {j+1}')\n",
    "    plt.title(f'Variogram {i+1}')\n",
    "    plt.xlabel('Bin')\n",
    "    plt.ylabel('Variogram Value')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e9ec8-b0c9-49b0-b800-d9fd1e4fcf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2D NumPy array with NaN values\n",
    "array_2d = np.array([[1.0, 2.0, np.nan],\n",
    "                    [4.0, np.nan, 6.0],\n",
    "                    [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Create a boolean mask for rows with NaN values\n",
    "nan_rows_mask = np.any(np.isnan(array_2d), axis=1)\n",
    "\n",
    "# Use the mask to select rows without NaN values\n",
    "array_without_nan_rows = array_2d[~nan_rows_mask]\n",
    "\n",
    "print(array_without_nan_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1548d-dbce-4ccc-abef-4b722ba5c5e2",
   "metadata": {},
   "source": [
    "## Testing different bin functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1b1e5-b803-4678-ad53-3ea1d8c19834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "nsamples=1\n",
    "bin_functions = ['even','uniform','sturges','scott','ward', 'stable_entropy']\n",
    "coords = random.sample(coords_all,10000)\n",
    "print('selecting random samples from coordinates array, first sample: ',coords[0])\n",
    "values = np.fromiter((z[c[0], c[1]] for c in coords), dtype=float)\n",
    "print('shape of coords : ',np.shape(coords))\n",
    "print('shape of values : ',np.shape(values))\n",
    "\n",
    "for bf in bin_functions:\n",
    "    print('    working on bin function : ',bf)\n",
    "    V      = skg.Variogram(coords, values,n_lags=200,bin_func=bf)\n",
    "    print('    estimator name is : ',V.estimator)#\n",
    "    plt.plot(V.bins/10.0, V.experimental,label='bin func: '+bf)\n",
    "    plt.title('$\\Theta$ Variograms for multiple binning functions for a sameple of size 10000')\n",
    "    plt.xlabel('distance (km)')\n",
    "    #plt.ylabel('$w^{2} (m^{2}s^{-2})$')\n",
    "    plt.ylabel('$\\Theta^{2} (m^{2}s^{-2})$')\n",
    "    print('    \\n\\n')\n",
    "    \n",
    "plt.legend()\n",
    "#plt.savefig('experimental_variogram_theta_samples.png',dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57642660-a56c-469f-a513-de6c169ce9c0",
   "metadata": {},
   "source": [
    "## PDFs of RAMS/WRF variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d63059-4e85-4792-a4be-47f23acc3e97",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7083f9-0255-44ae-a1a3-f980bdd8c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile save_coincident_variograms_multiprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as crs\n",
    "import random\n",
    "import skgstat as skg\n",
    "plt.style.use('ggplot')\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import read_vars_WRF_RAMS\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "#from scipy.stats import powerlaw\n",
    "import powerlaw\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S'), pd_time\n",
    "\n",
    "def find_closest_datetime_index(datetime_list, target_datetime):\n",
    "    \"\"\"\n",
    "    Find the index of the closest datetime in the datetime_list to the target_datetime.\n",
    "    \"\"\"\n",
    "    closest_datetime = min(datetime_list, key=lambda x: abs(x - target_datetime))\n",
    "    closest_index = datetime_list.index(closest_datetime)\n",
    "    return closest_index\n",
    "   \n",
    "def read_head(headfile,h5file):\n",
    "        # Function that reads header files from RAMS\n",
    "\n",
    "        # Inputs:\n",
    "        #   headfile: header file including full path in str format\n",
    "        #   h5file: h5 datafile including full path in str format\n",
    "\n",
    "        # Returns:\n",
    "        #   zmn: height levels for momentum values (i.e., grid box upper and lower levels)\n",
    "        #   ztn: height levels for thermodynaic values (i.e., grid box centers)\n",
    "        #   nx:: the number of x points for the domain associated with the h5file\n",
    "        #   ny: the number of y points for the domain associated with the h5file\n",
    "        #   npa: the number of surface patches\n",
    "\n",
    "\n",
    "        dom_num = h5file[h5file.index('.h5')-1] # Find index of .h5 to determine position showing which nest domain to use\n",
    "\n",
    "        with open(headfile) as f:\n",
    "            contents = f.readlines()\n",
    "\n",
    "        idx_zmn = contents.index('__zmn0'+dom_num+'\\n')\n",
    "        nz_m = int(contents[idx_zmn+1])\n",
    "        zmn = np.zeros(nz_m)\n",
    "        for i in np.arange(0,nz_m):\n",
    "            zmn[i] =  float(contents[idx_zmn+2+i])\n",
    "\n",
    "        idx_ztn = contents.index('__ztn0'+dom_num+'\\n')\n",
    "        nz_t = int(contents[idx_ztn+1])\n",
    "        ztn = np.zeros(nz_t)\n",
    "        for i in np.arange(0,nz_t):\n",
    "            ztn[i] =  float(contents[idx_ztn+2+i])\n",
    "\n",
    "        ztop = np.max(ztn) # Model domain top (m)\n",
    "\n",
    "        # Grad the size of the horizontal grid spacing\n",
    "        idx_dxy = contents.index('__deltaxn\\n')\n",
    "        dxy = float(contents[idx_dxy+1+int(dom_num)].strip())\n",
    "\n",
    "        idx_npatch = contents.index('__npatch\\n')\n",
    "        npa = int(contents[idx_npatch+2])\n",
    "\n",
    "        idx_ny = contents.index('__nnyp\\n')\n",
    "        idx_nx = contents.index('__nnxp\\n')\n",
    "        ny = np.ones(int(contents[idx_ny+1]))\n",
    "        nx = np.ones(int(contents[idx_ny+1]))\n",
    "        for i in np.arange(0,len(ny)):\n",
    "            nx[i] = int(contents[idx_nx+2+i])\n",
    "            ny[i] = int(contents[idx_ny+2+i])\n",
    "\n",
    "        ny_out = ny[int(dom_num)-1]\n",
    "        nx_out = nx[int(dom_num)-1]\n",
    "\n",
    "        return zmn, ztn, nx_out, ny_out, dxy, npa \n",
    "\n",
    "def grab_intersection_gbig_gsmall(VARIABLE,RAMS_G1_or_G2_FILE,RAMS_G3_FILE):\n",
    "    z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(RAMS_G1_or_G2_FILE,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "    #print(np.min(z))\n",
    "    #print(np.max(z))\n",
    "    #z2, z_name2, z_units2, z_time2 = read_vars_WRF_RAMS.read_variable(RAMS_G3_FILE,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "    print('        done getting the variable ',VARIABLE[0],' with shape: ',np.shape(z),'\\n')\n",
    "    print('        subsetting the larger domain...\\n')\n",
    "    # read the variables for which you want the variogram\n",
    "    ds_big   = xr.open_dataset(RAMS_G1_or_G2_FILE,engine='h5netcdf',phony_dims='sort')[['GLAT','GLON']]\n",
    "    ds_small = xr.open_dataset(RAMS_G3_FILE,engine='h5netcdf',phony_dims='sort')[['GLAT','GLON']]\n",
    "    dim1, dim2 = ds_big.GLAT.dims\n",
    "    #print('        ',ds_big)\n",
    "    print('        -----')\n",
    "    #print('        ',ds_small)\n",
    "    #ds_big = ds_big.rename_dims({'phony_dim_0': 'y','phony_dim_1': 'x'})\n",
    "    #ds_small = ds_small.rename_dims({'phony_dim_0': 'y','phony_dim_1': 'x'})\n",
    "    min_lat_big = ds_big.GLAT.min().values\n",
    "    max_lat_big = ds_big.GLAT.max().values\n",
    "    min_lon_big = ds_big.GLON.min().values\n",
    "    max_lon_big = ds_big.GLON.max().values\n",
    "    print('        min and max lat for big domain = ',min_lat_big,' ',max_lat_big)\n",
    "    print('        min and max lon for big domain = ',min_lon_big,' ',max_lon_big)\n",
    "    #print('        ----')\n",
    "    min_lat_small = ds_small.GLAT.min().values\n",
    "    max_lat_small = ds_small.GLAT.max().values\n",
    "    min_lon_small = ds_small.GLON.min().values\n",
    "    max_lon_small = ds_small.GLON.max().values\n",
    "    print('        min and max lat for small domain = ',min_lat_small,' ',max_lat_small)\n",
    "    print('        min and max lon for small domain = ',min_lon_small,' ',max_lon_small)\n",
    "    print('        ----')\n",
    "    #subset by lat/lon - used so only region covered by inner grid is compared\n",
    "    ds = xr.Dataset({VARIABLE[0]: xr.DataArray(data   = z,  dims   = [dim1,dim2])})\n",
    "    ds = ds.assign(GLAT=ds_big.GLAT)\n",
    "    ds = ds.assign(GLON=ds_big.GLON)\n",
    "    #print('        ',ds)\n",
    "    ds = ds.where((ds.GLAT>=min_lat_small) & (ds.GLAT<=max_lat_small) & (ds.GLON>=min_lon_small) & (ds.GLON<=max_lon_small), drop=True)\n",
    "    #print(ds)\n",
    "    min_lat = ds.GLAT.min().values\n",
    "    max_lat = ds.GLAT.max().values\n",
    "    min_lon = ds.GLON.min().values\n",
    "    max_lon = ds.GLON.max().values\n",
    "    \n",
    "    print('        min and max lat for modified domain = ',min_lat,' ',max_lat)\n",
    "    print('        min and max lon for modified domain = ',min_lon,' ',max_lon)\n",
    "    print('        ----')\n",
    "    \n",
    "    #print(ds)\n",
    "    print('        shape of small domain: ',np.shape(ds_small.GLAT))\n",
    "    print('        shape of big domain: ',np.shape(ds_big.GLAT))\n",
    "    print('        shape of modified domain: ',np.shape(ds.GLAT))\n",
    "    #return z, z_name, z_units, z_time\n",
    "    return ds.variables[VARIABLE[0]].values, z_name, z_units, z_time\n",
    "\n",
    "def save_coincident_pdfs(WHICH_TIME, VARIABLE, SIMULATIONS, DOMAINS, COINCIDENT, POWER_LAW_FITTING, COLORS, PLOT):\n",
    "    \n",
    "    print('working on ',VARIABLE,'\\n')\n",
    "    #if VARIABLE[0]!='W':\n",
    "    #    POWER_LAW_FITTING=False\n",
    "\n",
    "    for ii,simulation in enumerate(SIMULATIONS): \n",
    "        print('  <<working on simulation: ',simulation,'>>\\n')\n",
    "        if PLOT:\n",
    "            if POWER_LAW_FITTING:\n",
    "                fig, axs = plt.subplots(1,2,figsize=(12,8)) \n",
    "                axs = axs.flatten()\n",
    "                ax = axs[0]\n",
    "            else:\n",
    "                fig = plt.figure(figsize=(8,8))\n",
    "                ax     = plt.gca()\n",
    "        for DOMAIN in DOMAINS:\n",
    "            print('    <<working on domain: ',DOMAIN,'>>\\n')\n",
    "            if DOMAIN==1:\n",
    "                dx = 1.6\n",
    "            if DOMAIN==2:\n",
    "                dx=0.4\n",
    "            if DOMAIN==3:\n",
    "                dx=0.1\n",
    "            \n",
    "            if simulation=='PHI2.1-R':\n",
    "                rams_g3_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G'+'3'+'/out_30s/Lite/'+'a-L-*g3.h5'))# CSU machine\n",
    "            else:\n",
    "                rams_g3_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G'+'3'+'/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "            print('        total # files = ',len(rams_g3_files))\n",
    "            print('        first file is ',rams_g3_files[0])\n",
    "            print('        last file is ',rams_g3_files[-1])\n",
    "\n",
    "            if WHICH_TIME=='start':\n",
    "                rams_g3_fil    = rams_g3_files[0]\n",
    "                print('        choosing the start file: ',rams_g3_fil)\n",
    "            if WHICH_TIME=='middle':\n",
    "                rams_g3_fil    = rams_g3_files[int(len(rams_g3_files)/2)]\n",
    "                print('        choosing the middle file: ',rams_g3_fil)\n",
    "            if WHICH_TIME=='end':\n",
    "                rams_g3_fil    = rams_g3_files[-1]\n",
    "                print('        choosing the end file: ',rams_g3_fil)\n",
    "\n",
    "            if DOMAIN==1 or DOMAIN ==2:\n",
    "                print('        searching for domain ',DOMAIN,' file for the same time in the directory: ',\\\n",
    "                      '/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G'+str(DOMAIN)+'/out/')\n",
    "                g3_time = get_time_from_RAMS_file(rams_g3_fil)[2]\n",
    "                print('        time in G3 file ',g3_time)\n",
    "                if simulation=='PHI2.1-R':\n",
    "                    rams_g1_or_g2_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G3/out_30s/Lite/'+'a-L-*g'+str(DOMAIN)+'.h5'))# CSU machine\n",
    "                else:\n",
    "                    rams_g1_or_g2_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G3/out_30s/'+'a-L-*g'+str(DOMAIN)+'.h5'))# CSU machine\n",
    "                print('        found total ',len(rams_g1_or_g2_files),' files for domain ',DOMAIN,' for this simulation')\n",
    "                list_of_times = [get_time_from_RAMS_file(fil)[2] for fil in rams_g1_or_g2_files]\n",
    "                ind_g1_or_g2_file_index = find_closest_datetime_index(list_of_times, g3_time)\n",
    "                rams_larger_grid_file = rams_g1_or_g2_files[ind_g1_or_g2_file_index]\n",
    "                print('        found the file ',rams_larger_grid_file)\n",
    "\n",
    "\n",
    "            if DOMAIN<3:\n",
    "                print('Domain intersection portion...')\n",
    "                z, z_name, z_units, z_time = grab_intersection_gbig_gsmall(VARIABLE,rams_larger_grid_file,rams_g3_fil)\n",
    "            else:\n",
    "                z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(rams_g3_fil,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "\n",
    "\n",
    "            if DOMAIN==1:\n",
    "                linestyle = '-'\n",
    "                color_hist= 'blue'\n",
    "            if DOMAIN==2:\n",
    "                linestyle = '--'\n",
    "                color_hist= 'green'\n",
    "            if DOMAIN==3:\n",
    "                linestyle = ':'\n",
    "                color_hist= 'red'\n",
    "\n",
    "            if PLOT:\n",
    "                flat_data = z.flatten()\n",
    "                \n",
    "                if VARIABLE[0]!='W':\n",
    "                    #ax.hist(flat_data,density=True,histtype='step',color = COLORS[ii],label=simulation+' domain d0'+str(DOMAIN))\n",
    "                    #hist, edges = np.histogram(flat_data, bins='auto', density=True)\n",
    "                    #bin_centers = (edges[1:]+edges[:-1])/2.0\n",
    "                    #ax.plot(bin_centers, hist, color = COLORS[ii],label=simulation+' domain d0'+str(DOMAIN),linestyle=linestyle)\n",
    "                    sns.kdeplot(data=flat_data,fill=False, color=COLORS[ii],label=simulation+' domain d0'+str(DOMAIN)+\\\n",
    "                                      '| $\\mu$:'+str(np.round(np.nanmean(flat_data),4))+\\\n",
    "                                     '; $\\sigma$:'+str(np.round(np.nanstd(flat_data),4)),linestyle=linestyle,ax=ax)\n",
    "                else:\n",
    "                    flat_data = flat_data[flat_data>0.5]\n",
    "                    powerlaw.plot_pdf(flat_data, color = COLORS[ii],label=simulation+' domain d0'+str(DOMAIN)+\\\n",
    "                                      '| $\\mu$:'+str(np.round(np.nanmean(flat_data),4))+\\\n",
    "                                     '; $\\sigma$:'+str(np.round(np.nanstd(flat_data),4)),linestyle=linestyle,ax = ax)\n",
    "                \n",
    "                #ax.axvline(x = np.nanmean(flat_data), color = COLORS[ii], linestyle=linestyle)\n",
    "                ax.set_ylabel('density')\n",
    "                ax.set_xlabel(VARIABLE[3])\n",
    "                ax.legend()\n",
    "                \n",
    "                \n",
    "                if POWER_LAW_FITTING:\n",
    "                    fit = powerlaw.Fit(flat_data,xmin=0.6,ax = axs[0])\n",
    "                    fit.power_law.plot_pdf(color=color_hist, linestyle='-',ax = axs[1],label = 'power law d0'+str(DOMAIN))\n",
    "                    fit.lognormal.plot_pdf( color=color_hist, linestyle='--',ax = axs[1],label = 'lognormal d0'+str(DOMAIN))\n",
    "                    fit.exponential.plot_pdf(color=color_hist, linestyle=':',ax = axs[1],label = 'exponential d0'+str(DOMAIN))\n",
    "                    axs[1].set_ylabel('density')\n",
    "                    axs[1].set_xlabel(VARIABLE[3])\n",
    "                    axs[1].set_title('Curve fitting')\n",
    "                    axs[1].legend()\n",
    "                    print('fitting to power law...')\n",
    "                    print('        exponent: ',fit.power_law.alpha)\n",
    "                    print('        xmin: ',fit.power_law.xmin)\n",
    "                    print('        ----')\n",
    "                    print('        fitting to lognormal...')\n",
    "                    print('        mu: ',fit.lognormal.mu)\n",
    "                    print('        sigma: ',fit.lognormal.sigma)\n",
    "                    print('        -----')\n",
    "                    print('        comparing pairs of distributions...')\n",
    "                    R, p = fit.distribution_compare('power_law', 'lognormal')\n",
    "                    print('        R,p for power law and lognormal: ',R,p)\n",
    "                    #axs[1].text(0.03, 0.98,'R, p for PL and L: '+str(np.round(R,2))+', '+str(np.round(p,2)) , ha='left', va='top', transform=axs[1].transAxes, color=COLORS[ii])\n",
    "\n",
    "                    R, p = fit.distribution_compare('power_law', 'exponential', normalized_ratio=True)\n",
    "                    print('        R,p for power law and exponential: ',R,p)\n",
    "                    #axs[1].text(0.03, 0.93,'R, p for PL and E: '+str(np.round(R,2))+', '+str(np.round(p,2)) , ha='left', va='top', transform=axs[1].transAxes, color=COLORS[ii])\n",
    "\n",
    "                    R, p = fit.distribution_compare('power_law', 'truncated_power_law',)# normalized_ratio=True)\n",
    "                    print('        R,p for power law and truncated power law: ',R,p)\n",
    "                    #axs[1].text(0.03, 0.88,'R, p for PL and TPL: '+str(np.round(R,2))+', '+str(np.round(p,2)) , ha='left', va='top', transform=axs[1].transAxes, color=COLORS[ii])\n",
    "                    \n",
    "                    R, p = fit.distribution_compare('lognormal', 'exponential',)# normalized_ratio=True)\n",
    "                    print('        R,p for lognormal and exponential: ',R,p)\n",
    "                    #axs[1].text(0.03, 0.83,'R, p for L and E: '+str(np.round(R,2))+', '+str(np.round(p,2)) , ha='left', va='top', transform=axs[1].transAxes, color=COLORS[ii])\n",
    "\n",
    "                    plt.tight_layout()\n",
    "                    #plt.hist(flat_data, density=True, color = color_hist, bins='doane', histtype='stepfilled', alpha=0.3)\n",
    "                    #fig4 = fit.plot_ccdf(linewidth=3)\n",
    "                    #fit.power_law.plot_ccdf(ax=fig4, color='r', linestyle='--')\n",
    "                    #fit.lognormal.plot_ccdf(ax=fig4, color='g', linestyle='--')\n",
    "                    #plt.xlim([0,50])\n",
    "        if PLOT:\n",
    "            if VARIABLE[2]:\n",
    "                title_string = 'PDF for '+VARIABLE[0]+' at '+VARIABLE[2]+' level '+str(int(VARIABLE[1]))+'\\nat mid-simulation'\n",
    "            else:\n",
    "                title_string = 'PDF for '+VARIABLE[0]+'\\nat mid-simulation'  \n",
    "                \n",
    "            ax.set_title(title_string)\n",
    "            \n",
    "            if VARIABLE[2]:\n",
    "                filename = 'PDF_RAMS_'+simulation+'_coincident_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_1_sample_no_mask_mid-simulation.png'\n",
    "            else:\n",
    "                filename = 'PDF_RAMS_'+simulation+'_coincident_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_1_sample_no_mask_mid-simulation.png'\n",
    "            print('        saving plot to file: ',filename)\n",
    "            plt.savefig(filename,dpi=150)\n",
    "            #print('\\n\\n')\n",
    "        print('------------------------------------------------------------------------\\n')\n",
    "\n",
    "# PARAMETERS\n",
    "simulations=['AUS1.1-R','BRA1.1-R','PHI2.1-R','DRC1.1-R','PHI1.1-R','WPO1.1-R','USA1.1-R']\n",
    "domains=[1,2,3]\n",
    "variables = [['QV', 0, 'model', '$Qvapor_{sfc} (kgkg^{-1})$']]#['W', 750, 'pressure', '$W_{750} (ms^{-1})$']]\n",
    "variables = [['THETA', 0, 'model', '$Theta_{sfc} (K)$'], ['QV', 0, 'model', '$Qvapor_{sfc} (kgkg^{-1})$'],\\\n",
    "             ['W', 750, 'pressure', '$W_{750} (ms^{-1})$'],['MCAPE', -999, None, '$MCAPE (Jkg^{-1})$']]             \n",
    "# [['TOP_SOIL_MOISTURE', -999, None, '$SM^{2} (m^{3} m^{3})$'], ['LHF', -999, None, '$LHF^{2} (Wm^{-2})$'],\\\n",
    "#              ['SHF', -999, None, '$SHF^{2} (Wm^{-2})$'],\\\n",
    "#              ['Tk', 0, 'model', '$T_{sfc}^{2} (K^{2})$']             , ['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$'], ['RH', 0, 'model', '$RH_{sfc}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 0, 'model', '$U_{sfc}^{2} (m^{2}s^{-2})$']        , ['V', 0, 'model', '$V_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 0, 'model', '$WSPD_{sfc}^{2} (m^{2}s^{-2})$']  , ['W', 0, 'model', '$W_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['MCAPE', -999, None, '$MCAPE^{2} (J^{2}kg^{-2})$']     , ['MCIN', -999, None, '$MCIN^{2} (J^{2}kg^{-2})$'], \\\n",
    "#              ['Tk', 750, 'pressure', '$T_{750}^{2} (K^{2})$']        , ['THETA', 750, 'pressure', '$Theta_{750}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 750, 'pressure', '$Qvapor_{750}^{2} (kg^{2}kg^{-2})$'], ['RH', 750, 'pressure', '$RH_{750}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 750, 'pressure', '$U_{750}^{2} (m^{2}s^{-2})$']   , ['V', 750, 'pressure', '$V_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 750, 'pressure', '$WSPD_{750}^{2} (m^{2}s^{-2})$'], ['W', 750, 'pressure', '$W_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['Tk', 500, 'pressure', '$T_{500}^{2} (K^{2})$']        , ['THETA', 500, 'pressure', '$Theta_{500}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 500, 'pressure', '$Qvapor_{500}^{2} (kg^{2}kg^{-2})$'], ['RH', 500, 'pressure', '$RH_{500}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 500, 'pressure', '$U_{500}^{2} (m^{2}s^{-2})$']   , ['V', 500, 'pressure', '$V_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 500, 'pressure', '$WSPD_{500}^{2} (m^{2}s^{-2})$'], ['W', 500, 'pressure', '$W_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['Tk', 200, 'pressure', '$T_{200}^{2} (K^{2})$']        , ['THETA', 200, 'pressure', '$Theta_{200}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 200, 'pressure', '$Qvapor_{200}^{2} (kg^{2}kg^{-2})$'], ['RH', 200, 'pressure', '$RH_{200}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 200, 'pressure', '$U_{200}^{2} (m^{2}s^{-2})$']   , ['V', 200, 'pressure', '$V_{200}^{2} (m^{2}s^{-2})$'], \\\n",
    "#              ['WSPD', 200, 'pressure', '$WSPD_{200}^{2} (m^{2}s^{-2})$'], ['W', 200, 'pressure', '$W_{200}^{2} (m^{2}s^{-2})$']]\n",
    "colors    =  ['#000000','#E69F00','#56B4E9','#009E73','#000000','#0072B2','#D55E00','#CC79A7']\n",
    "# for variable in variables:\n",
    "#     save_coincident_pdfs('middle', variable, simulations, domains, False, False, colors, True)\n",
    "#     print('=====================================================================================\\n\\n\\n\\n')\n",
    "          \n",
    "          \n",
    "#Running on the terminal in parallel\n",
    "argument = []\n",
    "for variable in variables:\n",
    "    argument = argument + [('middle', variable, simulations, domains, False, False, colors, True)]\n",
    "\n",
    "print('length of argument is: ',len(argument))\n",
    "\n",
    "\n",
    "# # ############################### FIRST OF ALL ################################\n",
    "cpu_count1 = 18 #cpu_count()\n",
    "print('number of cpus: ',cpu_count1)\n",
    "# # #############################################################################\n",
    "\n",
    "def main(FUNCTION, ARGUMENT):\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(processes = (cpu_count1-1)) as pool:\n",
    "        data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "      \n",
    "if __name__ == \"__main__\":\n",
    "    main(save_coincident_pdfs, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4857758-ca6f-4825-8d9a-287bd1d43ee6",
   "metadata": {},
   "source": [
    "### PDF of RESAMPLED data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbe696-b741-4812-ad6f-d4782d25e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile save_coincident_variograms_multiprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as crs\n",
    "import random\n",
    "import skgstat as skg\n",
    "plt.style.use('ggplot')\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import read_vars_WRF_RAMS\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "#from scipy.stats import powerlaw\n",
    "import powerlaw\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S'), pd_time\n",
    "\n",
    "def find_closest_datetime_index(datetime_list, target_datetime):\n",
    "    \"\"\"\n",
    "    Find the index of the closest datetime in the datetime_list to the target_datetime.\n",
    "    \"\"\"\n",
    "    closest_datetime = min(datetime_list, key=lambda x: abs(x - target_datetime))\n",
    "    closest_index = datetime_list.index(closest_datetime)\n",
    "    return closest_index\n",
    "   \n",
    "def read_head(headfile,h5file):\n",
    "        # Function that reads header files from RAMS\n",
    "\n",
    "        # Inputs:\n",
    "        #   headfile: header file including full path in str format\n",
    "        #   h5file: h5 datafile including full path in str format\n",
    "\n",
    "        # Returns:\n",
    "        #   zmn: height levels for momentum values (i.e., grid box upper and lower levels)\n",
    "        #   ztn: height levels for thermodynaic values (i.e., grid box centers)\n",
    "        #   nx:: the number of x points for the domain associated with the h5file\n",
    "        #   ny: the number of y points for the domain associated with the h5file\n",
    "        #   npa: the number of surface patches\n",
    "\n",
    "\n",
    "        dom_num = h5file[h5file.index('.h5')-1] # Find index of .h5 to determine position showing which nest domain to use\n",
    "\n",
    "        with open(headfile) as f:\n",
    "            contents = f.readlines()\n",
    "\n",
    "        idx_zmn = contents.index('__zmn0'+dom_num+'\\n')\n",
    "        nz_m = int(contents[idx_zmn+1])\n",
    "        zmn = np.zeros(nz_m)\n",
    "        for i in np.arange(0,nz_m):\n",
    "            zmn[i] =  float(contents[idx_zmn+2+i])\n",
    "\n",
    "        idx_ztn = contents.index('__ztn0'+dom_num+'\\n')\n",
    "        nz_t = int(contents[idx_ztn+1])\n",
    "        ztn = np.zeros(nz_t)\n",
    "        for i in np.arange(0,nz_t):\n",
    "            ztn[i] =  float(contents[idx_ztn+2+i])\n",
    "\n",
    "        ztop = np.max(ztn) # Model domain top (m)\n",
    "\n",
    "        # Grad the size of the horizontal grid spacing\n",
    "        idx_dxy = contents.index('__deltaxn\\n')\n",
    "        dxy = float(contents[idx_dxy+1+int(dom_num)].strip())\n",
    "\n",
    "        idx_npatch = contents.index('__npatch\\n')\n",
    "        npa = int(contents[idx_npatch+2])\n",
    "\n",
    "        idx_ny = contents.index('__nnyp\\n')\n",
    "        idx_nx = contents.index('__nnxp\\n')\n",
    "        ny = np.ones(int(contents[idx_ny+1]))\n",
    "        nx = np.ones(int(contents[idx_ny+1]))\n",
    "        for i in np.arange(0,len(ny)):\n",
    "            nx[i] = int(contents[idx_nx+2+i])\n",
    "            ny[i] = int(contents[idx_ny+2+i])\n",
    "\n",
    "        ny_out = ny[int(dom_num)-1]\n",
    "        nx_out = nx[int(dom_num)-1]\n",
    "\n",
    "        return zmn, ztn, nx_out, ny_out, dxy, npa \n",
    "\n",
    "def grab_intersection_gbig_gsmall(VARIABLE,RAMS_G1_or_G2_FILE,RAMS_G3_FILE):\n",
    "    z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(RAMS_G1_or_G2_FILE,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "    #print(np.min(z))\n",
    "    #print(np.max(z))\n",
    "    #z2, z_name2, z_units2, z_time2 = read_vars_WRF_RAMS.read_variable(RAMS_G3_FILE,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "    print('        done getting the variable ',VARIABLE[0],' with shape: ',np.shape(z),'\\n')\n",
    "    print('        subsetting the larger domain...\\n')\n",
    "    # read the variables for which you want the variogram\n",
    "    ds_big   = xr.open_dataset(RAMS_G1_or_G2_FILE,engine='h5netcdf',phony_dims='sort')[['GLAT','GLON']]\n",
    "    ds_small = xr.open_dataset(RAMS_G3_FILE,engine='h5netcdf',phony_dims='sort')[['GLAT','GLON']]\n",
    "    dim1, dim2 = ds_big.GLAT.dims\n",
    "    #print('        ',ds_big)\n",
    "    print('        -----')\n",
    "    #print('        ',ds_small)\n",
    "    #ds_big = ds_big.rename_dims({'phony_dim_0': 'y','phony_dim_1': 'x'})\n",
    "    #ds_small = ds_small.rename_dims({'phony_dim_0': 'y','phony_dim_1': 'x'})\n",
    "    min_lat_big = ds_big.GLAT.min().values\n",
    "    max_lat_big = ds_big.GLAT.max().values\n",
    "    min_lon_big = ds_big.GLON.min().values\n",
    "    max_lon_big = ds_big.GLON.max().values\n",
    "    print('        min and max lat for big domain = ',min_lat_big,' ',max_lat_big)\n",
    "    print('        min and max lon for big domain = ',min_lon_big,' ',max_lon_big)\n",
    "    #print('        ----')\n",
    "    min_lat_small = ds_small.GLAT.min().values\n",
    "    max_lat_small = ds_small.GLAT.max().values\n",
    "    min_lon_small = ds_small.GLON.min().values\n",
    "    max_lon_small = ds_small.GLON.max().values\n",
    "    print('        min and max lat for small domain = ',min_lat_small,' ',max_lat_small)\n",
    "    print('        min and max lon for small domain = ',min_lon_small,' ',max_lon_small)\n",
    "    print('        ----')\n",
    "    #subset by lat/lon - used so only region covered by inner grid is compared\n",
    "    ds = xr.Dataset({VARIABLE[0]: xr.DataArray(data   = z,  dims   = [dim1,dim2])})\n",
    "    ds = ds.assign(GLAT=ds_big.GLAT)\n",
    "    ds = ds.assign(GLON=ds_big.GLON)\n",
    "    #print('        ',ds)\n",
    "    ds = ds.where((ds.GLAT>=min_lat_small) & (ds.GLAT<=max_lat_small) & (ds.GLON>=min_lon_small) & (ds.GLON<=max_lon_small), drop=True)\n",
    "    #print(ds)\n",
    "    min_lat = ds.GLAT.min().values\n",
    "    max_lat = ds.GLAT.max().values\n",
    "    min_lon = ds.GLON.min().values\n",
    "    max_lon = ds.GLON.max().values\n",
    "    \n",
    "    print('        min and max lat for modified domain = ',min_lat,' ',max_lat)\n",
    "    print('        min and max lon for modified domain = ',min_lon,' ',max_lon)\n",
    "    print('        ----')\n",
    "    \n",
    "    #print(ds)\n",
    "    print('        shape of small domain: ',np.shape(ds_small.GLAT))\n",
    "    print('        shape of big domain: ',np.shape(ds_big.GLAT))\n",
    "    print('        shape of modified domain: ',np.shape(ds.GLAT))\n",
    "    #return z, z_name, z_units, z_time\n",
    "    return ds.variables[VARIABLE[0]].values, z_name, z_units, z_time\n",
    "\n",
    "def save_resampled_coincident_pdfs(WHICH_TIME, VARIABLE, SIMULATIONS, DOMAINS, COINCIDENT, POWER_LAW_FITTING, COLORS, PLOT):\n",
    "    \n",
    "    print('working on ',VARIABLE,'\\n')\n",
    "    #if VARIABLE[0]!='W':\n",
    "    #    POWER_LAW_FITTING=False\n",
    "\n",
    "    for ii,simulation in enumerate(SIMULATIONS): \n",
    "        print('  <<working on simulation: ',simulation,'>>\\n')\n",
    "        if PLOT:\n",
    "            if POWER_LAW_FITTING:\n",
    "                fig, axs = plt.subplots(1,2,figsize=(12,8)) \n",
    "                axs = axs.flatten()\n",
    "                ax = axs[0]\n",
    "            else:\n",
    "                fig = plt.figure(figsize=(8,8))\n",
    "                ax     = plt.gca()\n",
    "        for DOMAIN in DOMAINS:\n",
    "            print('    <<working on domain: ',DOMAIN,'>>\\n')\n",
    "            if DOMAIN==1:\n",
    "                dx = 1.6\n",
    "            if DOMAIN==2:\n",
    "                dx=0.4\n",
    "            if DOMAIN==3:\n",
    "                dx=0.1\n",
    "            \n",
    "            if simulation=='PHI2.1-R':\n",
    "                rams_g3_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G'+'3'+'/out_30s/Lite/'+'a-L-*g3.h5'))# CSU machine\n",
    "            else:\n",
    "                rams_g3_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G'+'3'+'/out_30s/'+'a-L-*g3.h5'))# CSU machine\n",
    "            print('        total # files = ',len(rams_g3_files))\n",
    "            print('        first file is ',rams_g3_files[0])\n",
    "            print('        last file is ',rams_g3_files[-1])\n",
    "\n",
    "            if WHICH_TIME=='start':\n",
    "                rams_g3_fil    = rams_g3_files[0]\n",
    "                print('        choosing the start file: ',rams_g3_fil)\n",
    "            if WHICH_TIME=='middle':\n",
    "                rams_g3_fil    = rams_g3_files[int(len(rams_g3_files)/2)]\n",
    "                print('        choosing the middle file: ',rams_g3_fil)\n",
    "            if WHICH_TIME=='end':\n",
    "                rams_g3_fil    = rams_g3_files[-1]\n",
    "                print('        choosing the end file: ',rams_g3_fil)\n",
    "\n",
    "            if DOMAIN==1 or DOMAIN ==2:\n",
    "                print('        searching for domain ',DOMAIN,' file for the same time in the directory: ',\\\n",
    "                      '/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G'+str(DOMAIN)+'/out/')\n",
    "                g3_time = get_time_from_RAMS_file(rams_g3_fil)[2]\n",
    "                print('        time in G3 file ',g3_time)\n",
    "                if simulation=='PHI2.1-R':\n",
    "                    rams_g1_or_g2_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G3/out_30s/Lite/'+'a-L-*g'+str(DOMAIN)+'.h5'))# CSU machine\n",
    "                else:\n",
    "                    rams_g1_or_g2_files=sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/V0/'+simulation+'-V0/G3/out_30s/'+'a-L-*g'+str(DOMAIN)+'.h5'))# CSU machine\n",
    "                print('        found total ',len(rams_g1_or_g2_files),' files for domain ',DOMAIN,' for this simulation')\n",
    "                list_of_times = [get_time_from_RAMS_file(fil)[2] for fil in rams_g1_or_g2_files]\n",
    "                ind_g1_or_g2_file_index = find_closest_datetime_index(list_of_times, g3_time)\n",
    "                rams_larger_grid_file = rams_g1_or_g2_files[ind_g1_or_g2_file_index]\n",
    "                print('        found the file ',rams_larger_grid_file)\n",
    "\n",
    "\n",
    "            if DOMAIN<3:\n",
    "                print('Domain intersection portion...')\n",
    "                z, z_name, z_units, z_time = grab_intersection_gbig_gsmall(VARIABLE,rams_larger_grid_file,rams_g3_fil)\n",
    "                \n",
    "            else:\n",
    "                z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable(rams_g3_fil,VARIABLE[0],'RAMS',output_height=False,interpolate=VARIABLE[1]>-1,level=VARIABLE[1],interptype=VARIABLE[2])\n",
    "\n",
    "\n",
    "            if DOMAIN==1:\n",
    "                linestyle = '-'\n",
    "                color_hist= 'blue'\n",
    "            if DOMAIN==2:\n",
    "                # Reshape and average to reduce size by 4 in each dimension\n",
    "                print('Resampling the domain 2 array...')\n",
    "                new_shape = (z.shape[0] // 4, z.shape[1] // 4)\n",
    "                z_temp = z[:new_shape[0]*4, :new_shape[1]*4].reshape(new_shape[0], 4, new_shape[1], 4)\n",
    "                print('        shape of the temporary reshaped array is ',z_temp.shape)\n",
    "                z = np.nanmean(z_temp,axis=(1, 3))\n",
    "                print('        shape of the smaller array is ',z.shape)\n",
    "                linestyle = '--'\n",
    "                color_hist= 'green'\n",
    "            if DOMAIN==3:\n",
    "                print('        Resampling the domain 3 array...')\n",
    "                new_shape = (z.shape[0] // 16, z.shape[1] // 16)\n",
    "                z_temp = z[:new_shape[0]*16, :new_shape[1]*16].reshape(new_shape[0], 16, new_shape[1], 16)\n",
    "                print('        shape of the temporary reshaped array is ',z_temp.shape)\n",
    "                z = np.nanmean(z_temp, axis=(1, 3))\n",
    "                print('        shape of the smaller array is ',z.shape)\n",
    "                linestyle = ':'\n",
    "                color_hist= 'red'\n",
    "\n",
    "            if PLOT:\n",
    "                flat_data = z.flatten()\n",
    "                flat_data = flat_data[~np.isnan(flat_data)]\n",
    "                \n",
    "                if VARIABLE[0]!='W':\n",
    "                    #ax.hist(flat_data,density=True,histtype='step',color = COLORS[ii],label=simulation+' domain d0'+str(DOMAIN))\n",
    "                    #hist, edges = np.histogram(flat_data, bins='auto', density=True)\n",
    "                    #bin_centers = (edges[1:]+edges[:-1])/2.0\n",
    "                    #ax.plot(bin_centers, hist, color = COLORS[ii],label=simulation+' domain d0'+str(DOMAIN),linestyle=linestyle)\n",
    "                    sns.kdeplot(data=flat_data,fill=False, color=COLORS[ii],label=simulation+' domain d0'+str(DOMAIN)+\\\n",
    "                                      '| $\\mu$:'+str(np.round(np.nanmean(flat_data),4))+\\\n",
    "                                     '; $\\sigma$:'+str(np.round(np.nanstd(flat_data),4)),linestyle=linestyle,ax=ax)\n",
    "                else:\n",
    "                    flat_data = flat_data[flat_data>0.5]\n",
    "                    powerlaw.plot_pdf(flat_data, color = COLORS[ii],label=simulation+' domain d0'+str(DOMAIN)+\\\n",
    "                                      '| $\\mu$:'+str(np.round(np.nanmean(flat_data),4))+\\\n",
    "                                     '; $\\sigma$:'+str(np.round(np.nanstd(flat_data),4)),linestyle=linestyle,ax = ax)\n",
    "                \n",
    "                #ax.axvline(x = np.nanmean(flat_data), color = COLORS[ii], linestyle=linestyle)\n",
    "                ax.set_ylabel('density')\n",
    "                ax.set_xlabel(VARIABLE[3])\n",
    "                ax.legend()\n",
    "                \n",
    "                \n",
    "                if POWER_LAW_FITTING:\n",
    "                    fit = powerlaw.Fit(flat_data,xmin=0.6,ax = axs[0])\n",
    "                    fit.power_law.plot_pdf(color=color_hist, linestyle='-',ax = axs[1],label = 'power law d0'+str(DOMAIN))\n",
    "                    fit.lognormal.plot_pdf( color=color_hist, linestyle='--',ax = axs[1],label = 'lognormal d0'+str(DOMAIN))\n",
    "                    fit.exponential.plot_pdf(color=color_hist, linestyle=':',ax = axs[1],label = 'exponential d0'+str(DOMAIN))\n",
    "                    axs[1].set_ylabel('density')\n",
    "                    axs[1].set_xlabel(VARIABLE[3])\n",
    "                    axs[1].set_title('Curve fitting')\n",
    "                    axs[1].legend()\n",
    "                    print('fitting to power law...')\n",
    "                    print('        exponent: ',fit.power_law.alpha)\n",
    "                    print('        xmin: ',fit.power_law.xmin)\n",
    "                    print('        ----')\n",
    "                    print('        fitting to lognormal...')\n",
    "                    print('        mu: ',fit.lognormal.mu)\n",
    "                    print('        sigma: ',fit.lognormal.sigma)\n",
    "                    print('        -----')\n",
    "                    print('        comparing pairs of distributions...')\n",
    "                    R, p = fit.distribution_compare('power_law', 'lognormal')\n",
    "                    print('        R,p for power law and lognormal: ',R,p)\n",
    "                    #axs[1].text(0.03, 0.98,'R, p for PL and L: '+str(np.round(R,2))+', '+str(np.round(p,2)) , ha='left', va='top', transform=axs[1].transAxes, color=COLORS[ii])\n",
    "\n",
    "                    R, p = fit.distribution_compare('power_law', 'exponential', normalized_ratio=True)\n",
    "                    print('        R,p for power law and exponential: ',R,p)\n",
    "                    #axs[1].text(0.03, 0.93,'R, p for PL and E: '+str(np.round(R,2))+', '+str(np.round(p,2)) , ha='left', va='top', transform=axs[1].transAxes, color=COLORS[ii])\n",
    "\n",
    "                    R, p = fit.distribution_compare('power_law', 'truncated_power_law',)# normalized_ratio=True)\n",
    "                    print('        R,p for power law and truncated power law: ',R,p)\n",
    "                    #axs[1].text(0.03, 0.88,'R, p for PL and TPL: '+str(np.round(R,2))+', '+str(np.round(p,2)) , ha='left', va='top', transform=axs[1].transAxes, color=COLORS[ii])\n",
    "                    \n",
    "                    R, p = fit.distribution_compare('lognormal', 'exponential',)# normalized_ratio=True)\n",
    "                    print('        R,p for lognormal and exponential: ',R,p)\n",
    "                    #axs[1].text(0.03, 0.83,'R, p for L and E: '+str(np.round(R,2))+', '+str(np.round(p,2)) , ha='left', va='top', transform=axs[1].transAxes, color=COLORS[ii])\n",
    "\n",
    "                    plt.tight_layout()\n",
    "                    #plt.hist(flat_data, density=True, color = color_hist, bins='doane', histtype='stepfilled', alpha=0.3)\n",
    "                    #fig4 = fit.plot_ccdf(linewidth=3)\n",
    "                    #fit.power_law.plot_ccdf(ax=fig4, color='r', linestyle='--')\n",
    "                    #fit.lognormal.plot_ccdf(ax=fig4, color='g', linestyle='--')\n",
    "                    #plt.xlim([0,50])\n",
    "        if PLOT:\n",
    "            if VARIABLE[2]:\n",
    "                title_string = 'PDF for resampled '+VARIABLE[0]+' at '+VARIABLE[2]+' level '+str(int(VARIABLE[1]))+'\\nat mid-simulation'\n",
    "            else:\n",
    "                title_string = 'PDF for resampled '+VARIABLE[0]+'\\nat mid-simulation'  \n",
    "                \n",
    "            ax.set_title(title_string)\n",
    "            \n",
    "            if VARIABLE[2]:\n",
    "                filename = 'PDF_RAMS_'+simulation+'_resampled_coincident_'+VARIABLE[0]+'_levtype_'+VARIABLE[2]+'_lev_'+str(int(VARIABLE[1]))+'_1_sample_no_mask_mid-simulation.png'\n",
    "            else:\n",
    "                filename = 'PDF_RAMS_'+simulation+'_resampled_coincident_'+VARIABLE[0]+'_levtype_'+'None'+'_lev_'+'None'+'_1_sample_no_mask_mid-simulation.png'\n",
    "            print('        saving plot to file: ',filename)\n",
    "            plt.savefig(filename,dpi=150)\n",
    "            #print('\\n\\n')\n",
    "        print('------------------------------------------------------------------------\\n')\n",
    "\n",
    "# PARAMETERS\n",
    "simulations=['AUS1.1-R','BRA1.1-R','PHI2.1-R','DRC1.1-R','PHI1.1-R','WPO1.1-R','USA1.1-R']\n",
    "domains=[1,2,3]\n",
    "#variables = [['QV', 0, 'model', '$Qvapor_{sfc} (kgkg^{-1})$']]#['W', 750, 'pressure', '$W_{750} (ms^{-1})$']]\n",
    "variables = [['THETA', 0, 'model', '$Theta_{sfc} (K)$'], ['QV', 0, 'model', '$Qvapor_{sfc} (kgkg^{-1})$'],\\\n",
    "             ['W', 750, 'pressure', '$W_{750} (ms^{-1})$'],['MCAPE', -999, None, '$MCAPE (Jkg^{-1})$']]             \n",
    "# [['TOP_SOIL_MOISTURE', -999, None, '$SM^{2} (m^{3} m^{3})$'], ['LHF', -999, None, '$LHF^{2} (Wm^{-2})$'],\\\n",
    "#              ['SHF', -999, None, '$SHF^{2} (Wm^{-2})$'],\\\n",
    "#              ['Tk', 0, 'model', '$T_{sfc}^{2} (K^{2})$']             , ['THETA', 0, 'model', '$Theta_{sfc}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 0, 'model', '$Qvapor_{sfc}^{2} (kg^{2}kg^{-2})$'], ['RH', 0, 'model', '$RH_{sfc}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 0, 'model', '$U_{sfc}^{2} (m^{2}s^{-2})$']        , ['V', 0, 'model', '$V_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 0, 'model', '$WSPD_{sfc}^{2} (m^{2}s^{-2})$']  , ['W', 0, 'model', '$W_{sfc}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['MCAPE', -999, None, '$MCAPE^{2} (J^{2}kg^{-2})$']     , ['MCIN', -999, None, '$MCIN^{2} (J^{2}kg^{-2})$'], \\\n",
    "#              ['Tk', 750, 'pressure', '$T_{750}^{2} (K^{2})$']        , ['THETA', 750, 'pressure', '$Theta_{750}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 750, 'pressure', '$Qvapor_{750}^{2} (kg^{2}kg^{-2})$'], ['RH', 750, 'pressure', '$RH_{750}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 750, 'pressure', '$U_{750}^{2} (m^{2}s^{-2})$']   , ['V', 750, 'pressure', '$V_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 750, 'pressure', '$WSPD_{750}^{2} (m^{2}s^{-2})$'], ['W', 750, 'pressure', '$W_{750}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['Tk', 500, 'pressure', '$T_{500}^{2} (K^{2})$']        , ['THETA', 500, 'pressure', '$Theta_{500}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 500, 'pressure', '$Qvapor_{500}^{2} (kg^{2}kg^{-2})$'], ['RH', 500, 'pressure', '$RH_{500}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 500, 'pressure', '$U_{500}^{2} (m^{2}s^{-2})$']   , ['V', 500, 'pressure', '$V_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['WSPD', 500, 'pressure', '$WSPD_{500}^{2} (m^{2}s^{-2})$'], ['W', 500, 'pressure', '$W_{500}^{2} (m^{2}s^{-2})$'],\\\n",
    "#              ['Tk', 200, 'pressure', '$T_{200}^{2} (K^{2})$']        , ['THETA', 200, 'pressure', '$Theta_{200}^{2} (K^{2})$'],\\\n",
    "#              ['QV', 200, 'pressure', '$Qvapor_{200}^{2} (kg^{2}kg^{-2})$'], ['RH', 200, 'pressure', '$RH_{200}^{2} (percent^{2})$'],\\\n",
    "#              ['U', 200, 'pressure', '$U_{200}^{2} (m^{2}s^{-2})$']   , ['V', 200, 'pressure', '$V_{200}^{2} (m^{2}s^{-2})$'], \\\n",
    "#              ['WSPD', 200, 'pressure', '$WSPD_{200}^{2} (m^{2}s^{-2})$'], ['W', 200, 'pressure', '$W_{200}^{2} (m^{2}s^{-2})$']]\n",
    "colors    =  ['#000000','#E69F00','#56B4E9','#009E73','#000000','#0072B2','#D55E00','#CC79A7']\n",
    "# for variable in variables:\n",
    "#     save_coincident_pdfs('middle', variable, simulations, domains, False, False, colors, True)\n",
    "#     print('=====================================================================================\\n\\n\\n\\n')\n",
    "          \n",
    "          \n",
    "#Running on the terminal in parallel\n",
    "argument = []\n",
    "for variable in variables:\n",
    "    argument = argument + [('middle', variable, simulations, domains, False, False, colors, True)]\n",
    "\n",
    "print('length of argument is: ',len(argument))\n",
    "\n",
    "\n",
    "# # ############################### FIRST OF ALL ################################\n",
    "cpu_count1 = 18 #cpu_count()\n",
    "print('number of cpus: ',cpu_count1)\n",
    "# # #############################################################################\n",
    "\n",
    "def main(FUNCTION, ARGUMENT):\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(processes = (cpu_count1-1)) as pool:\n",
    "        data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "      \n",
    "if __name__ == \"__main__\":\n",
    "    main(save_resampled_coincident_pdfs, argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d10f53-7bdb-44d6-baaa-335ea6de4324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_vars_WRF_RAMS\n",
    "\n",
    "z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable('/monsoon/MODEL/LES_MODEL_DATA/V0/PHI1.1-R-V0/G3/out_30s/a-L-2019-09-10-121900-g1.h5',\\\n",
    "                                                              'W','RAMS',output_height=False,interpolate=True,level=750,interptype='pressure')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36188009-7d83-4068-94cd-d81bea769140",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(z))\n",
    "print(np.shape(z)[0]/4)\n",
    "print(np.shape(z)[1]/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89747174-84a3-455a-896b-c70334f67f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and average to reduce size by 4 in each dimension\n",
    "new_shape = (z.shape[0] // 4, z.shape[1] // 4)\n",
    "reduced_array = z[:new_shape[0]*4, :new_shape[1]*4].reshape(new_shape[0], 4, new_shape[1], 4).mean(axis=(1, 3))\n",
    "\n",
    "# Example: Access reduced array\n",
    "print(reduced_array.shape)  # Output: (50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42884e50-fc76-4c36-b1c6-e97ba00b45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(reduced_array,cmap='seismic',vmin=-20,vmax=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca64a9-dca1-4378-bfd7-bfc3357115be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(z,cmap='seismic',vmin=-20,vmax=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6a032-be14-4021-bc42-d51d413a9316",
   "metadata": {},
   "source": [
    "### Power law testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415dfef-22a7-40c7-a6e3-7d758a0aceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_vars_WRF_RAMS\n",
    "import powerlaw\n",
    "\n",
    "z, z_name, z_units, z_time = read_vars_WRF_RAMS.read_variable('/monsoon/MODEL/LES_MODEL_DATA/V0/PHI1.1-R-V0/G3/out_30s/a-L-2019-09-10-121900-g1.h5',\\\n",
    "                                                              'W','RAMS',output_height=False,interpolate=True,level=750,interptype='pressure')\n",
    "flat_data = z.flatten()\n",
    "flat_data = flat_data[flat_data>0.5]\n",
    "print(len(flat_data))\n",
    "print(np.min(flat_data))\n",
    "print(np.max(flat_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49857e9e-ad5d-4056-8e9b-fc4801d9b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "powerlaw.plot_pdf(flat_data, color='b')\n",
    "#powerlaw.plot_pdf(flat_data, linear_bins=True, color='r')\n",
    "plt.hist(flat_data, density=True, color = 'green', bins='doane', histtype='stepfilled', alpha=0.3)\n",
    "plt.axvline(x = 3.2, color = 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c72cd8-abe0-4698-bc91-b03ce0f9a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#powerlaw.Fit(xmin=min(flat_data),xmax=max(flat_data),discrete=False,fit_method='Likelihood',\\\n",
    "# data=flat_data,discrete_approximation='round', parent_Fit=None)\n",
    "\n",
    "fit = powerlaw.Fit(flat_data)\n",
    "print(fit.power_law.alpha)\n",
    "print(fit.power_law.xmin)\n",
    "print(fit.fixed_xmin)\n",
    "#print(results.power_law.parameter1_name)\n",
    "print(fit.power_law.D)\n",
    "print('----')\n",
    "print(fit.lognormal.mu)\n",
    "print(fit.lognormal.sigma)\n",
    "#print(dir(fit.lognormal))\n",
    "print('-----')\n",
    "R, p = fit.distribution_compare('power_law', 'lognormal')\n",
    "print(R,p)\n",
    "R, p = fit.distribution_compare('power_law', 'exponential', normalized_ratio=True)\n",
    "print(R,p)\n",
    "R, p = fit.distribution_compare('power_law', 'truncated_power_law',)# normalized_ratio=True)\n",
    "print(R,p)\n",
    "\n",
    "powerlaw.plot_pdf(flat_data, color='b')\n",
    "plt.hist(flat_data, density=True, color = 'green', bins='doane', histtype='stepfilled', alpha=0.3)\n",
    "#fig4 = fit.plot_ccdf(linewidth=3)\n",
    "#fit.power_law.plot_ccdf(ax=fig4, color='r', linestyle='--')\n",
    "#fit.lognormal.plot_ccdf(ax=fig4, color='g', linestyle='--')\n",
    "#plt.xlim([0,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9b529-8fa7-4443-ae81-4e0604f36399",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit.xmins, fit.Ds, color='r')\n",
    "plt.plot(fit.xmins, fit.sigmas, color='g')\n",
    "plt.plot(fit.xmins, fit.sigmas/fit.alphas, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e9440-f719-448a-a814-360efe42fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.supported_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0c958-85a0-4dac-929d-f575a8476cca",
   "metadata": {},
   "source": [
    "# Jennie CMF/anvil testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e86136-4322-4530-b744-2c2f569c39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import powerlaw\n",
    "import scipy\n",
    "#from scipy.stats import powerlaw\n",
    "\n",
    "\n",
    "# Generate example data\n",
    "# Define parameters\n",
    "a = 2.5  # Shape parameter (> 1)\n",
    "\n",
    "# Generate random samples\n",
    "data = scipy.stats.powerlaw.rvs(a, size=1000)\n",
    "#rv = powerlaw(a)\n",
    "#ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
    "#print(data)\n",
    "#data = np.random.pareto(a=2, size=1000)  # Example data, replace this with your 1D numpy array\n",
    "\n",
    "# Create figure and main axis\n",
    "fig, ax_main = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot PDF using seaborn kdeplot\n",
    "#sns.kdeplot(data, ax=ax_main)\n",
    "ax_main.hist(data, density=True, bins='auto')#, histtype='stepfilled', alpha=0.2)\n",
    "ax_main.set_title('Probability Density Function (PDF)')\n",
    "ax_main.set_xlabel('Value')\n",
    "ax_main.set_ylabel('Density')\n",
    "\n",
    "# Create inset axis for log-log plot\n",
    "ax_inset = inset_axes(ax_main, width=\"30%\", height=\"30%\", loc='upper left')\n",
    "\n",
    "# Plot log-log plot in inset\n",
    "sns.kdeplot(data, fill=False,ax=ax_inset)#density=True, alpha=0.5)\n",
    "ax_inset.set_xscale('log')\n",
    "ax_inset.set_yscale('log')\n",
    "#ax_inset.set_title('Log-Log Plot')\n",
    "ax_inset.set_xlabel('Value')\n",
    "ax_inset.set_ylabel('Density')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff272c-a940-45e2-8db9-939c906aea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import powerlaw\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "#ax = axs.flatten()\n",
    "a = .659\n",
    "data = scipy.stats.powerlaw.rvs(a, size=100000)\n",
    "x = np.linspace(powerlaw.ppf(0.01, a),powerlaw.ppf(0.99, a), 100)\n",
    "ax.plot(x, powerlaw.pdf(x, a),'r-', lw=5, alpha=0.6, label='powerlaw pdf')\n",
    "ax.hist(data, density=True, bins='auto', histtype='stepfilled', alpha=0.6)\n",
    "\n",
    "#data = powerlaw.rvs(a, size=1000)\n",
    "sns.kdeplot(data, ax=ax)\n",
    "# ax.set_xlim([x[0], x[-1]])\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c662e3-8b04-42e8-a05d-2d8308132ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.draw import ellipse\n",
    "from skimage.measure import label, regionprops, regionprops_table, find_contours\n",
    "from skimage.transform import rotate\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage import filters\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import glob\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import read_vars_WRF_RAMS\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def get_time_from_RAMS_GeoIR_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "simulations= ['USA1.1-R','ARG1.1-R_old','PHI1.1-R','PHI2.1-R','WPO1.1-R','BRA1.1-R','DRC1.1-R','AUS1.1-R']\n",
    "properties = ['area' , 'intensity_mean']\n",
    "WHICH_TIME = 'middle'\n",
    "CMF_level = 400 # pressure level to calculate CMF\n",
    "simulation = simulations[3] # choose 'PHI2.1-R' only for now\n",
    "\n",
    "data_list =[] # list to store dictionaries to anvils and their properties and the associated 400 hPa CMF\n",
    "\n",
    "geo_ir_files=sorted(glob.glob('/monsoon/MODEL/FORWARD_MODEL_DATA/'+simulation+'/GEO_IR/*.nc'))# grab GEO IR files for this simulation on the CSU machine\n",
    "print('total # GeoIR files in the folder = ',len(geo_ir_files))\n",
    "print('first file is ',geo_ir_files[0])\n",
    "print('last file is ',geo_ir_files[-1])\n",
    "if WHICH_TIME=='start':\n",
    "    geo_ir_fil    = geo_ir_files[0]\n",
    "if WHICH_TIME=='middle':\n",
    "    geo_ir_fil    = geo_ir_files[int(len(geo_ir_files)/2)]\n",
    "if WHICH_TIME=='end':\n",
    "    geo_ir_fil    = geo_ir_files[-1]\n",
    "print('choosing the ',WHICH_TIME,' file: ',geo_ir_fil)\n",
    "\n",
    "da = xr.open_dataset(geo_ir_fil) # open this GEO IR file\n",
    "BT_values = da.Tb[-1,:,:].values# grab the brightness temp values\n",
    "\n",
    "#plt.imshow(BT_values)\n",
    "img         = BT_values\n",
    "threshold   = 200.0         # change BT threshold (in K) here \n",
    "bw          = closing(img < threshold , square(3)) # apply fixed threshold\n",
    "cleared     = clear_border(bw)             # remove artifacts connected to image border\n",
    "labels      = label(cleared)           #  label the different regions\n",
    "regions     = regionprops(labels, img) # grab the regions and put them in a list called regions. We'll deal with only this list.\n",
    "print('found ',len(regions),' anvil regions')\n",
    "if len(regions)==0:\n",
    "    print('regionprops could not find any anvil < 200 K !!!\\n\\n\\n')\n",
    "\n",
    "# get corresponding RAMS file: \n",
    "rams_fil  = sorted(glob.glob('/monsoon/MODEL/LES_MODEL_DATA/'+simulation+'/G1/out/'+os.path.basename(geo_ir_fil)[0:20]+'*.h5'))[0]\n",
    "print('found corresponding RAMS file',rams_fil)\n",
    "print('calculating CMF at ',CMF_level,' hPa ...')\n",
    "w, w_name, w_units, w_time         = read_vars_WRF_RAMS.read_variable(rams_fil,'W','RAMS',output_height=False,interpolate=True,level=CMF_level,interptype='pressure')\n",
    "rho, rho_name, rho_units, rho_time = read_vars_WRF_RAMS.read_variable(rams_fil,'RHO','RAMS',output_height=False,interpolate=True,level=CMF_level,interptype='pressure')\n",
    "cmf_400hPa = np.where(w > 0.0, w, np.nan)*rho\n",
    "\n",
    "# Set a threshold for the minimum area you want to keep\n",
    "min_area_threshold = 30  # probabaly need to increase this\n",
    "# Create a list to store accepted regions\n",
    "accepted_regions   = []  # empty list to store regions that meet the area criterion\n",
    "# Iterate through regions and reject based on area\n",
    "for region in regions:\n",
    "    if region.area >= min_area_threshold:\n",
    "        accepted_regions.append(region)\n",
    "print('based on the area threshold, ',len(accepted_regions),' were detected')\n",
    "\n",
    "##### This snippet plots the different segmented regions (anvils) #######\n",
    "##### comment out if the plot is not needed ###############\n",
    "fig = px.imshow(img, binary_string=True)\n",
    "#fig.update_traces(hoverinfo='skip') # hover is only for label info\n",
    "\n",
    "# For each label, add a filled scatter trace for its contour,\n",
    "# and display the properties of the label in the hover of this trace.\n",
    "for index in range(len(accepted_regions)):\n",
    "    label_i = accepted_regions[index].label\n",
    "    contour = find_contours(labels == label_i, 0.5)[0]\n",
    "    y, x = contour.T\n",
    "    hoverinfo = ''\n",
    "    for prop_name in properties:\n",
    "        hoverinfo += f'<b>{prop_name}: {getattr(accepted_regions[index], prop_name):.2f}</b><br>'\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x, y=y, name=label_i,\n",
    "        mode='lines', fill='toself', showlegend=False,\n",
    "        hovertemplate=hoverinfo, hoveron='points+fills'))\n",
    "plotly.io.show(fig)\n",
    "# fig.write_image(\"plotly_anvil_test.png\") save this image; need to install kaleido package for that\n",
    "########## Plotting snipped end #######\n",
    "\n",
    "\n",
    "# go through each of the anvils that make it through the area criterion\n",
    "for ii, anvil in enumerate(accepted_regions):\n",
    "    print('working on anvil#',ii, 'with label ',accepted_regions[ii].label)\n",
    "    for prop_name in properties:\n",
    "        print(prop_name,': ',getattr(anvil, prop_name))\n",
    "    chosen_area_label_coords_array = anvil.coords             # mask creation\n",
    "    rows = [uu[0] for uu in chosen_area_label_coords_array]   # mask creation\n",
    "    cols = [uu[1] for uu in chosen_area_label_coords_array]   # mask creation\n",
    "    segmented_arr = np.zeros_like(img)*np.nan                 # mask creation\n",
    "    segmented_arr[rows, cols] = 1.0                           # mask creation\n",
    "    #fig = plt.figure()\n",
    "    #plt.imshow(segmented_arr)                                # I've commented out the code to create images for all the masks\n",
    "    #plt.colorbar()\n",
    "    #plt.title('anvil #'+str(ii)+'\\n area = '+str(getattr(anvil, 'area'))+'\\n label: '+str(accepted_regions[ii].label))\n",
    "   \n",
    "    anvil_mean_cmf = np.nanmean(cmf_400hPa*segmented_arr)     # calculate mean anvil cmf mean(cmf*mask)\n",
    "    print('mean anvil cmf:',anvil_mean_cmf)\n",
    "    # area is in pixel^2\n",
    "    current_dict = {\n",
    "        'anvil#': ii, \n",
    "        'area': getattr(anvil, 'area'),\n",
    "        'perimeter': getattr(anvil, 'perimeter'),\n",
    "        'intensity_mean'  : getattr(anvil, 'intensity_mean'),\n",
    "        'anvil_mean_cmf':anvil_mean_cmf\n",
    "    }    # store all the information in a dictionary\n",
    "    \n",
    "    # Appending the current dictionary to the list\n",
    "    data_list.append(current_dict)\n",
    "    print('---------\\n')\n",
    "    \n",
    "# Display the list of dictionaries\n",
    "print(data_list)\n",
    "# make a scatter plot of anvil area v. anvil_mean_cmf\n",
    "fig1 = plt.figure()\n",
    "for element in data_list:\n",
    "    plt.scatter(element['area'],element['anvil_mean_cmf'])\n",
    "    plt.xlabel('Anvil area')\n",
    "    plt.ylabel('Anvil mean CMF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150476a3-f984-47fd-8771-5e9dd5b32147",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.array([1,2,3,np.nan,5,6])\n",
    "print(aa>3)\n",
    "print(aa[aa>3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b6532-2a53-4a5a-8232-56947739d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_choice_criteria='closest'\n",
    "# choose the region whose centroid is closest to the centroid of tobac cell in question\n",
    "if region_choice_criteria=='closest':\n",
    "    centroids = [r.centroid for r in regions] #  y0, x0 = props.centroid; note the order 'y' and 'x'\n",
    "    areas     = [r.area for r in regions]\n",
    "    dist      = []  # calculate distances of the centroids of ud from the given point\n",
    "    for centt in centroids:\n",
    "        centt = np.array((centt))  # convert tuple to np array\n",
    "        dist.append(np.linalg.norm(centt - np.array([np.shape(vertical_vel)[0]//2,np.shape(vertical_vel)[1]//2])))\n",
    "\n",
    "    ind_closest_ud = np.argmin(np.array(dist))\n",
    "\n",
    "    if areas[ind_closest_ud] <0.5:\n",
    "        print('area of the selected updraft is too small... moving on to the next timestep\\n*\\n*\\n*\\n')\n",
    "        continue\n",
    "\n",
    "    print('distances of all the detected updrafts from the tobac thermal centroid are: ', dist)\n",
    "\n",
    "    cell_area          = areas[ind_closest_ud]\n",
    "    chosen_region      = regions[ind_closest_ud]\n",
    "    chosen_centroid    = centroids[ind_closest_ud]\n",
    "    chosen_centroid    = list(chosen_centroid) \n",
    "    chosen_centroid[0] = int(chosen_centroid[0])   # convert to integer # Y\n",
    "    chosen_centroid[1] = int(chosen_centroid[1])                        # X\n",
    "    print('    index of chosen region is ',     ind_closest_ud)\n",
    "    print('    centroid of chosen region is ', chosen_centroid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad8c49-704c-4b48-b502-8a557d7b42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "print(len(coords_all))\n",
    "random.seed(1)\n",
    "list1 = random.sample(coords_all,10000)\n",
    "list1_products = [element[0]*element[1] for element in list1]\n",
    "random.seed(10000)\n",
    "list2 = random.sample(coords_all,10000)\n",
    "list2_products = [element[0]*element[1] for element in list2]\n",
    "common_elements = set(list1_products) & set(list2_products)\n",
    "\n",
    "print(len(common_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115252e-c288-420f-9238-3561cd81faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(V.bins,group_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f9c95-ac08-4cc7-8a2b-1a6a125fa1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    " V.distance_difference_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b78511-8780-4bbf-a792-6db5d429dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.errstate(all='raise', over='raise', under='ignore', divide='ignore'):\n",
    "    dx = 100./1000. # in kms\n",
    "    dy = 100./1000. # in kms\n",
    "    y = np.arange(0,da.dims['phony_dim_1']*dy,dy)[800:-800].astype(np.float32)\n",
    "    x = np.arange(0,da.dims['phony_dim_2']*dx,dx)[800:-800].astype(np.float32)\n",
    "    print(np.shape(y))\n",
    "    print(np.shape(x))\n",
    "\n",
    "    xx,yy = np.meshgrid(x,y)\n",
    "    print(np.shape(yy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda804b4-c986-452b-9391-54eb4fe03b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample numpy arrays (replace these with your actual data)\n",
    "X = xx.flatten()\n",
    "Y = yy.flatten()\n",
    "Values = da['UP'][0,800:-800,800:-800].values.flatten()\n",
    "print(np.shape(da['UP'][0,800:-800,800:-800].values))\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "del(xx,yy)\n",
    "# Combine the coordinates (X, Y) and field values into a single array\n",
    "data = np.column_stack((X, Y, Values))\n",
    "\n",
    "# Calculate pairwise Euclidean distances between all data points\n",
    "distances = pdist(data[:, :2])\n",
    "print('calculated distances')\n",
    "# Create a square distance matrix from the pairwise distances\n",
    "distance_matrix = squareform(distances)\n",
    "print('distance_matrix')\n",
    "# Calculate the squared differences in field values\n",
    "semivariance = pdist(data[:, 2:], 'sqeuclidean')\n",
    "\n",
    "# Define lag distances (you can adjust the number and range as needed)\n",
    "lag_distances = np.linspace(0, distance_matrix.max(), 10)\n",
    "\n",
    "# Create a list to store the semivariance for each lag distance\n",
    "semivariance_values = []\n",
    "\n",
    "# Calculate the semivariance for each lag distance\n",
    "for lag in lag_distances:\n",
    "    bin_mask = (distance_matrix >= lag) & (distance_matrix < lag + (distance_matrix.max() / len(lag_distances)))\n",
    "    n_pairs = bin_mask.sum()\n",
    "    if n_pairs > 0:\n",
    "        semivariance_values.append(semivariance[bin_mask].mean())\n",
    "    else:\n",
    "        semivariance_values.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439515f5-3dab-4047-a694-8c041c8dae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame (replace this with your actual data)\n",
    "data = {'X': np.arange(0,100,1),\n",
    "        'Y':  np.arange(0,100,1),\n",
    "        'Value': np.random.random(100)}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Combine the coordinates (X, Y) and field values into a single array\n",
    "data = np.column_stack((df['X'], df['Y'], df['Value']))\n",
    "\n",
    "# Calculate pairwise Euclidean distances between all data points\n",
    "distances = pdist(data[:, :2])\n",
    "print('shape of distances: ', np.shape(distances))\n",
    "# Create a square distance matrix from the pairwise distances\n",
    "distance_matrix = squareform(distances)\n",
    "print('shape of distance_matrix: ', np.shape(distance_matrix))\n",
    "# Create a list to store the semivariance for each lag distance\n",
    "lags = np.linspace(0, distance_matrix.max(), 10)\n",
    "semivariance_values = []\n",
    "\n",
    "# Calculate the semivariance for each lag distance\n",
    "for lag in lags:\n",
    "    semivariance = 0\n",
    "    count = 0\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i + 1, len(df)):\n",
    "            dx = df.iloc[i]['X'] - df.iloc[j]['X']\n",
    "            dy = df.iloc[i]['Y'] - df.iloc[j]['Y']\n",
    "            dist = np.sqrt(dx**2 + dy**2)\n",
    "            if dist >= lag and dist < lag + (distance_matrix.max() / len(lags)):\n",
    "                semivariance += (df.iloc[i]['Value'] - df.iloc[j]['Value'])**2\n",
    "                count += 1\n",
    "    if count > 0:\n",
    "        semivariance /= (2 * count)\n",
    "    semivariance_values.append(semivariance)\n",
    "\n",
    "# Plot the variogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(lags, semivariance_values, 'bo-', label='Experimental Variogram')\n",
    "plt.xlabel('Lag Distance')\n",
    "plt.ylabel('Semivariance')\n",
    "plt.title('Variogram Plot')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25109894",
   "metadata": {},
   "source": [
    "# Get environmental soundings: Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2094ddc",
   "metadata": {},
   "source": [
    "## Create csv files for thermal surroundings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870a7e6-d4c5-4cf1-ba10-2962db480e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_environmental_soundings_multiprocessing_2mps_copy.py\n",
    "# masking of other updrafts\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as colors\n",
    "import datetime\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import hdf5plugin\n",
    "import h5py\n",
    "# have to install h5netcdf for xarray to read .h5 files\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import cartopy.crs as crs\n",
    "\n",
    "import matplotlib.axis as maxis\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.spines as mspines\n",
    "import matplotlib.transforms as transforms\n",
    "import metpy.calc as mpcalc\n",
    "import sharppy.sharptab.interp as interp\n",
    "import sharppy.sharptab.params as params\n",
    "import sharppy.sharptab.profile as profile\n",
    "import sharppy.sharptab.thermo as thermo\n",
    "import sharppy.sharptab.utils as utils\n",
    "import sharppy.sharptab.winds as winds\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.projections import register_projection\n",
    "from metpy.cbook import get_test_data\n",
    "from metpy.plots import Hodograph, SkewT\n",
    "from metpy.units import units\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "from cartopy.geodesic import Geodesic\n",
    "import shapely.geometry as sgeom\n",
    "\n",
    "from skimage.draw import ellipse\n",
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage.transform import rotate\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.segmentation import clear_border\n",
    "\n",
    "import csv\n",
    "import random\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import time\n",
    "\n",
    "from RAMS_functions import read_head, get_time_from_RAMS_file\n",
    "from sounding_functions import create_indices, plot_skewT, plot_area_average_sounding_RAMS_around_point\n",
    "\n",
    "def create_circular_mask(h, w, keep_in_or_out, center=None, radius=None):\n",
    "\n",
    "    if center is None: # use the middle of the image\n",
    "        center = (int(w/2), int(h/2))\n",
    "    if radius is None: # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w-center[0], h-center[1])\n",
    "\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((X - center[0])**2 + (Y-center[1])**2)\n",
    "\n",
    "    if keep_in_or_out=='in':\n",
    "        mask = np.where(dist_from_center <= radius, 1.0, np.nan)\n",
    "    elif keep_in_or_out=='out':\n",
    "        mask = np.where(dist_from_center >= radius, 1.0, np.nan)\n",
    "    else:\n",
    "        print('Please provide <<in>> or <<out>> values for the --keep_in_or_out-- argument')\n",
    "    return mask\n",
    "\n",
    "def create_environmental_soundings_parallel(DOMAIN,DXY,DF_CELL,CELL_NO,ZM,ZT,PLOTTING_RANGE,FIXED_THREHOLD,MASK_OTHER_UPDRAFTS):\n",
    "    \n",
    "    # declare function-wide parameters\n",
    "    Cp=1004.0\n",
    "    Rd=287.0\n",
    "    p00 = 100000.0\n",
    "    ##############\n",
    "    # Retreive all positions of this cell\n",
    "    print('\\n\\n=============================================================')\n",
    "    print('working on cell # ',CELL_NO)\n",
    "    print('this cell has '+str(len(DF_CELL))+' time steps -- lifetime of ',len(DF_CELL)*0.5,' mins')\n",
    "    xpos_all_times         = DF_CELL.X.values.astype(int)\n",
    "    ypos_all_times         = DF_CELL.Y.values.astype(int)\n",
    "    zpos_all_times         = DF_CELL.zmn.values.astype(int)\n",
    "    cell_lat_all_times     = DF_CELL.lat.values\n",
    "    cell_lon_all_times     = DF_CELL.lon.values\n",
    "    times_tracked          = DF_CELL.timestr.values\n",
    "    thresholds_all_times   = DF_CELL.threshold_value.values\n",
    "    print('x-positions: ',xpos_all_times)\n",
    "    print('y-positions: ',ypos_all_times)\n",
    "    print('z-positions: ',zpos_all_times)\n",
    "    print('threshold for this cell: ',thresholds_all_times)\n",
    "    print('times for this cell: ',times_tracked)\n",
    "    \n",
    "    for counter,(tim,xpos,ypos,zpos,cell_lat,cell_lon,tobac_threshold) in enumerate(zip(times_tracked,xpos_all_times,ypos_all_times,zpos_all_times,cell_lat_all_times,cell_lon_all_times,thresholds_all_times)):\n",
    "        print('\\n\\n-----------------------')\n",
    "        print('timestep '+str(counter)+': '+tim)\n",
    "        tim_pd = pd.to_datetime(tim)\n",
    "        #rams_fil=glob.glob('/nobackup/pmarines/DATA_FM/'+DOMAIN+'/LES_data/'+'a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5')[0] # Pleiades\n",
    "        rams_fil=glob.glob('/monsoon/LES_MODEL_DATA/'+DOMAIN+'/G3/out_30s/'+'a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5')[0] # CSU machine\n",
    "        print('    RAMS date file: ',rams_fil)\n",
    "        da = xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "        #da = read_file_h5py(h5files1[-1])\n",
    "        ############################### WP SNAPSHOTS ################################\n",
    "        rams_lats=da['GLAT'][ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values\n",
    "        rams_lons=da['GLON'][ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values\n",
    "        \n",
    "        RAMS_closest_level = np.argmin(np.abs(ZM-zpos))\n",
    "        print('    RAMS closest vertical level to the thermal centroid is ',RAMS_closest_level)\n",
    "        vertical_vel = da['WP'][RAMS_closest_level,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values\n",
    "        print('    dimensions of plotted field is ',np.shape(vertical_vel))  \n",
    "        \n",
    "        # 2D segmentation of the updraft\n",
    "        IMAGE       = vertical_vel*create_circular_mask(np.shape(vertical_vel)[0],np.shape(vertical_vel)[1], 'in', center=None, radius=60)\n",
    "        bw          = closing(IMAGE > FIXED_THREHOLD, square(3)) # apply fixed threshold\n",
    "        cleared     = clear_border(bw)             # remove artifacts connected to image border\n",
    "        label_image = label(cleared)           # label image regions\n",
    "        regions     = [rr for rr in regionprops(label_image)]\n",
    "        #print('regions: ',regions)\n",
    "        #print('length of regions list ',len(regions))\n",
    "        if len(regions)==0:\n",
    "            print('regionprops could not find any updraft!!!\\n\\n\\n')\n",
    "            continue\n",
    "            \n",
    "        region_choice_criteria='closest'\n",
    "        # choose the region whose centroid is closest to the centroid of tobac cell in question\n",
    "        if region_choice_criteria=='closest':\n",
    "            centroids = [r.centroid for r in regions] #  y0, x0 = props.centroid; note the order 'y' and 'x'\n",
    "            areas     = [r.area for r in regions]\n",
    "            dist      = []  # calculate distances of the centroids of ud from the given point\n",
    "            for centt in centroids:\n",
    "                centt = np.array((centt))  # convert tuple to np array\n",
    "                dist.append(np.linalg.norm(centt - np.array([np.shape(vertical_vel)[0]//2,np.shape(vertical_vel)[1]//2])))\n",
    "            \n",
    "            ind_closest_ud = np.argmin(np.array(dist))\n",
    "            \n",
    "            if areas[ind_closest_ud] <0.5:\n",
    "                print('area of the selected updraft is too small... moving on to the next timestep\\n*\\n*\\n*\\n')\n",
    "                continue\n",
    "            \n",
    "            print('distances of all the detected updrafts from the tobac thermal centroid are: ', dist)\n",
    "            \n",
    "            cell_area          = areas[ind_closest_ud]\n",
    "            chosen_region      = regions[ind_closest_ud]\n",
    "            chosen_centroid    = centroids[ind_closest_ud]\n",
    "            chosen_centroid    = list(chosen_centroid) \n",
    "            chosen_centroid[0] = int(chosen_centroid[0])   # convert to integer # Y\n",
    "            chosen_centroid[1] = int(chosen_centroid[1])                        # X\n",
    "            print('    index of chosen region is ',     ind_closest_ud)\n",
    "            print('    centroid of chosen region is ', chosen_centroid)\n",
    "            \n",
    "        elif region_choice_criteria=='largest':\n",
    "            areas = [r.area for r in regions]\n",
    "            centroids = [r.centroid for r in regions]\n",
    "            max_area=max(np.array(areas))\n",
    "            max_area_ind= areas.index(max(areas))\n",
    "            chosen_region=regions[max_area_ind]\n",
    "            chosen_centroid = centroids[max_area_ind]\n",
    "            chosen_centroid = list(chosen_centroid) \n",
    "            chosen_centroid[0] = int(chosen_centroid[0])   # convert to integer\n",
    "            chosen_centroid[1] = int(chosen_centroid[1])\n",
    "            print('    index of chosen region is ', max_area_ind)\n",
    "            print('    centroid of chosen region is ', chosen_centroid)\n",
    "\n",
    "        else:\n",
    "            print('    closest or max are the only options for choosing areas')\n",
    "            \n",
    "        chosen_area_label_coords_array = chosen_region.coords\n",
    "\n",
    "        rows = [uu[0] for uu in chosen_area_label_coords_array]\n",
    "        cols = [uu[1] for uu in chosen_area_label_coords_array]\n",
    "\n",
    "        segmented_arr = np.zeros_like(label_image)  # *np.nan\n",
    "        segmented_arr[rows, cols] = 1111111.0\n",
    "\n",
    "        updraft_radius = chosen_region.axis_minor_length*0.75 + chosen_region.axis_major_length*0.25\n",
    "        print('    found an updraft with radius = ',updraft_radius,' gridpoints or ',updraft_radius*dxy/1000.,' km')\n",
    "        print('    using it to create updraft mask')\n",
    "        \n",
    "        mask_center_string = 'detected_updraft_center' # 'tobac_cell_centroid'\n",
    "        \n",
    "        if mask_center_string   == 'detected_updraft_center':\n",
    "            mask_center          = [chosen_centroid[1],chosen_centroid[0]] # has to X, Y\n",
    "            circle_center_latlon = [rams_lats[chosen_centroid[0],chosen_centroid[1]],rams_lons[chosen_centroid[0],chosen_centroid[1]]]\n",
    "            print('    lat-lon of updated cell position is : ',circle_center_latlon)\n",
    "        elif mask_center_string == 'tobac_cell_centroid':\n",
    "            mask_center          = [int(np.shape(vertical_vel)[1]/2), int(np.shape(vertical_vel)[0]/2)]\n",
    "            circle_center_latlon = [cell_lat,cell_lon]\n",
    "\n",
    "        # mask out the main updraft with a circle surrounding it\n",
    "        updraft_mask1_2D = create_circular_mask(np.shape(vertical_vel)[0],np.shape(vertical_vel)[1], 'out', center=mask_center, radius=int(updraft_radius))\n",
    "        vertical_vel_3D  = da['WP'][:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values\n",
    "        \n",
    "        # mask out all the updrafts in 3d \n",
    "        if MASK_OTHER_UPDRAFTS:\n",
    "            all_uds_mask_3D  = np.where(vertical_vel_3D<=2.0,1.0,np.nan)\n",
    "            masking_filename_label='other_uds_masked'\n",
    "        else:\n",
    "            all_uds_mask_3D  = np.ones_like(vertical_vel_3D)\n",
    "            masking_filename_label='other_uds_not_masked'\n",
    "            \n",
    "        #env radius dependent on updraft radius\n",
    "        env_radius_array = np.arange(10,210,10) # in units of pixels\n",
    "        \n",
    "        for env_radius in env_radius_array:\n",
    "            print('    \\n---- working on radius ---- : ',env_radius ,'grid points or '+str(int(env_radius*DXY))+' m')\n",
    "            # create mask for this environmental width\n",
    "            env_mask_2D = create_circular_mask(np.shape(vertical_vel)[0],np.shape(vertical_vel)[1], 'in', center=mask_center, radius=env_radius+updraft_radius)#*\\\n",
    "            total_3D_mask=updraft_mask1_2D[np.newaxis,:,:]*env_mask_2D[np.newaxis,:,:]*all_uds_mask_3D\n",
    "            print('    shape  of 3d mask: ',np.shape(total_3D_mask))\n",
    "            ######### PLOT WP SNAPSHOT and ENV ANNULUS FOR THIS CELL AT THIS TIME #########\n",
    "            plot_snapshots=True\n",
    "            current_cmap = plt.get_cmap('bwr').copy()\n",
    "            if env_radius == 70:\n",
    "                if plot_snapshots:\n",
    "                    coords = 'latlon'\n",
    "                    if coords=='cartesian':\n",
    "                        fig = plt.figure(figsize=(8,8))\n",
    "                        ax = plt.gca()\n",
    "                        ax.axis('equal')\n",
    "                        C111 = ax.contourf(vertical_vel,levels=np.arange(-20,21,1),cmap=current_cmap,extend='both')#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "                        plt.colorbar(C111,shrink=0.7, pad=0.02,fraction=0.11)\n",
    "                        ax.set_title('Plan view of vertical velocity at height '+str(zpos)+' m AGL; Cell#'+str(CELL_NO)+'\\n'+get_time_from_RAMS_file(rams_fil)[0])\n",
    "\n",
    "                        # for zoomed in \n",
    "                        tobac_features_scatter = ax.scatter(ypos,xpos,label='cell#'+str(CELL_NO),marker='.',s=85.5,c='k')\n",
    "\n",
    "                    elif coords=='latlon':\n",
    "                        fig = plt.figure(figsize=(8,8))\n",
    "                        ax = fig.add_subplot(1, 1, 1, projection=crs.PlateCarree(),facecolor='lightgray')\n",
    "                        # plot vertical velocity of the environment\n",
    "                        C111 = ax.contourf(rams_lons ,rams_lats, vertical_vel*total_3D_mask[RAMS_closest_level,:,:],levels=np.arange(-20,21,1),cmap=current_cmap,extend='both',transform=crs.PlateCarree())#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "                        # plot the segmented main updrafts\n",
    "                        C7 = ax.contour(rams_lons ,rams_lats, segmented_arr, levels=[1111111.0], colors='darkblue', linewidths=1.1, linestyles='--')\n",
    "                        # plot the centroid the segmented updraft\n",
    "                        ax.scatter(rams_lons[chosen_centroid[0],chosen_centroid[1]],rams_lats[chosen_centroid[0],chosen_centroid[1]],marker='^',s=55.5,c='fuchsia',)\n",
    "\n",
    "                        plt.colorbar(C111,shrink=0.7, pad=0.02,fraction=0.11)\n",
    "                        ax.set_title('Plan view of vertical velocity at height '+str(zpos)+' m AGL; Cell#'+str(CELL_NO)+'\\n'+get_time_from_RAMS_file(rams_fil)[0])\n",
    "\n",
    "                        # plot the cell point\n",
    "                        tobac_features_scatter = ax.scatter(cell_lon,cell_lat,label='cell#'+str(CELL_NO),marker='.',s=55.5,c='k',transform=crs.PlateCarree())\n",
    "                        gd = Geodesic()\n",
    "                        env_circle = gd.circle(lon=circle_center_latlon[1], lat=circle_center_latlon[0], radius=(env_radius+updraft_radius)*dxy)\n",
    "                        updraft_circle = gd.circle(lon=circle_center_latlon[1], lat=circle_center_latlon[0], radius=updraft_radius*dxy)\n",
    "                        ax.add_geometries([sgeom.Polygon(env_circle)], crs=crs.PlateCarree(), edgecolor='green', facecolor=\"none\")\n",
    "                        ax.add_geometries([sgeom.Polygon(updraft_circle)], crs=crs.PlateCarree(), edgecolor='maroon', facecolor=\"none\")\n",
    "\n",
    "                        gl = ax.gridlines()\n",
    "                        ax.coastlines(resolution='50m')\n",
    "                        gl.xlines = True\n",
    "                        gl.ylines = True\n",
    "                        LATLON_LABELS=True\n",
    "                        if LATLON_LABELS:\n",
    "                            #print('LATLON labels are on')\n",
    "                            gl.xlabels_top = True\n",
    "                            gl.ylabels_right = False\n",
    "                            gl.ylabels_left = True\n",
    "                            gl.ylabels_bottom = True\n",
    "                        else:\n",
    "                            gl.xlabels_top = False\n",
    "                            gl.ylabels_right = False\n",
    "                            gl.ylabels_left = False\n",
    "                            gl.ylabels_bottom = True\n",
    "\n",
    "                        gl.xlabel_style = {'size': 13, 'color': 'gray'}\n",
    "                        gl.ylabel_style = {'size': 13, 'color': 'gray'}\n",
    "\n",
    "                        import matplotlib.transforms as transforms\n",
    "                        trans = transforms.blended_transform_factory(ax.transAxes,ax.transData)\n",
    "                        props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "                        ax.text(0.52, 0.94, 'cell radius = '+str(np.round(updraft_radius*dxy/1000.,1))+\\\n",
    "                                ' km'+'\\n'+'tobac cell detection threshold ='+str(int(tobac_threshold))+' m/s\\n'+\\\n",
    "                                'w threshold for sounding ='+str(int(FIXED_THREHOLD))+' m/s\\n'+\\\n",
    "                                'environment width = '+str(int(env_radius*DXY/1000.0))+' km',\n",
    "                                fontsize=9,verticalalignment='top',\\\n",
    "                                bbox=props,transform=ax.transAxes)\n",
    "\n",
    "                    else:\n",
    "                        print('coords option for w snapshots has to be <cartesian> or <latlon>')\n",
    "\n",
    "                #snapshot_png_saving_directory = '/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/' #Pleaides\n",
    "                #snapshot_png_saving_directory = '/Users/isingh/SVH/SVH_paper1/scratch/'  # Personal computer\n",
    "                snapshot_png_saving_directory  = '/home/isingh/code/scratch/environmental_assessment/' # CSU machine\n",
    "                \n",
    "                wp_snapshot_with_environment_annulus_png=snapshot_png_saving_directory + 'WP_snapshot_cellcenter_'+mask_center_string+'_envwidth'+str(int(env_radius*DXY))+'m_cell'+str(CELL_NO)+'_xpos'+str(xpos)+'_ypos'+str(ypos)+'_zpos'+str(zpos)+'_'+get_time_from_RAMS_file(rams_fil)[1]+'_'+DOMAIN+'_comb_track_filt_01_02_50_02_sr5017_setpos_'+masking_filename_label+'.png'\n",
    "                print('    saving plan view wp snapshot with name: \\n',wp_snapshot_with_environment_annulus_png)\n",
    "                plt.savefig(wp_snapshot_with_environment_annulus_png,dpi=75)\n",
    "                plt.close()\n",
    "            ########### ################## GET SOUNDING DATA ############################# \n",
    "            save_soundings = True\n",
    "            \n",
    "            if save_soundings:\n",
    "                #sounding_saving_directory = '/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/' # Pleaides\n",
    "                #sounding_saving_directory = '/Users/isingh/SVH/SVH_paper1/scratch/' # personal computer\n",
    "                sounding_saving_directory =  '/home/isingh/code/scratch/environmental_assessment/' # CSU machine\n",
    "                print('    saving soundings...\\n')\n",
    "\n",
    "                rams_exner       = da['PI']   [:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values/1004.0*total_3D_mask\n",
    "                rams_theta       = da['THETA'][:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values*total_3D_mask# Kelvin\n",
    "                rams_temp_K      = rams_exner*rams_theta*units('K') # Kelvin\n",
    "                rams_temp_degC   = rams_temp_K.to('degC')\n",
    "                rams_pres_Pa     = (p00*(rams_exner)**(Cp/Rd))*units('Pa')\n",
    "                rams_pres_hPa    = rams_pres_Pa.to('hPa')\n",
    "                rams_rv          = da['RV'][:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values*total_3D_mask*units('kg/kg') # kg/kg\n",
    "                rams_sphum       = mpcalc.specific_humidity_from_mixing_ratio(rams_rv)\n",
    "                rams_dewpt       = mpcalc.dewpoint_from_specific_humidity(rams_pres_Pa,rams_temp_degC,rams_sphum)\n",
    "                rams_u           = da['UP'][:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values*total_3D_mask*units('m/s')\n",
    "                rams_v           = da['VP'][:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values*total_3D_mask*units('m/s')\n",
    "                rams_ter         = np.nanmean(da['TOPT'][ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values*env_mask_2D,axis=(0,1))\n",
    "                rams_hgt_msl     = (rams_ter + ZT)*units('meter')\n",
    "\n",
    "                #print('shape of rams_hgt_msl: ', np.shape(rams_hgt_msl))\n",
    "\n",
    "                output_median = False\n",
    "                output_mean   = True\n",
    "                output_std    = True\n",
    "                \n",
    "                if output_median: \n",
    "                    print('    saving median sounding')\n",
    "                    rams_temp_degC_median      = np.nanmedian(rams_temp_degC.magnitude,axis=(1,2))\n",
    "                    rams_dewpt_median          = np.nanmedian(rams_dewpt.magnitude,axis=(1,2))\n",
    "                    rams_u_median              = np.nanmedian(rams_u.magnitude,axis=(1,2))\n",
    "                    rams_v_median              = np.nanmedian(rams_v.magnitude,axis=(1,2))\n",
    "                    rams_pres_hPa_median       = np.nanmedian(rams_pres_hPa.magnitude,axis=(1,2))\n",
    "                    csv_sounding_file_name_mean= sounding_saving_directory +'area_avgd_annulus_envwidth_'+str(int(env_radius*DXY))+'m_2mps_'+'median_sounding_cell_'  +str(CELL_NO)+'_'+get_time_from_RAMS_file(rams_fil)[1]+'_'+DOMAIN+'_comb_track_filt_01_02_50_02_sr5017_setpos_'+masking_filename_label+'.csv'\n",
    "                    csv_median_sounding_df     = pd.DataFrame(data={'height_m':rams_hgt_msl,'pressure_hPa':rams_pres_hPa_median,'temp_degC':rams_temp_degC_median,'dewpt_degC':rams_dewpt_median,'uwnd_mps':rams_u_median,'vwnd_mps':rams_v_median})\n",
    "                    csv_median_sounding_df.to_csv(csv_sounding_file_name_median,index=False)\n",
    "                    print('    writing median sounding csv file for cell#',CELL_NO,' for time ',tim,' : <<<',csv_sounding_file_name_median,'>>>')\n",
    "                    \n",
    "                if output_mean: \n",
    "                    print('    saving mean sounding')\n",
    "                    rams_temp_degC_mean        = np.nanmean(rams_temp_degC.magnitude,axis=(1,2))\n",
    "                    rams_dewpt_mean            = np.nanmean(rams_dewpt.magnitude,axis=(1,2))\n",
    "                    rams_u_mean                = np.nanmean(rams_u.magnitude,axis=(1,2))\n",
    "                    rams_v_mean                = np.nanmean(rams_v.magnitude,axis=(1,2))\n",
    "                    rams_pres_hPa_mean         = np.nanmean(rams_pres_hPa.magnitude,axis=(1,2))\n",
    "                    csv_sounding_file_name_mean= sounding_saving_directory +'area_avgd_annulus_envwidth_'+str(int(env_radius*DXY))+'m_2mps_'+'mean_sounding_cell_'  +str(CELL_NO)+'_'+get_time_from_RAMS_file(rams_fil)[1]+'_'+DOMAIN+'_comb_track_filt_01_02_50_02_sr5017_setpos_'+masking_filename_label+'.csv'\n",
    "                    csv_mean_sounding_df = pd.DataFrame(data={'height_m':rams_hgt_msl,'pressure_hPa':rams_pres_hPa_mean,'temp_degC':rams_temp_degC_mean,'dewpt_degC':rams_dewpt_mean,'uwnd_mps':rams_u_mean,'vwnd_mps':rams_v_mean})\n",
    "                    csv_mean_sounding_df.to_csv(csv_sounding_file_name_mean,index=False)\n",
    "                    print('    writing mean sounding csv file for cell#',CELL_NO,' for time ',tim,' : <<<',csv_sounding_file_name_mean,'>>>')\n",
    "                    \n",
    "                if output_std:\n",
    "                    print('    saving std sounding')\n",
    "                    rams_temp_degC_std         = np.nanstd(rams_temp_degC.magnitude,axis=(1,2))\n",
    "                    rams_dewpt_std             = np.nanstd(rams_dewpt.magnitude,axis=(1,2))\n",
    "                    rams_u_std                 = np.nanstd(rams_u.magnitude,axis=(1,2))\n",
    "                    rams_v_std                 = np.nanstd(rams_v.magnitude,axis=(1,2))\n",
    "                    rams_pres_hPa_std          = np.nanstd(rams_pres_hPa.magnitude,axis=(1,2))\n",
    "                    csv_sounding_file_name_std = sounding_saving_directory +'area_avgd_annulus_envwidth_'+str(int(env_radius*DXY))+'m_2mps_'+'std_sounding_cell_'   +str(CELL_NO)+'_'+get_time_from_RAMS_file(rams_fil)[1]+'_'+DOMAIN+'_comb_track_filt_01_02_50_02_sr5017_setpos_'+masking_filename_label+'.csv'\n",
    "                    csv_std_sounding_df = pd.DataFrame(data={'height_m':rams_hgt_msl,'pressure_hPa':rams_pres_hPa_std,'temp_degC':rams_temp_degC_std,'dewpt_degC':rams_dewpt_std,'uwnd_mps':rams_u_std,'vwnd_mps':rams_v_std})\n",
    "                    csv_std_sounding_df.to_csv(csv_sounding_file_name_std,index=False)\n",
    "                    print('    writing std sounding csv file for cell#',CELL_NO,' for time ',tim,' : <<<',csv_sounding_file_name_std,'>>>')\n",
    "\n",
    "    print('====================== END OF CELL ======================\\n\\n')\n",
    "    \n",
    "\n",
    "############################################################################\n",
    "# Paths to model data and where to save data\n",
    "\n",
    "domain='DRC1.1-R'\n",
    "\n",
    "#path = '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/'     # Pleiades\n",
    "#path = '/Users/isingh/SVH/INCUS/sample_LES_data/'+domain+'/' # personal macbook\n",
    "path = '/monsoon/LES_MODEL_DATA/'+domain+'/G3/out_30s/'         # CSU machine\n",
    "\n",
    "#savepath = './'\n",
    "#tobac_data='/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'                                 # Pleiades\n",
    "#tobac_data='/Users/isingh/SVH/INCUS/jupyter_nbks/tobac_thermals/peter_tobac_output/'+domain+'/'# personal macbook\n",
    "tobac_data='/monsoon/pmarin/Tracking/Updrafts/'+domain+'/tobac_data/'                           # CSU machine\n",
    "\n",
    "#tobac_filename = 'comb_track_filt_01_02_05_10_20.p'\n",
    "tobac_filename = 'comb_track_filt_01_02_50_02_sr5017_setpos.p'\n",
    "tobac_filepath  = tobac_data+tobac_filename\n",
    "\n",
    "\n",
    "# Grab all the rams files \n",
    "h5filepath = path+'a-L*g3.h5'\n",
    "h5files1 = sorted(glob.glob(h5filepath))\n",
    "hefilepath = path+'a-L*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "#print(h5files1)\n",
    "start_time=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "end_time=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "print('Simulation name: ',domain)\n",
    "print('starting time of the simulation: ',start_time)\n",
    "print('ending time of the simulation: ',end_time)\n",
    "\n",
    "ds=xr.open_dataset(h5files1[-1],engine='h5netcdf', phony_dims='sort')\n",
    "wp = ds['WP']\n",
    "#ds = h5py.File(h5files1[-1], 'r')\n",
    "#wp = read_var_h5py(h5files1[-1],'WP')\n",
    "#ds#.TOPT.values\n",
    "\n",
    "domain_z_dim,domain_y_dim,domain_x_dim=np.shape(wp)\n",
    "print('domain_z_dim: ',domain_z_dim)\n",
    "print('domain_y_dim: ',domain_y_dim)\n",
    "print('domain_x_dim: ',domain_x_dim)\n",
    "\n",
    "zm, zt, nx, ny, dxy, npa = read_head(hefiles1[0],h5files1[0])\n",
    "\n",
    "#******************\n",
    "plotting_range = 250 # 25 km # need to know now for filtering cells close to edges\n",
    "#******************\n",
    "\n",
    "##### read in tobac data #####\n",
    "print('reading ',tobac_filepath)\n",
    "tdata = pd.read_pickle(tobac_filepath)\n",
    "\n",
    "def filter_cells(g):\n",
    "    return ((g.zmn.max() >= 2000.0) & (g.zmn.min() <= 15000.) & (g.X.max() <= domain_x_dim-plotting_range-1) & (g.X.min() >= plotting_range+1) &\n",
    "            (g.Y.max() <= domain_y_dim-plotting_range-1) & (g.Y.min() >= plotting_range+1) & (g.threshold_value.count() >= 5)  \\\n",
    "             & (pd.to_datetime(g.timestr).min() > pd.to_datetime(start_time)) & (pd.to_datetime(g.timestr).max() <  pd.to_datetime(end_time)))\n",
    "\n",
    "tdata_temp=tdata.groupby('cell').filter(filter_cells)\n",
    "#print(tdata_temp)\n",
    "all_cells = tdata_temp.cell.unique().tolist()\n",
    "\n",
    "print('number of unique cells identified: ',len(all_cells))\n",
    "#print('these cells are: ',all_cells)\n",
    "############################################################################\n",
    "\n",
    "\n",
    "# Running in the notebook\n",
    "# cn = random.choice(all_cells)\n",
    "# print('randomly chosen cell#: ',cn)\n",
    "# tdata_subset=tdata_temp[tdata_temp['cell']==cn]\n",
    "# #print(tdata_subset)\n",
    "# create_environmental_soundings_parallel(domain,dxy,tdata_subset,cn,zm,zt,plotting_range,2.0,True)\n",
    "\n",
    "#Running on the terminal in parallel\n",
    "#selected_cells = random.sample(all_cells,100)\n",
    "selected_cells=[ 8961, 9687, 11239, 11776, 11816, 11910, 12508, 12517, 14048, 14080, 14305, 14935, 14947, 15001,\\\n",
    "                15047, 15635, 15650, 15669, 15752, 15772, 16404, 17050, 17052, 17160, 17165, 17193, 17860, 17862,\\\n",
    "                18431, 20083, 22353, 22905, 23153, 24569, 25444, 25475, 28322, 28435, 28545, 29100, 29621, 29833,\\\n",
    "                29837, 29889, 30520, 31830, 34238, 34318, 34933, 34939, 35735, 36254, 37121, 40050, 40123, 41566,\\\n",
    "                43635, 44436, 45892, 46530, 46674, 47321, 47915, 48039, 48708, 49257, 52425, 54908, 56310, 56979,\\\n",
    "                59846, 62013, 62830, 66438, 67063, 69909, 70551, 70563, 70681, 73919, 74684, 74712, 75306, 76540,\\\n",
    "                76725, 78575, 79269, 79361, 79617, 80186, 81956, 81959, 82133, 83435, 83535, 84105, 86891, 88337,\\\n",
    "                90184, 28327]\n",
    "print('100 randomly selected cells are: ',selected_cells)\n",
    "argument = []\n",
    "#for cn in all_cells:\n",
    "for cn in selected_cells:\n",
    "    tdata_subset=tdata_temp[tdata_temp['cell']==cn]\n",
    "    argument = argument + [(domain,dxy,tdata_subset,cn,zm,zt,plotting_range,2.0,False)]\n",
    "\n",
    "print('length of argument is: ',len(argument))\n",
    "\n",
    "############################## FIRST OF ALL ################################\n",
    "cpu_count1 = 32 #int(cpu_count()/2)\n",
    "print('number of cpus: ',cpu_count1)\n",
    "############################################################################\n",
    "\n",
    "\n",
    "def main(FUNCTION, ARGUMENT):\n",
    "    pool = Pool(cpu_count1)\n",
    "    start_time = time.perf_counter()\n",
    "    results = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(create_environmental_soundings_parallel, argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f61cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get isolate cells only: separated from the nearest neighbour by atleast 500 m or something\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "test_df = tdata_temp[pd.to_datetime(tdata_temp.timestr) == pd.to_datetime('2016-12-30 11:59:30') ]\n",
    "#print(test_df)\n",
    "\n",
    "x_temp = np.array(test_df.X)#.T\n",
    "y_temp = np.array(test_df.Y)#.T\n",
    "X = np.column_stack((x_temp,y_temp))\n",
    "#print(X)\n",
    "print(np.shape(X))\n",
    "tree = KDTree(X)\n",
    "nearest_dist, nearest_ind = tree.query(X, k=2)  # k=2 nearest neighbors where k1 = identity\n",
    "nearest_dist_final = nearest_dist[:, 1]\n",
    "nearest_ind_final = nearest_ind[:,1]\n",
    "print(nearest_dist[:, 1])    # drop id; assumes sorted -> see args!\n",
    "print(nearest_ind[:, 1])     # \n",
    "\n",
    "plt.scatter(x_temp,y_temp)\n",
    "print(x_temp[nearest_ind[0]])\n",
    "for ii in range(len(test_df)):\n",
    "    plt.plot((x_temp[ii],x_temp[nearest_ind_final[ii]]),(y_temp[ii],y_temp[nearest_ind_final[ii]]))\n",
    "    plt.text((x_temp[ii]+x_temp[nearest_ind_final[ii]])/2,(y_temp[ii]+y_temp[nearest_ind_final[ii]])/2,\\\n",
    "             str(int(nearest_dist_final[ii])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef8674-ed71-49c1-8a13-f314941fb297",
   "metadata": {},
   "source": [
    "## Plot sample sounding from one of the csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51180ac2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "plot_one_file = False\n",
    "\n",
    "if plot_one_file:\n",
    "    #dff = pd.read_csv('/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/area_avgd_annulus_envradius_6000m_2mps_median_sounding_cell25168_20140331190430_BRA1.1-R.csv')\n",
    "    dff = pd.read_csv('/home/isingh/code/scratch/environmental_assessment/area_avgd_annulus_envwidth_4000m_2mps_final_mean_sounding_cell42217_20161230111800_DRC1.1-R.csv')\n",
    "    print(dff)\n",
    "    # plot_skewT(dff.temp_degC.values*units('degC'),dff.dewpt_degC.values*units('degC'),dff.uwnd_mps.values*units('m/s'),dff.vwnd_mps.values*units('m/s'),\\\n",
    "    #               dff.height_m.values*units('meters'),dff.pressure_hPa.values*units('hPa'),'Area- and lifetime-averaged environment sounding around cell#'+str(99999999)+' - '+str(len([9,9])*0.5)+' mins'+'\\n'+get_time_from_RAMS_file(rams_fil)[0],\\\n",
    "    #               'area_avgd_lifetime_sounding_cell'+str(9999999999)+'_'+get_time_from_RAMS_file(rams_fil)[1]+'_TEST_DELETE.png',BARB_INTERVAL=6,PLOT_PARCEL_PROFILE=None, PROF_TV=None, PRINT_INDICES=None, BUNKERS=None,\\\n",
    "    #               WRITE_SRH_OBS=None, L_OR_R=None, U_STORM_OBS=None, V_STORM_OBS=None)\n",
    "\n",
    "    plot_skewT(dff.temp_degC.values*units('degC'),dff.dewpt_degC.values*units('degC'),dff.uwnd_mps.values*units('m/s'),dff.vwnd_mps.values*units('m/s'),\\\n",
    "                  dff.height_m.values*units('meters'),dff.pressure_hPa.values*units('hPa'),'RANDOM CELL',\\\n",
    "                  'area_avgd_sounding_cell_TEST_DELETE.png',BARB_INTERVAL=6,PLOT_PARCEL_PROFILE=None, PROF_TV=None, PRINT_INDICES=None, BUNKERS=None,\\\n",
    "                  WRITE_SRH_OBS=None, L_OR_R=None, U_STORM_OBS=None, V_STORM_OBS=None)\n",
    "\n",
    "plot_multiple_soundings = True\n",
    "\n",
    "\n",
    "if plot_multiple_soundings:\n",
    "    #csv_files = sorted(glob.glob('/home/isingh/code/scratch/environmental_assessment/area_avgd_annulus_envwidth_4000m_2mps_final_mean_sounding_cell42217_20161230111800_DRC1.1-R.csv'))\n",
    "    csv_files = sorted(glob.glob('/home/isingh/code/scratch/environmental_assessment/area_avgd_annulus_envwidth_*m_2mps_final_mean_sounding_cell42217_20161230111800_DRC1.1-R.csv'))\n",
    "    \n",
    "    print('print # files ', len(csv_files)) \n",
    "    \n",
    "    for fil in csv_files:\n",
    "        #print(fil)\n",
    "        fname = os.path.basename(fil)\n",
    "        split_filename = fname.split(\"_\")\n",
    "        #print(split_filename)\n",
    "        env_radius = re.sub('[^0-9]','', split_filename[4])\n",
    "        #print(env_radius)\n",
    "        cellno     = split_filename[9][4:]\n",
    "        timestr    = split_filename[10]\n",
    "        \n",
    "        sounding_png_name = 'area_avgd_sounding_cell'+cellno+'_envradius'+str(env_radius)+'_time'+str(timestr)+'.png'\n",
    "        #print(sounding_png_name)\n",
    "        dff = pd.read_csv(fil)\n",
    "        plot_skewT(dff.temp_degC.values*units('degC'),dff.dewpt_degC.values*units('degC'),dff.uwnd_mps.values*units('m/s'),dff.vwnd_mps.values*units('m/s'),\\\n",
    "                  dff.height_m.values*units('meters'),dff.pressure_hPa.values*units('hPa'),'cell#'+str(cellno)+' Env width: '+str(env_radius),\\\n",
    "                  sounding_png_name,BARB_INTERVAL=6,PLOT_PARCEL_PROFILE=None, PROF_TV=None, PRINT_INDICES=None, BUNKERS=None,\\\n",
    "                  WRITE_SRH_OBS=None, L_OR_R=None, U_STORM_OBS=None, V_STORM_OBS=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e57f97",
   "metadata": {},
   "source": [
    "After soundings have been stored, we need to identify cells for which soundings have been written. After that, we need to create environmental indices and storm properties dataframes for those cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b93b2",
   "metadata": {},
   "source": [
    "## Examine the csv sounding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd49b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "domain='DRC1.1-R'\n",
    "\n",
    "#tobac_filepath = '/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'+'comb_track_filt_01_02_05_10_20.p'     # Pleiades\n",
    "tobac_filepath  = '/monsoon/pmarin/Tracking/Updrafts/'+domain+'/tobac_data/comb_track_filt_01_02_50_02_sr5017_setpos.p' # CSU machine\n",
    "#comb_track_filt_01_02_05_10_20.p\n",
    "##### read in tobac data #####\n",
    "print('reading ',tobac_filepath)\n",
    "tdata = pd.read_pickle(tobac_filepath)\n",
    "\n",
    "#tdata_subset=tdata[(tdata.cell.isin(cells_processed))]\n",
    "\n",
    "#csv_folder = '/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/'                         # Pleiades\n",
    "#csv_folder = '/Users/isingh/SVH/SVH_paper1/scratch/'                                                    # Personal computer\n",
    "csv_folder  = '/home/isingh/code/scratch/environmental_assessment/' # CSU machine\n",
    "\n",
    "csv_files=sorted(glob.glob(csv_folder+'area_avgd_annulus_envwidth_*m_2mps_mean_sounding_cell_*_*_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos_other_uds_not_masked.csv'))\n",
    "#print('# csv files',len(csv_files))\n",
    "cells_processed = []\n",
    "env_radii       = []\n",
    "timestr_list    = []\n",
    "\n",
    "for fil in csv_files:\n",
    "    fname = os.path.basename(fil)\n",
    "    split_filename = fname.split(\"_\")\n",
    "    #print(split_filename)\n",
    "    env_radius = re.sub('[^0-9]','', split_filename[4])\n",
    "    #print(env_radius)\n",
    "    cellno     = split_filename[9]\n",
    "    timestr    = split_filename[10]\n",
    "    cells_processed.append(int(cellno))\n",
    "    env_radii.append(int(env_radius))\n",
    "    timestr_list.append(int(timestr))\n",
    "  \n",
    "print('# files found',len(csv_files))\n",
    "print('unique cells processed: ',len(np.unique(np.array(cells_processed))),' cells namely: ',list(np.unique(np.array(cells_processed))))\n",
    "#print('env radii: ',np.unique(np.array(env_radii)))\n",
    "#print('times: ',np.unique(np.array(timestr_list)))\n",
    "print('-----------------\\n')\n",
    "\n",
    "good_cells=[]\n",
    "bad_cells=[]\n",
    "\n",
    "for cell in np.unique(np.array(cells_processed)):\n",
    "    print('cell is :',cell)\n",
    "    csv_files_cell=sorted(glob.glob(csv_folder+'area_avgd_annulus_envwidth_*m_2mps_mean_sounding_cell_'+str(cell)+'_*_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos_other_uds_not_masked.csv'))\n",
    "    print(cell,'->',int(len(csv_files_cell)/20),' | ',len(tdata[tdata['cell']==cell]),'\\n')\n",
    "    if int(len(csv_files_cell)/20)==len(tdata[tdata['cell']==cell]):\n",
    "        good_cells.append(cell)\n",
    "    else:\n",
    "        bad_cells.append(cell)\n",
    "    #print('---')\n",
    "print('there are ',len(good_cells),' complete cells: ',  good_cells) \n",
    "print('there are ',len(bad_cells),' INcomplete cells: ',  bad_cells) \n",
    "    #print('#csv files per envwidth/timsesteps for this cell are: ',len(csv_files_cell))\n",
    "    #print('#timesteps for this cell ',len(tdata[tdata['cell']==cell]))\n",
    "# remove problematic cells based on WP snapshots\n",
    "#cells_processed = [item for item in cells_processed if item not in (5665, 7202, 3340, 3836, 4896, 5987)]#WPO1.1-R\n",
    "#print(len(cells_processed))\n",
    "#cells_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81290f88",
   "metadata": {},
   "source": [
    "## Read all the soundings and write environmental indices dataframe/csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099fa3af-7270-4da2-917d-f9d2184c109d",
   "metadata": {},
   "source": [
    "### Serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240759b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call all the modules used in sounding functions\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sounding_functions import create_indices, plot_skewT\n",
    "# Metpy\n",
    "from metpy.units import units as munits\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.cbook import get_test_data\n",
    "from metpy.plots import Hodograph, SkewT\n",
    "from metpy.units import units\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "##################################################\n",
    "# SharpPy: https://github.com/sharppy/SHARPpy/archive/refs/heads/master.zip\n",
    "import sharppy.sharptab.profile as profile\n",
    "import sharppy.sharptab.interp as interp\n",
    "import sharppy.sharptab.winds as winds\n",
    "import sharppy.sharptab.utils as utils\n",
    "import sharppy.sharptab.params as params\n",
    "import sharppy.sharptab.thermo as thermo\n",
    "\n",
    "# Matplotlib helper modules/functions for skew-T\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.transforms as transforms\n",
    "import matplotlib.axis as maxis\n",
    "import matplotlib.spines as mspines\n",
    "import matplotlib.path as mpath\n",
    "from matplotlib.projections import register_projection\n",
    "##################################################\n",
    "\n",
    "domain='DRC1.1-R'\n",
    "#csv_folder='/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/'  # Pleaides\n",
    "#csv_folder='/Users/isingh/SVH/SVH_paper1/scratch/'                             # Personal computer\n",
    "csv_folder='/home/isingh/code/scratch/environmental_assessment/'                # CSU machine\n",
    "\n",
    "# good_cells = [9248, 11337, 11338, 12359, 12375, 12433, 13385, 18361, 18367, 22293, 22337, 23259, 23283, 24234, 25223, 25249, \\\n",
    "#               25266, 26292, 26368, 30365, 30392, 31336, 35269, 36244, 38196, 39147, 40177, 40186, 41205, 42212, 46105, 46112, \\\n",
    "#               47077, 47084, 47106, 48108, 50021, 50022, 50966, 53788, 53790, 53803, 53859, 57689, 58626, 58632, 58648, 59611,\\\n",
    "#               59615, 61601, 62583, 62668, 69790, 69794, 69843, 70804, 75524, 75609, 77534, 81338, 82296, 84244, 85178, 86169,\\\n",
    "#               87056, 88814, 88820, 91621, 97939, 97952, 100682, 101563, 104265, 105990, 106841, 107801, 110523, 111367, 112325]\n",
    "\n",
    "good_cells = [9441, 9668, 10438, 10973, 10983, 11115, 12536, 12648, 12688, 13468, 13535, 14251, 15766, 17154, 17845, \\\n",
    "              17871, 17947, 18604, 19410, 20068, 23216, 23709, 24327, 24707, 25433, 26103, 26162, 28334, 28809, 29822,\\\n",
    "              30526, 31191, 31926, 32722, 32822, 33509, 33516, 33583, 33944, 33997, 34668, 34908, 35624, 36463, 37574,\\\n",
    "              39193, 39293, 39773, 40055, 40092, 40148, 40773, 40799, 41375, 43670, 45573, 45820, 46535, 46602, 46618,\\\n",
    "              46636, 46669, 47251, 53193, 53273, 53281, 53957, 55559, 55612, 57085, 57713, 57789, 58483, 59985, 61327,\\\n",
    "              62644, 64841, 64927, 67835, 69885, 70440, 70569, 71923, 72566, 73964, 74546, 74653, 74703, 76029, 77210,\\\n",
    "              77376, 78016, 79256, 79401, 80771, 81395, 82776, 84694, 87474]\n",
    "\n",
    "csv_files=[]\n",
    "for cell in good_cells:\n",
    "    for fil in sorted(glob.glob(csv_folder+'area_avgd_annulus_envwidth_*_2mps_mean_sounding_nearby_uds_incl_cell_'+str(cell)+'_*_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos.csv')):\n",
    "        csv_files.append(fil)\n",
    "\n",
    "print('#csv files: ',len(csv_files))\n",
    "#csv_files=sorted(glob.glob(csv_folder+'area_avgd_annulus_envwidth_*_2mps_mean_sounding_nearby_uds_incl_cell*_*_'+domain+'.csv'))\n",
    "\n",
    "dfs_list = []\n",
    "cell_no=[]\n",
    "\n",
    "ii=0\n",
    "for fil in csv_files:\n",
    "    #print(ii)\n",
    "    print(fil)\n",
    "    \n",
    "    fname = os.path.basename(fil)\n",
    "    split_filename = fname.split(\"_\")\n",
    "    #print(split_filename)\n",
    "    env_radius = re.sub('[^0-9]','', split_filename[4])\n",
    "    cellno     = split_filename[9][4:]\n",
    "    timestr    = split_filename[10]\n",
    "    #print('cell#: ',cellno)\n",
    "    #print('env radius: ',env_radius)\n",
    "    #print('time: ',timestr)\n",
    "    df_inflow = pd.read_csv(fil)[1:]\n",
    "    #print(df_inflow)\n",
    "    #print('##########')\n",
    "    # Drop any rows with all NaN values for T, Td, winds\n",
    "    df_inflow = df_inflow.dropna(subset=('height_m', 'pressure_hPa', 'temp_degC', 'dewpt_degC', 'uwnd_mps', 'vwnd_mps'),\n",
    "                                 how='all').reset_index(drop=True)\n",
    "    p_inflow      = df_inflow['pressure_hPa'].values * units.hPa\n",
    "    h_inflow      = df_inflow['height_m'].values     * units.m\n",
    "    tc_inflow     = df_inflow['temp_degC'].values    * units.degC\n",
    "    td_inflow     = df_inflow['dewpt_degC'].values   * units.degC\n",
    "    u_inflow      = df_inflow['uwnd_mps'].values     * units('m/s')\n",
    "    u_inflow      = u_inflow.to('knots')\n",
    "    v_inflow      = df_inflow['vwnd_mps'].values     * units('m/s')\n",
    "    v_inflow      = v_inflow.to('knots')\n",
    "    inflow_params = create_indices(p_inflow, h_inflow,tc_inflow,td_inflow,u_inflow,v_inflow, WRITE_SRH_OBS=False, WRITE_ADVANCED_INDICES=False, L_OR_R=None, U_STORM_OBS=None, V_STORM_OBS=None)\n",
    "    inflow_params_df_temp = pd.DataFrame.from_dict(inflow_params).drop(labels=1)#.iloc[0]\n",
    "    inflow_params_df_temp['cell']=cellno\n",
    "    inflow_params_df_temp['env_radius']=env_radius\n",
    "    inflow_params_df_temp['time']=timestr\n",
    "    dfs_list.append(inflow_params_df_temp)\n",
    "    ii=ii+1\n",
    "    #print('\\n\\n')\n",
    "\n",
    "df_all = pd.concat(dfs_list, ignore_index=True)\n",
    "#df_all.index = df1.index\n",
    "#print(df_all)\n",
    "thermo_indices_data_csv_file = csv_folder+'thermodynamic_indices_init_' + domain + '_comb_track_filt_01_02_50_02_sr5017_setpos_part2.csv'\n",
    "df_all.to_csv(thermo_indices_data_csv_file)  # sounding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e642c-81ec-430d-b70e-e4effc4497e4",
   "metadata": {},
   "source": [
    "### Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4749c6-5529-4d9e-a3fc-d27890e9b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile calculate_thermo_indices_from_soundings.py\n",
    "\n",
    "# PARALLEL\n",
    "\n",
    "# Call all the modules used in sounding functions\n",
    "import pandas as pd\n",
    "import re, time, glob, os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from sounding_functions import create_indices, plot_skewT\n",
    "# Metpy\n",
    "from metpy.units import units as munits\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.cbook import get_test_data\n",
    "from metpy.plots import Hodograph, SkewT\n",
    "from metpy.units import units\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "##################################################\n",
    "# SharpPy: https://github.com/sharppy/SHARPpy/archive/refs/heads/master.zip\n",
    "import sharppy.sharptab.profile as profile\n",
    "import sharppy.sharptab.interp as interp\n",
    "import sharppy.sharptab.winds as winds\n",
    "import sharppy.sharptab.utils as utils\n",
    "import sharppy.sharptab.params as params\n",
    "import sharppy.sharptab.thermo as thermo\n",
    "\n",
    "# Matplotlib helper modules/functions for skew-T\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.transforms as transforms\n",
    "import matplotlib.axis as maxis\n",
    "import matplotlib.spines as mspines\n",
    "import matplotlib.path as mpath\n",
    "from matplotlib.projections import register_projection\n",
    "##################################################\n",
    "\n",
    "def create_thermodynamic_indices(CSVFILE,UNUSED):   \n",
    "        \n",
    "    fname = os.path.basename(CSVFILE)\n",
    "    split_filename = fname.split(\"_\")\n",
    "    env_radius = re.sub('[^0-9]','', split_filename[4])\n",
    "    cellno     = split_filename[12]\n",
    "    timestr    = split_filename[13]\n",
    "    #print('cell#: ',cellno)\n",
    "    #print('env radius: ',env_radius)\n",
    "    #print('time: ',timestr)\n",
    "    df_inflow = pd.read_csv(CSVFILE)[1:]\n",
    "    #print(df_inflow)\n",
    "    #print('##########')\n",
    "    # Drop any rows with all NaN values for T, Td, winds\n",
    "    df_inflow = df_inflow.dropna(subset=('height_m', 'pressure_hPa', 'temp_degC', 'dewpt_degC', 'uwnd_mps', 'vwnd_mps'),\n",
    "                                 how='all').reset_index(drop=True)\n",
    "    p_inflow      = df_inflow['pressure_hPa'].values * units.hPa\n",
    "    h_inflow      = df_inflow['height_m'].values     * units.m\n",
    "    tc_inflow     = df_inflow['temp_degC'].values    * units.degC\n",
    "    td_inflow     = df_inflow['dewpt_degC'].values   * units.degC\n",
    "    u_inflow      = df_inflow['uwnd_mps'].values     * units('m/s')\n",
    "    u_inflow      = u_inflow.to('knots')\n",
    "    v_inflow      = df_inflow['vwnd_mps'].values     * units('m/s')\n",
    "    v_inflow      = v_inflow.to('knots')\n",
    "    inflow_params = create_indices(p_inflow, h_inflow,tc_inflow,td_inflow,u_inflow,v_inflow, WRITE_SRH_OBS=False, WRITE_ADVANCED_INDICES=False, L_OR_R=None, U_STORM_OBS=None, V_STORM_OBS=None)\n",
    "    inflow_params_df_temp = pd.DataFrame.from_dict(inflow_params).drop(labels=1)#.iloc[0]\n",
    "    inflow_params_df_temp['cell']=cellno\n",
    "    inflow_params_df_temp['env_radius']=env_radius\n",
    "    inflow_params_df_temp['time']=timestr\n",
    "    #print(inflow_params_df_temp)\n",
    "    return inflow_params_df_temp\n",
    "\n",
    "###-------------------------------------------------###\n",
    "domain='DRC1.1-R'\n",
    "#csv_folder='/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/'  # Pleaides\n",
    "#csv_folder='/Users/isingh/SVH/SVH_paper1/scratch/'                             # Personal computer\n",
    "csv_folder='/home/isingh/code/scratch/environmental_assessment/'                # CSU machine\n",
    "\n",
    "good_cells = [9441, 9668, 10438, 10973, 10983, 11115, 12536, 12648, 12688, 13468, 13535, 14251, 15766, 17154, 17845, \\\n",
    "              17871, 17947, 18604, 19410, 20068, 23216, 23709, 24327, 24707, 25433, 26103, 26162, 28334, 28809, 29822,\\\n",
    "              30526, 31191, 31926, 32722, 32822, 33509, 33516, 33583, 33944, 33997, 34668, 34908, 35624, 36463, 37574,\\\n",
    "              39193, 39293, 39773, 40055, 40092, 40148, 40773, 40799, 41375, 43670, 45573, 45820, 46535, 46602, 46618,\\\n",
    "              46636, 46669, 47251, 53193, 53273, 53281, 53957, 55559, 55612, 57085, 57713, 57789, 58483, 59985, 61327,\\\n",
    "              62644, 64841, 64927, 67835, 69885, 70440, 70569, 71923, 72566, 73964, 74546, 74653, 74703, 76029, 77210,\\\n",
    "              77376, 78016, 79256, 79401, 80771, 81395, 82776, 84694, 87474]\n",
    "csv_files=[]\n",
    "for cell in good_cells:\n",
    "    for fil in sorted(glob.glob(csv_folder+'area_avgd_annulus_envwidth_*_2mps_mean_sounding_nearby_uds_incl_cell_'+str(cell)+'_*_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos.csv')):\n",
    "        csv_files.append(fil)\n",
    "\n",
    "print('total #csv files: ',len(csv_files))\n",
    "#csv_files=sorted(glob.glob(csv_folder+'area_avgd_annulus_envwidth_*_2mps_mean_sounding_nearby_uds_incl_cell*_*_'+domain+'.csv'))#\n",
    "\n",
    "#Running in the notebook\n",
    "# cf = random.choice(csv_files)\n",
    "# print('sample csv file #: ',cf)\n",
    "# sample_output_df = create_thermodynamic_indices(cf,None)\n",
    "# sample_output_df.to_csv('sample_file_thermodynamic_indices'+'.csv')\n",
    "# \n",
    "\n",
    "#Running on the terminal in parallel\n",
    "argument = []\n",
    "for fil in csv_files:\n",
    "    argument = argument + [(fil,None)]\n",
    "\n",
    "print('length of argument is: ',len(argument))\n",
    "\n",
    "\n",
    "# # ############################### FIRST OF ALL ################################\n",
    "cpu_count1 = 33 #cpu_count()\n",
    "print('number of cpus: ',cpu_count1)\n",
    "# # #############################################################################\n",
    "\n",
    "def main(DOMAIN, FUNCTION, ARGUMENT):\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(processes = (cpu_count1-1)) as pool:\n",
    "        data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "    df_all = pd.concat(data, ignore_index=True)\n",
    "    thermo_indices_data_csv_file = csv_folder+'thermodynamic_indices_' + DOMAIN + '_comb_track_filt_01_02_50_02_sr5017_setpos.csv'\n",
    "    print('saving thermodynamic indices to the file: ',thermo_indices_data_csv_file)\n",
    "    df_all.to_csv(thermo_indices_data_csv_file)  # sounding data\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main(domain, create_thermodynamic_indices, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c068b-45ac-4361-9143-a5f9ecebce29",
   "metadata": {},
   "source": [
    "### Parallel tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc26881-d79c-4ad4-9292-9fcd3e567ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile calculate_thermo_indices_from_soundings_tqdm.py\n",
    "\n",
    "# PARALLEL\n",
    "\n",
    "# Call all the modules used in sounding functions\n",
    "import pandas as pd\n",
    "import re, time, glob, os , tqdm \n",
    "#from tqdm.notebook import trange, tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import istarmap \n",
    "\n",
    "from sounding_functions import create_indices, plot_skewT\n",
    "# Metpy\n",
    "from metpy.units import units as munits\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.cbook import get_test_data\n",
    "from metpy.plots import Hodograph, SkewT\n",
    "from metpy.units import units\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "##################################################\n",
    "# SharpPy: https://github.com/sharppy/SHARPpy/archive/refs/heads/master.zip\n",
    "import sharppy.sharptab.profile as profile\n",
    "import sharppy.sharptab.interp as interp\n",
    "import sharppy.sharptab.winds as winds\n",
    "import sharppy.sharptab.utils as utils\n",
    "import sharppy.sharptab.params as params\n",
    "import sharppy.sharptab.thermo as thermo\n",
    "\n",
    "# Matplotlib helper modules/functions for skew-T\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.transforms as transforms\n",
    "import matplotlib.axis as maxis\n",
    "import matplotlib.spines as mspines\n",
    "import matplotlib.path as mpath\n",
    "from matplotlib.projections import register_projection\n",
    "##################################################\n",
    "\n",
    "def create_thermodynamic_indices(CSVFILE,UNUSED):   \n",
    "        \n",
    "    fname = os.path.basename(CSVFILE)\n",
    "    split_filename = fname.split(\"_\")\n",
    "    env_radius = re.sub('[^0-9]','', split_filename[4])\n",
    "    cellno     = split_filename[12]\n",
    "    timestr    = split_filename[13]\n",
    "    #print('cell#: ',cellno)\n",
    "    #print('env radius: ',env_radius)\n",
    "    #print('time: ',timestr)\n",
    "    df_inflow = pd.read_csv(CSVFILE)[1:]\n",
    "    #print(df_inflow)\n",
    "    #print('##########')\n",
    "    # Drop any rows with all NaN values for T, Td, winds\n",
    "    df_inflow = df_inflow.dropna(subset=('height_m', 'pressure_hPa', 'temp_degC', 'dewpt_degC', 'uwnd_mps', 'vwnd_mps'),\n",
    "                                 how='all').reset_index(drop=True)\n",
    "    p_inflow      = df_inflow['pressure_hPa'].values * units.hPa\n",
    "    h_inflow      = df_inflow['height_m'].values     * units.m\n",
    "    tc_inflow     = df_inflow['temp_degC'].values    * units.degC\n",
    "    td_inflow     = df_inflow['dewpt_degC'].values   * units.degC\n",
    "    u_inflow      = df_inflow['uwnd_mps'].values     * units('m/s')\n",
    "    u_inflow      = u_inflow.to('knots')\n",
    "    v_inflow      = df_inflow['vwnd_mps'].values     * units('m/s')\n",
    "    v_inflow      = v_inflow.to('knots')\n",
    "    inflow_params = create_indices(p_inflow, h_inflow,tc_inflow,td_inflow,u_inflow,v_inflow, WRITE_SRH_OBS=False, WRITE_ADVANCED_INDICES=False, L_OR_R=None, U_STORM_OBS=None, V_STORM_OBS=None)\n",
    "    inflow_params_df_temp = pd.DataFrame.from_dict(inflow_params).drop(labels=1)#.iloc[0]\n",
    "    inflow_params_df_temp['cell']=cellno\n",
    "    inflow_params_df_temp['env_radius']=env_radius\n",
    "    inflow_params_df_temp['time']=timestr\n",
    "    #print(inflow_params_df_temp)\n",
    "    return inflow_params_df_temp\n",
    "\n",
    "###-------------------------------------------------###\n",
    "domain='DRC1.1-R'\n",
    "#csv_folder='/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/'  # Pleaides\n",
    "#csv_folder='/Users/isingh/SVH/SVH_paper1/scratch/'                             # Personal computer\n",
    "csv_folder='/home/isingh/code/scratch/environmental_assessment/'                # CSU machine\n",
    "\n",
    "good_cells = [9441, 9668, 10438, 10973, 10983, 11115, 12536, 12648, 12688, 13468, 13535, 14251, 15766, 17154, 17845, \\\n",
    "              17871, 17947, 18604, 19410, 20068, 23216, 23709, 24327, 24707, 25433, 26103, 26162, 28334, 28809, 29822,\\\n",
    "              30526, 31191, 31926, 32722, 32822, 33509, 33516, 33583, 33944, 33997, 34668, 34908, 35624, 36463, 37574,\\\n",
    "              39193, 39293, 39773, 40055, 40092, 40148, 40773, 40799, 41375, 43670, 45573, 45820, 46535, 46602, 46618,\\\n",
    "              46636, 46669, 47251, 53193, 53273, 53281, 53957, 55559, 55612, 57085, 57713, 57789, 58483, 59985, 61327,\\\n",
    "              62644, 64841, 64927, 67835, 69885, 70440, 70569, 71923, 72566, 73964, 74546, 74653, 74703, 76029, 77210,\\\n",
    "              77376, 78016, 79256, 79401, 80771, 81395, 82776, 84694, 87474]\n",
    "csv_files=[]\n",
    "for cell in good_cells:\n",
    "    for fil in sorted(glob.glob(csv_folder+'area_avgd_annulus_envwidth_*_2mps_mean_sounding_nearby_uds_incl_cell_'+str(cell)+'_*_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos.csv')):\n",
    "        csv_files.append(fil)\n",
    "\n",
    "print('total #csv files: ',len(csv_files))\n",
    "#csv_files=sorted(glob.glob(csv_folder+'area_avgd_annulus_envwidth_*_2mps_mean_sounding_nearby_uds_incl_cell*_*_'+domain+'.csv'))#\n",
    "\n",
    "#Running in the notebook\n",
    "# cf = random.choice(csv_files)\n",
    "# print('sample csv file #: ',cf)\n",
    "# sample_output_df = create_thermodynamic_indices(cf,None)\n",
    "# sample_output_df.to_csv('sample_file_thermodynamic_indices'+'.csv')\n",
    "# \n",
    "\n",
    "#Running on the terminal in parallel\n",
    "argument = []\n",
    "for fil in csv_files:\n",
    "    argument = argument + [(fil,None)]\n",
    "\n",
    "print('length of argument is: ',len(argument))\n",
    "\n",
    "\n",
    "# # ############################### FIRST OF ALL ################################\n",
    "cpu_count1 = 33 #cpu_count()\n",
    "print('number of cpus: ',cpu_count1)\n",
    "# # #############################################################################\n",
    "\n",
    "data=[]\n",
    "\n",
    "def main(DOMAIN, FUNCTION, ARGUMENT):\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(processes = (cpu_count1-1)) as pool:\n",
    "        for result in tqdm.tqdm(pool.istarmap(create_thermodynamic_indices, argument),total=len(argument)):\n",
    "            data.append(result)\n",
    "        #data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "    df_all = pd.concat(data, ignore_index=True)\n",
    "    thermo_indices_data_csv_file = csv_folder+'thermodynamic_indices_' + DOMAIN + '_comb_track_filt_01_02_50_02_sr5017_setpos.csv'\n",
    "    print('saving thermodynamic indices to the file: ',thermo_indices_data_csv_file)\n",
    "    df_all.to_csv(thermo_indices_data_csv_file)  # sounding data\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main(domain, create_thermodynamic_indices, argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c204034-4c93-4ed7-9780-342cefd4e82b",
   "metadata": {},
   "source": [
    "### test the output thermo indices output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4261f-e802-46dd-ad4e-992e02e4dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.read_csv('/home/isingh/code/scratch/environmental_assessment/thermodynamic_indices_DRC1.1-R_comb_track_filt_01_02_50_02_sr5017_setpos.csv',index_col=0)\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed1c277-839b-412d-80af-76335bbbb794",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_out.columns:\n",
    "    print(col)\n",
    "    print(df_out[col])\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a2462",
   "metadata": {},
   "source": [
    "## Get WP, Z, and other cell properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62f81e-8fba-44fe-a7ff-907e09b61eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile get_storm_properties.py\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as colors\n",
    "import datetime\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import hdf5plugin\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import cartopy.crs as crs\n",
    "#matplotlib.rcParams['axes.facecolor'] = 'white'\n",
    "#plt.rcParams[\"font.family\"] = \"helvetica\"\n",
    "\n",
    "from cartopy.geodesic import Geodesic\n",
    "import shapely.geometry as sgeom\n",
    "\n",
    "from skimage.draw import ellipse\n",
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage.transform import rotate\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.segmentation import clear_border\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "#import istarmap\n",
    "#import tqdm\n",
    "\n",
    "def radar_colormap():\n",
    "    nws_reflectivity_colors = [\n",
    "    \"#646464\", # ND\n",
    "    \"#ccffff\", # -30\n",
    "    \"#cc99cc\", # -25\n",
    "    \"#996699\", # -20\n",
    "    \"#663366\", # -15\n",
    "    \"#cccc99\", # -10\n",
    "    \"#999966\", # -5\n",
    "    \"#646464\", # 0\n",
    "    \"#04e9e7\", # 5\n",
    "    \"#019ff4\", # 10\n",
    "    \"#0300f4\", # 15\n",
    "    \"#02fd02\", # 20\n",
    "    \"#01c501\", # 25\n",
    "    \"#008e00\", # 30\n",
    "    \"#fdf802\", # 35\n",
    "    \"#e5bc00\", # 40\n",
    "    \"#fd9500\", # 45\n",
    "    \"#fd0000\", # 50\n",
    "    \"#d40000\", # 55\n",
    "    \"#bc0000\", # 60\n",
    "    \"#f800fd\", # 65\n",
    "    \"#9854c6\", # 70\n",
    "    \"#fdfdfd\" # 75\n",
    "    ]\n",
    "\n",
    "    return mpl.colors.ListedColormap(nws_reflectivity_colors)\n",
    "\n",
    "\n",
    "cma1=plt.get_cmap('bwr')\n",
    "cma2=radar_colormap()\n",
    "cma3=plt.get_cmap('tab20c')\n",
    "import nclcmaps as ncm\n",
    "cma4=ncm.cmap(\"WhiteBlueGreenYellowRed\")\n",
    "cma5=plt.get_cmap('gray_r')\n",
    "cma6=plt.get_cmap('rainbow')\n",
    "cma7=plt.get_cmap('Oranges')\n",
    "cma8=plt.get_cmap('coolwarm')\n",
    "#cma9=cma4.reversed()\n",
    "cma10=plt.get_cmap('gist_yarg')\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def create_circular_mask(h, w, keep_in_or_out, center=None, radius=None):\n",
    "    if center is None: # use the middle of the image\n",
    "        center = (int(w/2), int(h/2))\n",
    "    if radius is None: # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w-center[0], h-center[1])\n",
    "\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((X - center[0])**2 + (Y-center[1])**2)\n",
    "\n",
    "    if keep_in_or_out=='in':\n",
    "        mask = np.where(dist_from_center <= radius, 1.0, np.nan)\n",
    "    elif keep_in_or_out=='out':\n",
    "        mask = np.where(dist_from_center >= radius, 1.0, np.nan)\n",
    "    else:\n",
    "        print('Please provide <<in>> or <<out>> values for the --keep_in_or_out-- argument')\n",
    "    return mask\n",
    "\n",
    "\n",
    "def read_head(headfile,h5file):\n",
    "    \n",
    "    # Function that reads header files from RAMS\n",
    "    \n",
    "    # Inputs:\n",
    "    #   headfile: header file including full path in str format\n",
    "    #   h5file: h5 datafile including full path in str format\n",
    "    \n",
    "    # Returns:\n",
    "    #   zmn: height levels for momentum values (i.e., grid box upper and lower levels)\n",
    "    #   ztn: height levels for thermodynaic values (i.e., grid box centers)\n",
    "    #   nx:: the number of x points for the domain associated with the h5file\n",
    "    #   ny: the number of y points for the domain associated with the h5file\n",
    "    #   npa: the number of surface patches\n",
    "    \n",
    "    \n",
    "    dom_num = h5file[h5file.index('.h5')-1] # Find index of .h5 to determine position showing which nest domain to use\n",
    "\n",
    "    with open(headfile) as f:\n",
    "        contents = f.readlines()\n",
    "        \n",
    "    idx_zmn = contents.index('__zmn0'+dom_num+'\\n')\n",
    "    nz_m = int(contents[idx_zmn+1])\n",
    "    zmn = np.zeros(nz_m)\n",
    "    for i in np.arange(0,nz_m):\n",
    "        zmn[i] =  float(contents[idx_zmn+2+i])\n",
    "    \n",
    "    idx_ztn = contents.index('__ztn0'+dom_num+'\\n')\n",
    "    nz_t = int(contents[idx_ztn+1])\n",
    "    ztn = np.zeros(nz_t)\n",
    "    for i in np.arange(0,nz_t):\n",
    "        ztn[i] =  float(contents[idx_ztn+2+i])\n",
    "    \n",
    "    ztop = np.max(ztn) # Model domain top (m)\n",
    "    \n",
    "    # Grad the size of the horizontal grid spacing\n",
    "    idx_dxy = contents.index('__deltaxn\\n')\n",
    "    dxy = float(contents[idx_dxy+1+int(dom_num)].strip())\n",
    "\n",
    "    idx_npatch = contents.index('__npatch\\n')\n",
    "    npa = int(contents[idx_npatch+2])\n",
    "    \n",
    "    idx_ny = contents.index('__nnyp\\n')\n",
    "    idx_nx = contents.index('__nnxp\\n')\n",
    "    ny = np.ones(int(contents[idx_ny+1]))\n",
    "    nx = np.ones(int(contents[idx_ny+1]))\n",
    "    for i in np.arange(0,len(ny)):\n",
    "        nx[i] = int(contents[idx_nx+2+i])\n",
    "        ny[i] = int(contents[idx_ny+2+i])\n",
    "\n",
    "    ny_out = ny[int(dom_num)-1]\n",
    "    nx_out = nx[int(dom_num)-1]\n",
    "\n",
    "    return zmn, ztn, nx_out, ny_out, dxy, npa \n",
    "\n",
    "\n",
    "def get_cell_W_area_rad_itc_iwp_rain(DOMAIN,DXY,UPDRAFT_DETECTION_THRESHOLD,TOBAC_CELL_DF,CELLNO,ZM,ZT,PLOTTING_RANGE,UPDRAFT_CONTAMINATION_CHECK,PLOT_QUANTITIES):\n",
    "    # make a copy of the dataframe to work on\n",
    "    # this dataframe is a subset of the larger dataframe and contains data for only one cell \n",
    "    g                       = TOBAC_CELL_DF.copy()\n",
    "    #print('the DATAFRAME: ',g,'\\n\\n')\n",
    "    print('working on cell#',CELLNO)\n",
    "    print('this cell has '+str(len(g))+' time steps -- lifetime of ',len(g)*0.5,' mins')\n",
    "    xpos_all_times          = g.X.values.astype(int)\n",
    "    ypos_all_times          = g.Y.values.astype(int)\n",
    "    zpos_all_times          = g.zmn.values.astype(int)\n",
    "    cell_lat_all_times      = g.lat.values\n",
    "    cell_lon_all_times      = g.lon.values\n",
    "    times_tracked           = g.timestr.values\n",
    "    thresholds_all_times    = g.threshold_value.values\n",
    "    #print('x-positions: ',xpos)\n",
    "    #print('y-positions: ',ypos)\n",
    "    #print('z-positions: ',zpos)\n",
    "    print('thresholds for this cell: ',thresholds_all_times)\n",
    "    print('times for this cells: '    ,times_tracked)\n",
    "\n",
    "\n",
    "    for counter, (tim,xpos,ypos,zpos,cell_lat,cell_lon,tobac_threshold) in enumerate(zip(times_tracked,xpos_all_times,ypos_all_times,zpos_all_times,cell_lat_all_times,cell_lon_all_times,thresholds_all_times)): # loop over timesteps of this cell \n",
    "        print('------------------------------------------------')\n",
    "        print('timestep '+str(counter)+': '+tim)\n",
    "        tim_pd   = pd.to_datetime(tim)\n",
    "        # Pleiades\n",
    "        #rams_fil=glob.glob('/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5')[0]\n",
    "        # CSU machine\n",
    "        rams_fil = glob.glob('/monsoon/LES_MODEL_DATA/'+DOMAIN+'/G3/out_30s/'+'a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5')[0]\n",
    "\n",
    "        print('RAMS file for this timestep: ',rams_fil)\n",
    "\n",
    "        # use xarray like here or h5py\n",
    "        da = xr.open_dataset(rams_fil,engine='h5netcdf', phony_dims='sort')\n",
    "        rams_lats=da['GLAT'][ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values\n",
    "        rams_lons=da['GLON'][ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values\n",
    "        RAMS_closest_level = np.argmin(np.abs(ZM-zpos))\n",
    "        print('RAMS closest vertical level to the thermal centroid is ',RAMS_closest_level)\n",
    "        # get the maximum value of w in the vicinity of the tobacl cell centroid\n",
    "        vertical_vel_centroid = np.nanmax(da.WP[RAMS_closest_level-3:RAMS_closest_level+3,ypos-3:ypos+3,xpos-3:xpos+3].values)\n",
    "        print('vertical velocity at the centroid is ',vertical_vel_centroid,' m/s')\n",
    "        # 2D vertical velocity at the level of the tobac cell centroid\n",
    "        vertical_vel = da['WP'][RAMS_closest_level,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE]#.values\n",
    "        \n",
    "        ########## 2D SEGMENTATION HERE #############\n",
    "        IMAGE   = vertical_vel*create_circular_mask(np.shape(vertical_vel)[0],np.shape(vertical_vel)[1], 'in', center=None, radius=PLOTTING_RANGE-1)\n",
    "        bw      = closing(IMAGE >= UPDRAFT_DETECTION_THRESHOLD, square(3)) # apply fixed threshold\n",
    "        cleared = clear_border(bw)                                         # remove artifacts connected to image border\n",
    "        label_image = label(cleared)                                       # label image regions\n",
    "        regions = [rr for rr in regionprops(label_image)]                  # use regionprops to get regions of w with the goven value\n",
    "        #print('regions: ',regions)\n",
    "        #print('length of regions list ',len(regions))\n",
    "        if len(regions)==0:\n",
    "            print('regionprops could not find any updraft!!!\\n\\n\\n')\n",
    "            g.loc[g['timestr'] == tim, 'area_m2'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'calcnumpixels']  = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'perimeter_scipy_default'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'areaconvex_scipy_default'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'eqdiameterareascipy_m'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'calcellipseradius_m'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'minoraxis_m'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'majoraxis_m'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'eccentricity'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'wcentroid_mps'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'IWP_mm'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'ITC_mm'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'rainmax_mm_per_hr'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'rain_mean'] = np.nan\n",
    "            g.loc[g['timestr'] == tim, 'updraftcontamination_fraction'] = np.nan\n",
    "            continue # to the next timestep \n",
    "        else:\n",
    "            centroids = [r.centroid for r in regions]\n",
    "            areas     = [r.area for r in regions]\n",
    "            dist      = []  # calculate distances of the centroids of ud from the tobac cell centroid (which is at the center of the \n",
    "                            # subdomain\n",
    "            for centt in centroids:\n",
    "                centt = np.array((centt))  # convert tuple to np array\n",
    "                dist.append(np.linalg.norm(centt - np.array([np.shape(vertical_vel)[0]//2,np.shape(vertical_vel)[1]//2])))\n",
    "\n",
    "            print('distances of all the detected updrafts from the tobac thermal centroid are: ', dist)\n",
    "\n",
    "            ind_closest_ud = np.argmin(np.array(dist))\n",
    "            print('index of chosen region is ', ind_closest_ud)\n",
    "            print('checking if the area of the appropriate region is > 0.5')\n",
    "\n",
    "            if areas[ind_closest_ud] <0.5:\n",
    "                g.loc[g['timestr'] == tim, 'area_m2'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'calcnumpixels']  = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'perimeter_scipy_default'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'areaconvex_scipy_default'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'eqdiameterareascipy_m'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'calcellipseradius_m'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'minoraxis_m'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'majoraxis_m'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'eccentricity'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'wcentroid_mps'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'IWP_mm'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'ITC_mm'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'rainmax_mm_per_hr'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'rain_mean'] = np.nan\n",
    "                g.loc[g['timestr'] == tim, 'updraftcontamination_fraction'] = np.nan\n",
    "                print('area of the selected updraft is too small... moving on to the next timestep\\n*\\n*\\n*\\n')\n",
    "                continue\n",
    "            else:\n",
    "                print('Yes, it does.')\n",
    "                ################## AREA AND RADIUS #######################\n",
    "                cell_area          = areas[ind_closest_ud]*DXY*DXY\n",
    "                chosen_region      = regions[ind_closest_ud]\n",
    "                chosen_centroid    = centroids[ind_closest_ud]\n",
    "                chosen_centroid    = list(chosen_centroid) \n",
    "                chosen_centroid[0] = int(chosen_centroid[0])   # convert to integer\n",
    "                chosen_centroid[1] = int(chosen_centroid[1])\n",
    "                print('centroid of chosen updrafts is : ',chosen_centroid)\n",
    "                updraft_radius = (chosen_region.axis_minor_length*0.75 + chosen_region.axis_major_length*0.25)*DXY\n",
    "                print('the chosen updraft has radius = ',updraft_radius,' m')\n",
    "                print('the chosen updraft has area = ',cell_area,' m^2')\n",
    "                ####################### CREATE STORM MASK ###############################\n",
    "                chosen_area_label_coords_array = chosen_region.coords\n",
    "                rows = [uu[0] for uu in chosen_area_label_coords_array]\n",
    "                cols = [uu[1] for uu in chosen_area_label_coords_array]\n",
    "                storm_mask_zeros = np.zeros_like(label_image)#*np.nan  # *np.nan\n",
    "                storm_mask_zeros[rows, cols] = 1.0\n",
    "                storm_mask_nans = np.zeros_like(label_image)*np.nan  # *np.nan\n",
    "                storm_mask_nans[rows, cols] = 1.0\n",
    "                ########################################################################\n",
    "                \n",
    "                if len(regions) > 1:\n",
    "                    if UPDRAFT_CONTAMINATION_CHECK:\n",
    "                        overlap_list = []\n",
    "                        storm_circular_mask_boolean= ~np.isnan(create_circular_mask(np.shape(vertical_vel)[0],np.shape(vertical_vel)[1], 'in', center=[chosen_centroid[1],chosen_centroid[0]], radius=updraft_radius/DXY))\n",
    "                        regions_without_main_storm = regions.copy()\n",
    "                        regions_without_main_storm.pop(ind_closest_ud)\n",
    "                        print('original regions: ',regions)\n",
    "                        print('regions_without_main_storm:', regions_without_main_storm)\n",
    "                        radii_other_uds = [(r.axis_minor_length*0.75 + r.axis_major_length*0.25) for r in regions_without_main_storm]\n",
    "                        centroids_other_uds = [r.centroid for r in regions_without_main_storm]\n",
    "                        print('\\n\\n !!!!updraft contamination check!!!!')\n",
    "                        print('centroids of all detection regions are: ',centroids_other_uds)\n",
    "                        print('radii of all detection regions are: ',radii_other_uds)\n",
    "                        overlap_fraction_list = []\n",
    "                        iii = 0\n",
    "                        for dd,ee in list(zip(centroids_other_uds,radii_other_uds)):\n",
    "                            print('region#',iii,' : ',dd,ee)\n",
    "                            print('centoid of storm is',chosen_centroid)\n",
    "                            print('centroid is ',dd)\n",
    "                            print('radius is ',ee)\n",
    "                            adjacent_storm_circular_mask_boolean = ~np.isnan(create_circular_mask(np.shape(vertical_vel)[0],np.shape(vertical_vel)[1], 'in', center=[dd[1],dd[0]], radius=ee))\n",
    "                            overlap = storm_circular_mask_boolean * adjacent_storm_circular_mask_boolean # Logical AND\n",
    "                            IOS = np.count_nonzero(overlap)/np.count_nonzero(storm_circular_mask_boolean) \n",
    "                            print('overlap for this region is: ',IOS)\n",
    "                            overlap_fraction_list.append(IOS)\n",
    "                            iii = iii + 1\n",
    "                            print('\\n-----')\n",
    "                        print('maximum overlap is ',max(overlap_fraction_list))\n",
    "                        print('minimum overlap is ',min(overlap_fraction_list))\n",
    "                        print('\\n\\n !!!!updraft contamination check OVER!!!!')\n",
    "                        max_ud_contamination_fraction = max(overlap_fraction_list)\n",
    "                else:\n",
    "                    max_ud_contamination_fraction = 0.0\n",
    "                ####################### IWP ###############################\n",
    "                # Constants for calculating total integrated condensate\n",
    "                cp  = 1004    # J/kg/K\n",
    "                rd  = 287     # J/kg/K\n",
    "                p00 = 100000  # Reference Pressure\n",
    "                condensate = da['RTP'][:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values- \\\n",
    "                             da['RV'] [:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values\n",
    "\n",
    "                frozen_condensate = da['RPP'][:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values+\\\n",
    "                                    da['RSP'][:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values+\\\n",
    "                                    da['RAP'][:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values+\\\n",
    "                                    da['RGP'][:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values+\\\n",
    "                                    da['RHP'][:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values\n",
    "\n",
    "                # Load variables needed to calculate density\n",
    "                th = da['THETA'][:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values\n",
    "                nx = np.shape(th)[2]\n",
    "                ny = np.shape(th)[1]\n",
    "                pi = da['PI']   [:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values\n",
    "                rv = da['RV']   [:,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE].values\n",
    "                # Convert RAMS native variables to temperature and pressure\n",
    "                pres = np.power((pi/cp),cp/rd)*p00\n",
    "                temp = th*(pi/cp)\n",
    "                del(th,pi)\n",
    "                # Calculate atmospheric density\n",
    "                dens = pres/(rd*temp*(1+0.61*rv))\n",
    "                del(pres,temp,rv)\n",
    "                # Difference in heights (dz)    \n",
    "                diff_zt_3D = np.tile(np.diff(zt),(int(ny),int(nx),1))\n",
    "                diff_zt_3D = np.moveaxis(diff_zt_3D,2,0)\n",
    "                # Calculate integrated condensate\n",
    "                itc               = np.nansum(condensate[1:,:,:]*dens[1:,:,:]*diff_zt_3D,axis=0) # integrated total condensate in kg\n",
    "                itc_mm            = itc/997.0*1000 # integrated total condensate in mm\n",
    "                itc_mm[itc_mm<=0] = 0.001\n",
    "                itc_max           = np.nanmax(storm_mask_zeros*itc_mm)\n",
    "                # Calculate IWP (ice water path)\n",
    "                iwp               = np.nansum(frozen_condensate[1:,:,:]*dens[1:,:,:]*diff_zt_3D,axis=0) \n",
    "                iwp_mm            = iwp/997.0*1000 # integrated total frozen condensate in mm\n",
    "                iwp_mm[iwp_mm<=0] = 0.001\n",
    "                iwp_max           = np.nanmax(storm_mask_zeros*iwp_mm)\n",
    "                ####################### MAX SURFACE PRECIP RATE ###############################\n",
    "                rain              = da['PCPRR'][ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE]*3600.0#.values\n",
    "                rain_max          = np.nanmax(storm_mask_zeros*rain)\n",
    "                rain_mean         = np.nanmean(storm_mask_nans*rain)\n",
    "                #####################################################################\n",
    "                # add all these variables calculated above to the dataframe \n",
    "                g.loc[g['timestr'] == tim, 'area_m2']                  = cell_area\n",
    "                g.loc[g['timestr'] == tim, 'calcnumpixels']            = np.count_nonzero(~np.isnan(storm_mask_nans))\n",
    "                #g.loc[tdata_subset['timestr'] == tim, 'numpixels_scipy_default'] = chosen_region.num_pixels\n",
    "                g.loc[g['timestr'] == tim, 'perimeter_scipy_default']  = chosen_region.perimeter\n",
    "                g.loc[g['timestr'] == tim, 'areaconvex_scipy_default'] = chosen_region.area_convex\n",
    "                g.loc[g['timestr'] == tim, 'eqdiameterareascipy_m']    = chosen_region.equivalent_diameter_area*DXY/2.0\n",
    "                g.loc[g['timestr'] == tim, 'calcellipseradius_m']      = updraft_radius\n",
    "                g.loc[g['timestr'] == tim, 'minoraxis_m']              = chosen_region.axis_minor_length*DXY\n",
    "                g.loc[g['timestr'] == tim, 'majoraxis_m']              = chosen_region.axis_major_length*DXY\n",
    "                g.loc[g['timestr'] == tim, 'eccentricity']             = chosen_region.eccentricity\n",
    "                g.loc[g['timestr'] == tim, 'wcentroid_mps']            = vertical_vel_centroid\n",
    "                g.loc[g['timestr'] == tim, 'IWP_mm']                   = iwp_max\n",
    "                g.loc[g['timestr'] == tim, 'ITC_mm']                   = itc_max\n",
    "                g.loc[g['timestr'] == tim, 'rainmax_mm_per_hr']        = rain_max\n",
    "                g.loc[g['timestr'] == tim, 'rain_mean']                = rain_mean\n",
    "                if UPDRAFT_CONTAMINATION_CHECK:\n",
    "                    g.loc[g['timestr'] == tim, 'updraftcontamination_fraction'] = max_ud_contamination_fraction\n",
    "                else:\n",
    "                    g.loc[g['timestr'] == tim, 'updraftcontamination_fraction'] = np.nan\n",
    "                    \n",
    "                ######################### PLOTTING #########################\n",
    "                if PLOT_QUANTITIES:\n",
    "                    print('plotting = True')\n",
    "\n",
    "                    current_cmap = plt.get_cmap('bwr').copy()\n",
    "                    mask_center_string = 'detected_updraft_center' # 'tobac_cell_centroid'\n",
    "\n",
    "                    if mask_center_string == 'detected_updraft_center':\n",
    "                        mask_center = chosen_centroid\n",
    "                        circle_center_latlon = [rams_lats[chosen_centroid[0],chosen_centroid[1]],rams_lons[chosen_centroid[0],chosen_centroid[1]]]\n",
    "                        print('lat-lon of updated cell position is : ',circle_center_latlon)\n",
    "                    elif mask_center_string == 'tobac_cell_centroid':\n",
    "                        mask_center = [int(np.shape(vertical_vel)[1]/2), int(np.shape(vertical_vel)[0]/2)]\n",
    "                        circle_center_latlon = [cell_lat,cell_lon]\n",
    "\n",
    "                    ############ WP #############\n",
    "                    fig  = plt.figure(figsize=(8,8))\n",
    "                    ax1  = fig.add_subplot(2, 2, 1, projection=crs.PlateCarree())\n",
    "                    w_plotting =  da['WP'][RAMS_closest_level,ypos-PLOTTING_RANGE:ypos+PLOTTING_RANGE,xpos-PLOTTING_RANGE:xpos+PLOTTING_RANGE]#.values\n",
    "                    C111 = ax1.contourf(rams_lons ,rams_lats, w_plotting,levels=np.arange(-20,21,1),cmap=current_cmap,extend='both',transform=crs.PlateCarree())#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "                    C7   = ax1.contour(rams_lons ,rams_lats, storm_mask_zeros,  colors='k', linewidths=0.7, linestyles='-')\n",
    "                    #levels=[0.9],\n",
    "                    ax1.scatter(rams_lons[chosen_centroid[0],chosen_centroid[1]],rams_lats[chosen_centroid[0],chosen_centroid[1]],marker='^',s=55.5,color='limegreen')\n",
    "                    tobac_features_scatter = ax1.scatter(cell_lon,cell_lat,label='cell#'+str(CELLNO),marker='.',s=55.5,c='k',transform=crs.PlateCarree())\n",
    "                    #plt.colorbar(C111,shrink=0.7, pad=0.02,fraction=0.11)\n",
    "                    ax1.set_title('w (m/s) at height '+str(zpos)+' m AGL; Cell#'+str(CELLNO)+'\\n'+get_time_from_RAMS_file(rams_fil)[0])\n",
    "                    #plot the cell point\n",
    "                    gd = Geodesic()\n",
    "                    print('plotting the updraft radius circle with radius: ',updraft_radius)\n",
    "                    updraft_circle = gd.circle(lon=circle_center_latlon[1], lat=circle_center_latlon[0], radius=updraft_radius)\n",
    "                    ax1.add_geometries([sgeom.Polygon(updraft_circle)], crs=crs.PlateCarree(), edgecolor='maroon', facecolor=\"none\")\n",
    "                    ########################################################################################\n",
    "                    plot_all_other_detected_regions = False\n",
    "                    if len(regions) > 1:\n",
    "                        if plot_all_other_detected_regions:\n",
    "                            for centt,radius,ovlap in list(zip(centroids_other_uds,radii_other_uds,overlap_fraction_list)):\n",
    "                                centt_int = list(centt) \n",
    "                                centt_int[0] = int(centt_int[0])   # convert to integer\n",
    "                                centt_int[1] = int(centt_int[1])\n",
    "                                ax1.scatter(rams_lons[centt_int[0],centt_int[1]],rams_lats[centt_int[0],centt_int[1]],marker='^',s=10.5,color='lightgreen')\n",
    "                                ud_center = [rams_lats[centt_int[0],centt_int[1]].values,rams_lons[centt_int[0],centt_int[1]].values]\n",
    "                                adjacent_updraft_circle = gd.circle(lon=ud_center[1], lat=ud_center[0], radius=radius*DXY)\n",
    "                                ax1.add_geometries([sgeom.Polygon(adjacent_updraft_circle)], crs=crs.PlateCarree(), edgecolor='indianred', facecolor=\"none\")\n",
    "                                ax1.text(ud_center[1],ud_center[0],str(np.round(ovlap,2)))\n",
    "                    ########################################################################################\n",
    "                    gl = ax1.gridlines()\n",
    "                    ax1.coastlines(resolution='50m')\n",
    "                    gl.xlines = True\n",
    "                    gl.ylines = True\n",
    "                    LATLON_LABELS=True\n",
    "                    gl.xlabels_top = True\n",
    "                    gl.ylabels_right = False\n",
    "                    gl.ylabels_left = True\n",
    "                    gl.ylabels_bottom = False\n",
    "                    gl.xlabel_style = {'size': 9, 'color': 'gray'}\n",
    "                    gl.ylabel_style = {'size': 9, 'color': 'gray'}\n",
    "                    fig.colorbar(C111, ax=ax1, shrink=0.7)#,orientation='horizontal')\n",
    "                    ############ ITC (from Marinescu) #############\n",
    "                     # contour and colobar ticks and levels     \n",
    "                    itc_lvls = np.arange(0.01,10.01,0.01) # Adjusted these levels, such that figure shows regions with at least 1 grid box with 0.1 g/kg of condensate\n",
    "                    itc_cbar_ticks = np.log10(np.array([1,5,10]))\n",
    "                    itc_cbar_ticklbls = np.array([1,5,10])\n",
    "                    # Make new colorbar to blue (no condensate) to white (condensate)\n",
    "                    from matplotlib.colors import LinearSegmentedColormap\n",
    "                    colorlist=[\"darkblue\", \"lightsteelblue\", \"white\"]\n",
    "                    newcmp = LinearSegmentedColormap.from_list('testCmap', colors=colorlist, N=256)\n",
    "                    # Scale size of figure based on dimensions of domain\n",
    "                    max_dim = np.max([nx,ny])\n",
    "                    ax2 = fig.add_subplot(2, 2, 2, projection=crs.PlateCarree())\n",
    "                    C111 = ax2.contourf(rams_lons ,rams_lats, np.log10(itc_mm),levels = np.log10(itc_lvls),cmap=newcmp,extend='both',transform=crs.PlateCarree())#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "                    C7   = ax2.contour (rams_lons ,rams_lats, storm_mask_zeros,  colors='k', linewidths=0.7, linestyles='-')\n",
    "                    ax2.scatter(rams_lons[chosen_centroid[0],chosen_centroid[1]],rams_lats[chosen_centroid[0],chosen_centroid[1]],marker='^',s=55.5,color='limegreen')\n",
    "                    tobac_features_scatter = ax2.scatter(cell_lon,cell_lat,label='cell#'+str(CELLNO),marker='.',s=55.5,c='k',transform=crs.PlateCarree())\n",
    "                    ax2.add_geometries([sgeom.Polygon(updraft_circle)], crs=crs.PlateCarree(), edgecolor='maroon', facecolor=\"none\")\n",
    "                    #plt.colorbar(C111,shrink=0.7, pad=0.02,fraction=0.11)\n",
    "                    ax2.set_title('ITC (mm) for Cell#'+str(CELLNO)+'\\n'+get_time_from_RAMS_file(rams_fil)[0])\n",
    "                    gl = ax2.gridlines()\n",
    "                    ax2.coastlines(resolution='50m')\n",
    "                    gl.xlines = True\n",
    "                    gl.ylines = True\n",
    "                    gl.xlabels_top = True\n",
    "                    gl.ylabels_right = False\n",
    "                    gl.ylabels_left = False\n",
    "                    gl.ylabels_bottom = False\n",
    "                    gl.xlabel_style = {'size': 9, 'color': 'gray'}\n",
    "                    gl.ylabel_style = {'size': 9, 'color': 'gray'}\n",
    "                    cbar = plt.colorbar(C111,ax=ax2,ticks=itc_cbar_ticks,shrink=0.7)#,orientation='horizontal')\n",
    "                    cbar.ax.set_yticklabels(itc_cbar_ticklbls)\n",
    "                    # plot the cell point\n",
    "#                         updraft_circle = gd.circle(lon=circle_center_latlon[1], lat=circle_center_latlon[0], radius=updraft_radius*DXY)\n",
    "#                         ax2.add_geometries([sgeom.Polygon(updraft_circle)], crs=crs.PlateCarree(), edgecolor='maroon', facecolor=\"none\")\n",
    "                    ############ IWP (from Marinescu) #############\n",
    "                    ax3   = fig.add_subplot(2, 2, 3, projection=crs.PlateCarree())\n",
    "                    C_iwp = ax3.contourf(rams_lons ,rams_lats, np.log10(iwp_mm),levels = np.log10(itc_lvls),cmap=newcmp,extend='both',transform=crs.PlateCarree())#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "                    C7    = ax3.contour(rams_lons ,rams_lats, storm_mask_zeros,  colors='k', linewidths=0.7, linestyles='-')\n",
    "                    ax3.scatter(rams_lons[chosen_centroid[0],chosen_centroid[1]],rams_lats[chosen_centroid[0],chosen_centroid[1]],marker='^',s=55.5,color='limegreen')\n",
    "                    tobac_features_scatter = ax3.scatter(cell_lon,cell_lat,label='cell#'+str(CELLNO),marker='.',s=55.5,c='k',transform=crs.PlateCarree())\n",
    "                    ax3.add_geometries([sgeom.Polygon(updraft_circle)], crs=crs.PlateCarree(), edgecolor='maroon', facecolor=\"none\")\n",
    "                    #plt.colorbar(C111,shrink=0.7, pad=0.02,fraction=0.11)\n",
    "                    ax3.set_title('IWP (mm) for Cell#'+str(CELLNO)+'\\n'+get_time_from_RAMS_file(rams_fil)[0])\n",
    "                    gl = ax3.gridlines()\n",
    "                    ax3.coastlines(resolution='50m')\n",
    "                    gl.xlines = True\n",
    "                    gl.ylines = True\n",
    "                    LATLON_LABELS=True\n",
    "                    gl.xlabels_top = False\n",
    "                    gl.ylabels_right = False\n",
    "                    gl.ylabels_left = True\n",
    "                    gl.ylabels_bottom = True\n",
    "                    gl.xlabel_style = {'size': 9, 'color': 'gray'}\n",
    "                    gl.ylabel_style = {'size': 9, 'color': 'gray'}\n",
    "                    cbar = plt.colorbar(C_iwp,ax=ax3,ticks=itc_cbar_ticks,shrink=0.7)#,orientation='horizontal')\n",
    "                    cbar.ax.set_yticklabels(itc_cbar_ticklbls)\n",
    "                    # plot the cell point\n",
    "#                       updraft_circle = gd.circle(lon=circle_center_latlon[1], lat=circle_center_latlon[0], radius=updraft_radius*DXY)\n",
    "#                       ax3.add_geometries([sgeom.Polygon(updraft_circle)], crs=crs.PlateCarree(), edgecolor='maroon', facecolor=\"none\")\n",
    "                    ############ RR# #############\n",
    "                    ax4 = fig.add_subplot(2, 2, 4, projection=crs.PlateCarree())\n",
    "                    C111 = ax4.contourf(rams_lons ,rams_lats, rain,extend='both',cmap=cma4,levels=np.arange(5,35.5,.5),transform=crs.PlateCarree())#,colors=PLOT_ANOTHER_VAR_CONT[7],linestyles=np.where(levels >= 0, \"-\", \"--\"),linewidths=PLOT_ANOTHER_VAR_CONT[8])\n",
    "                    C7   = ax4.contour(rams_lons  ,rams_lats, storm_mask_zeros,  colors='k', linewidths=0.7, linestyles='-')\n",
    "                    ax4.scatter(rams_lons[chosen_centroid[0],chosen_centroid[1]],rams_lats[chosen_centroid[0],chosen_centroid[1]],marker='^',s=55.5,color='limegreen')\n",
    "                    tobac_features_scatter = ax4.scatter(cell_lon,cell_lat,label='cell#'+str(CELLNO),marker='.',s=55.5,c='k',transform=crs.PlateCarree())\n",
    "                    ax4.add_geometries([sgeom.Polygon(updraft_circle)], crs=crs.PlateCarree(), edgecolor='maroon', facecolor=\"none\")\n",
    "                    #plt.colorbar(C111,shrink=0.7, pad=0.02,fraction=0.11)\n",
    "                    ax4.set_title('Rain rate (mm/h) for Cell#'+str(CELLNO)+'\\n'+get_time_from_RAMS_file(rams_fil)[0])\n",
    "                    gl = ax4.gridlines()\n",
    "                    ax4.coastlines(resolution='50m')\n",
    "                    gl.xlines = True\n",
    "                    gl.ylines = True\n",
    "                    LATLON_LABELS=True\n",
    "                    gl.xlabels_top = False\n",
    "                    gl.ylabels_right = False\n",
    "                    gl.ylabels_left = False\n",
    "                    gl.ylabels_bottom = True\n",
    "                    gl.xlabel_style = {'size': 9, 'color': 'gray'}\n",
    "                    gl.ylabel_style = {'size': 9, 'color': 'gray'}\n",
    "                    fig.colorbar(C111, ax=ax4, shrink=0.7)#,orientation='horizontal')\n",
    "                    # Adjust spacing b/w subplots\n",
    "                    plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9,top=0.9,wspace=0.1,hspace=0.0001)\n",
    "                    #png_save_folder ='/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/'  # Pleaides\n",
    "                    png_save_folder =  '/home/isingh/code/scratch/environmental_assessment/'              # CSU machine\n",
    "                    pngfile = 'four_panel_wp_itc_iwp_rain_'+DOMAIN+'_cell'+str(CELLNO)+'_'+get_time_from_RAMS_file(rams_fil)[1]+'_comb_track_filt_01_02_50_02_sr5017_setpos.png'\n",
    "                    print('saving image to file: ',png_save_folder+pngfile)\n",
    "                    plt.savefig(png_save_folder+pngfile,dpi = 75)\n",
    "                    plt.close()\n",
    "                    #plt.tight_layout(pad=1.0)\n",
    "#                         import matplotlib.transforms as transforms\n",
    "#                         trans = transforms.blended_transform_factory(ax.transAxes,ax.transData)\n",
    "#                         props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "#                         ax.text(0.52, 0.94, 'cell radius = '+str(np.round(updraft_radius*DXY/1000.,1))+\\\n",
    "#                                 ' km'+'\\n'+'tobac cell detection threshold ='+str(int(thresholds[ii]))+' m/s\\n'+\\\n",
    "#                                 'w threshold for sounding ='+str(int(FIXED_THREHOLD))+' m/s\\n'+\\\n",
    "#                                 'environment width = '+str(int(env_radius*DXY/1000.0))+' km',\n",
    "#                                 fontsize=9,verticalalignment='top',\\\n",
    "#                                 bbox=props,transform=ax.transAxes)\n",
    "\n",
    "\n",
    "        print('===============================\\n\\n')\n",
    "    \n",
    "    print('saving cell properties to ',DOMAIN+'_cell_'+str(CELLNO)+'_properties_comb_track_filt_01_02_50_02_sr5017_setpos.csv')\n",
    "    g.to_csv('/home/isingh/code/scratch/environmental_assessment/'+DOMAIN+'_cell_'+str(CELLNO)+'_properties_comb_track_filt_01_02_50_02_sr5017_setpos.csv')\n",
    "    return g\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "############################################################################\n",
    "# Paths to model data and where to save data\n",
    "domain='DRC1.1-R'\n",
    "#path = '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/'     # Pleiades\n",
    "#path = '/Users/isingh/SVH/INCUS/sample_LES_data/'+domain+'/' # personal macbook\n",
    "path ='/monsoon/LES_MODEL_DATA/'+domain+'/G3/out_30s/'        # CSU machine\n",
    "\n",
    "#savepath = './'\n",
    "#tobac_data='/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/'                                 # Pleiades\n",
    "#tobac_data='/Users/isingh/SVH/INCUS/jupyter_nbks/tobac_thermals/peter_tobac_output/'+domain+'/'# personal macbook\n",
    "tobac_data='/monsoon/pmarin/Tracking/Updrafts/'+domain+'/tobac_data/'                           # CSU machine\n",
    "\n",
    "#tobac_filename  = 'comb_track_filt_01_02_05_10_20.p'\n",
    "tobac_filename   = 'comb_track_filt_01_02_50_02_sr5017_setpos.p'\n",
    "tobac_filepath  = tobac_data+tobac_filename\n",
    "\n",
    "\n",
    "# Grab all the rams files \n",
    "h5filepath = path+'a-L*g3.h5'\n",
    "h5files1 = sorted(glob.glob(h5filepath))\n",
    "hefilepath = path+'a-L*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "#print(h5files1)\n",
    "start_time=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "end_time=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "print('Simulation name: ',domain)\n",
    "print('starting time of the simulation: ',start_time)\n",
    "print('ending time of the simulation: ',end_time)\n",
    "\n",
    "da_dummy = xr.open_dataset(h5files1[0],engine='h5netcdf', phony_dims='sort')\n",
    "\n",
    "domain_z_dim,domain_y_dim,domain_x_dim=np.shape(da_dummy['WP'])\n",
    "print('domain_z_dim: ',domain_z_dim)\n",
    "print('domain_y_dim: ',domain_y_dim)\n",
    "print('domain_x_dim: ',domain_x_dim)\n",
    "\n",
    "zm, zt, nx, ny, dxy, npa = read_head(hefiles1[0],h5files1[0])\n",
    "\n",
    "#******************\n",
    "PLOTTING_RANGE = 75 # 25 km # need to know now for filtering cells close to edges\n",
    "#******************\n",
    "\n",
    "##### read in tobac data #####\n",
    "print('reading ',tobac_filepath)\n",
    "tdata = pd.read_pickle(tobac_filepath)\n",
    "\n",
    "def filter_cells(g):\n",
    "    return ((g.zmn.max() >= 2000.0) & (g.zmn.min() <= 15000.) & (g.X.max() <= domain_x_dim-PLOTTING_RANGE-1) & (g.X.min() >= PLOTTING_RANGE+1) &\n",
    "            (g.Y.max() <= domain_y_dim-PLOTTING_RANGE-1) & (g.Y.min() >= PLOTTING_RANGE+1) & (g.threshold_value.count() >= 10)\\\n",
    "             & (pd.to_datetime(g.timestr).min() > pd.to_datetime(start_time)) & (pd.to_datetime(g.timestr).max() <  pd.to_datetime(end_time)) \\\n",
    "            )\n",
    "\n",
    "tdata_temp=tdata.groupby('cell').filter(filter_cells)\n",
    "#print(tdata_temp)\n",
    "\n",
    "#all_cells = tdata_temp.cell.unique()\n",
    "#print('number of unique cells identified: ',len(all_cells))\n",
    "#print('these cells are: ',all_cells)\n",
    "############################################################################\n",
    "\n",
    "\n",
    "# # Running in the notebook\n",
    "# cn = random.choice(all_cells)\n",
    "# print('cell#: ',cn)\n",
    "# tdata_subset=tdata_temp[tdata_temp['cell']==cn]\n",
    "# #print(tdata_subset)\n",
    "# output_df = get_cell_W_area_rad_itc_iwp_rain(domain,dxy,2.0,tdata_subset,cn,zm,zt,100,UPDRAFT_CONTAMINATION_CHECK=False,PLOT_QUANTITIES=True)#(domain,dxy,tdata_subset,cn,zm,zt,PLOTTING_RANGE,2.0)\n",
    "#output_df.to_csv('./cell_properties_'+domain+'_cell'+str(cn)+'.csv')\n",
    "# #\n",
    "\n",
    "already_done_cells=[76725, 70551, 47915, 34238, 15669, 81956, 40050, 67063, 14305, 69909, 29100, 17193, 37121, 52425,\\\n",
    "                    78575, 17050, 79269, 14080, 41566, 62830, 15001, 14048, 56979, 14935, 44436, 86891, 70681, 82133,\\\n",
    "                    66438, 62013, 70563, 48039, 74684, 31830, 12517, 34318, 29621, 8961, 46674, 11239, 17860, 34939,\\\n",
    "                    75306, 59846, 22353, 17862, 17165, 23153, 80186, 28327, 18431, 9687, 17160, 11816, 83535, 29837,\\\n",
    "                    12508, 14947, 22905, 20083, 56310, 73919, 48708, 35735, 40123, 15772, 34933, 43635, 25444, 29833,\\\n",
    "                    28435, 49257]\n",
    "\n",
    "total_cells= [8776, 8800, 8961, 9663, 9687, 10283, 11239, 11776, 11816, 11910, 11919, 12508, 12517, 12534, 13311, 14048,\\\n",
    "              14080, 14305, 14935, 14947, 14989, 15001, 15047, 15070, 15635, 15650, 15657, 15669, 15752, 15772, 16277, 16392,\\\n",
    "              16404, 16426, 16450, 17050, 17052, 17061, 17160, 17165, 17193, 17633, 17834, 17860, 17862, 18431, 19181, 19898,\\\n",
    "              20061, 20083, 21744, 22353, 22496, 22905, 23153, 23181, 24569, 25444, 25475, 26747, 26857, 26864, 26933, 27576,\\\n",
    "              27650, 28322, 28327, 28340, 28435, 28545, 29067, 29100, 29621, 29833, 29837, 29889, 30520, 30526, 31224, 31830,\\\n",
    "              32010, 32610, 32714, 33358, 34238, 34318, 34933, 34939, 35611, 35735, 36254, 36367, 37121, 37808, 38468, 38600,\\\n",
    "              40050, 40123, 40505, 41338, 41566, 42197, 42422, 43547, 43635, 43651, 44436, 45164, 45185, 45280, 45856, 45892,\\\n",
    "              45928, 46530, 46674, 47321, 47915, 48039, 48708, 48714, 49257, 49490, 51073, 51088, 51653, 51867, 52393, 52425,\\\n",
    "              52521, 53290, 54057, 54132, 54171, 54908, 56044, 56243, 56310, 56979, 57761, 59143, 59778, 59846, 59906, 60658,\\\n",
    "              61618, 62013, 62038, 62070, 62830, 62915, 63471, 64226, 64977, 65009, 65563, 65682, 66425, 66438, 67063, 67149,\\\n",
    "              69213, 69841, 69909, 70551, 70563, 70681, 71871, 72618, 73919, 73941, 74684, 74712, 75306, 75323, 75342, 76540,\\\n",
    "              76725, 77789, 78575, 79269, 79361, 79465, 79466, 79617, 80119, 80186, 80870, 81477, 81956, 81959, 82133, 82791,\\\n",
    "              83435, 83535, 84105, 85485, 86891, 87634, 88337, 90184]\n",
    "\n",
    "all_cells = [cc for cc in total_cells if cc not in already_done_cells]\n",
    "print('will work on <<',len(all_cells),'>>',all_cells)\n",
    "#Running on the terminal in parallel\n",
    "argument = []\n",
    "for cn in all_cells:\n",
    "    tdata_subset=tdata_temp[tdata_temp['cell']==cn]\n",
    "    argument = argument + [(domain,dxy,2.0,tdata_subset,cn,zm,zt,100,False,True)]\n",
    "\n",
    "print('length of argument is: ',len(argument))\n",
    "\n",
    "# # ############################### FIRST OF ALL ################################\n",
    "cpu_count1 = 4 #cpu_count()\n",
    "print('number of cpus: ',cpu_count1)\n",
    "# # #############################################################################\n",
    "\n",
    "def main(DOMAIN, FUNCTION, ARGUMENT):\n",
    "    start_time = time.perf_counter()\n",
    "    with Pool(processes = cpu_count1) as pool:\n",
    "        #for _ in tqdm.tqdm(pool.istarmap(FUNCTION, argument),total=len(argument)):\n",
    "        #    pass\n",
    "        data = pool.starmap(FUNCTION, ARGUMENT)\n",
    "    #output_df = pd.concat(data, axis=0)\n",
    "    #output_df.to_csv('/home/isingh/code/scratch/environmental_assessment/'+DOMAIN+'_cell_properties.csv')\n",
    "    #print('saving cell properties to : ','/home/isingh/code/scratch/environmental_assessment/'+DOMAIN+'_cell_properties.csv')\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(domain, get_cell_W_area_rad_itc_iwp_rain, argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea5f44-cd49-4fa7-bea6-6e39407a6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'DRC1.1-R'\n",
    "cell_props_files = sorted(glob.glob(('/home/isingh/code/scratch/environmental_assessment/'+domain+'_cell_*_properties_comb_track_filt_01_02_50_02_sr5017_setpos.csv')))\n",
    "print(len(cell_props_files))      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5d482-f2b1-4bfa-9160-5e505e1adf27",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5beadde-2812-473b-bb9c-92ca4f3daa7e",
   "metadata": {},
   "source": [
    "### Load everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feaee28-22c9-4f84-9a38-0435da08fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain='DRC1.1-R'\n",
    "\n",
    "#csv_folder = '/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/' # Pleiades\n",
    "#csv_folder = '/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/' # Personal computer\n",
    "csv_folder='/home/isingh/code/scratch/environmental_assessment/'                 # CSU machine\n",
    "thermo_csv_file1 = csv_folder+'thermodynamic_indices_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos_100_cells_other_uds_masked.csv'\n",
    "thermo_csv_file2 = csv_folder+'thermodynamic_indices_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos_100_cells_other_uds_not_masked.csv'\n",
    "print('thermo file 1 is : ',thermo_csv_file1)\n",
    "print('thermo file 2 is : ',thermo_csv_file2)\n",
    "thermo_df1 = pd.read_csv(thermo_csv_file1,index_col=0)\n",
    "thermo_df2 = pd.read_csv(thermo_csv_file2,index_col=0)\n",
    "thermo_df1['time'] = pd.to_datetime(thermo_df1['time'],format='%Y%m%d%H%M%S')\n",
    "thermo_df2['time'] = pd.to_datetime(thermo_df2['time'],format='%Y%m%d%H%M%S')\n",
    "\n",
    "unique_cells1=thermo_df1.cell.unique()\n",
    "unique_cells2=thermo_df2.cell.unique()\n",
    "#thermo_df\n",
    "#print(thermo_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc06e4e-a51e-4482-be3e-aee43927c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all cell properties DFs\n",
    "df_list =[pd.read_csv(csv_folder+domain+'_cell_'+str(cell)+'_properties_comb_track_filt_01_02_50_02_sr5017_setpos.csv',index_col=0) for cell in unique_cells1]\n",
    "cell_properties_df = pd.concat(df_list,axis=0)\n",
    "unique_cells = np.unique(cell_properties_df.cell)\n",
    "print(len(unique_cells))\n",
    "cell_properties_df\n",
    "cell_properties_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742215dd-75af-4fbb-a8e7-6d7e703bceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_properties_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4f9b2-00d9-40ea-a263-bc540034ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all cell properties DFs\n",
    "domain = 'DRC1.1-R'\n",
    "tobac_data='/monsoon/pmarin/Tracking/Updrafts/'+domain+'/tobac_data/'                           # CSU machine\n",
    "tobac_filename   = 'comb_track_filt_01_02_50_02_sr5017_setpos.p'\n",
    "tobac_filepath  = tobac_data+tobac_filename\n",
    "tdata = pd.read_pickle(tobac_filepath)\n",
    "\n",
    "non_zero_cells = []\n",
    "sum_cells=0\n",
    "#for cell in unique_cells:\n",
    "cell_prop_files = glob.glob(csv_folder+domain+'_cell_*_properties_comb_track_filt_01_02_50_02_sr5017_setpos.csv')\n",
    "print(len(cell_prop_files))\n",
    "for fi in cell_prop_files:\n",
    "    df = pd.read_csv(fi,index_col=0)\n",
    "    fname = os.path.basename(fi)\n",
    "    split_filename = fname.split(\"_\")\n",
    "    cell     = split_filename[2]\n",
    "    #print(cell)\n",
    "    #df = pd.read_csv(csv_folder+domain+'_cell_'+str(cell)+'_properties_comb_track_filt_01_02_50_02_sr5017_setpos.csv',index_col=0)\n",
    "    #print(df)\n",
    "    if len(df) >= 1:\n",
    "        non_zero_cells.append(int(cell))\n",
    "        #print(len(df),' ',len(tdata[tdata.cell==cell]))\n",
    "        sum_cells=sum_cells+len(df)\n",
    "print(len(non_zero_cells))\n",
    "print(sum_cells)\n",
    "print(non_zero_cells)\n",
    "unique_cells=non_zero_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16b35d-48fb-4d86-b528-83a414659387",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b5fc1-6e47-4d1a-9f2b-9dc2713f3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.dist_dependence_measures import distance_correlation\n",
    "\n",
    "def calc_corr_coeff(THERMO_INDICES_DF,ENV_VAR,STORM_VAR,CELL_NOS,ENV_WIDTHS,COEFF_NAME):\n",
    "    \n",
    "    print('    ',ENV_VAR)\n",
    "    correlation_list=[]\n",
    "    env_var=[]\n",
    "    storm_var=[]\n",
    "    \n",
    "    for env_width in ENV_WIDTHS:\n",
    "        env_var=[]\n",
    "        storm_var=[]\n",
    "        for cell in CELL_NOS:\n",
    "            #print('cell# : ',cell)\n",
    "            thermo_df_cell_subset = THERMO_INDICES_DF[(THERMO_INDICES_DF.cell==cell) & (THERMO_INDICES_DF.env_width==env_width)]\n",
    "            cell_props_df_subset  = pd.read_csv(csv_folder+domain+'_cell_'+str(cell)+'_properties_comb_track_filt_01_02_50_02_sr5017_setpos.csv')\n",
    "            #print(cell_props_df_subset)\n",
    "            env_var.append(thermo_df_cell_subset[ENV_VAR].values[0])\n",
    "            storm_var.append(np.nanmax(cell_props_df_subset[STORM_VAR].values))\n",
    "\n",
    "        if COEFF_NAME=='pearson':\n",
    "            corr_coeff=np.corrcoef(env_var, storm_var)[0,1]\n",
    "            correlation_list.append(corr_coeff)\n",
    "            \n",
    "        if COEFF_NAME=='distance_correlation':\n",
    "            corr_coeff=distance_correlation(env_var, storm_var)\n",
    "            correlation_list.append(corr_coeff)\n",
    "            \n",
    "        if COEFF_NAME=='MIC':\n",
    "            # Create a MINE object\n",
    "            mine = MINE()\n",
    "            # Compute the MIC between x and y\n",
    "            mine.compute_score(env_var,storm_var)\n",
    "            # The MIC score is stored in the mic value\n",
    "            corr_coeff = mine.mic()\n",
    "            correlation_list.append(corr_coeff)\n",
    "    print('    ===========')\n",
    "    return correlation_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50524737-3def-4b49-97f7-99bdb5088a14",
   "metadata": {},
   "source": [
    "### Variation of Pearson CC with environmental width for all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52d504-1113-41d1-91ff-4d6ea84fbc8c",
   "metadata": {},
   "source": [
    "#### thermodynamic quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e499fa4-e252-4674-a4bb-5bbbeb06ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_widths=np.arange(10,210,10)*100.\n",
    "\n",
    "env_var_list=['MLCAPE'        ,'MLCIN'          , 'MLLCL'        , 'MLLFC',\\\n",
    "              'SFC-800 hPa RH', '800-400 hPa RH','400-100 hPa RH', 'Precipitable Water']\n",
    "\n",
    "\n",
    "for storm_prop in ['wcentroid_mps','calcellipseradius_m','IWP_mm','area_m2','ITC_mm','rainmax_mm_per_hr', 'rain_mean']:#,'area_m2','calcellipseradius_m','eccentricity','IWP_mm','ITC_mm','rainmax_mm_per_hr','rain_mean',]:\n",
    "    \n",
    "    print('\\n\\nworking on storm property: ',storm_prop)\n",
    "    \n",
    "    all_corr_list1 = [calc_corr_coeff(thermo_df1,aa,storm_prop,unique_cells,env_widths,'pearson') for aa in env_var_list]\n",
    "    #all_corr_list2 = [calc_corr_coeff(thermo_df2,aa,storm_prop,unique_cells,env_widths,'pearson') for aa in env_var_list]\n",
    "\n",
    "    ### plotting\n",
    "    linestyle_list1=['solid','dotted','dashed','dashdot',\\\n",
    "                    'solid','dotted','dashed','dashdot']\n",
    "    \n",
    "    #linestyle_list1=['solid','solid','solid','solid',\\\n",
    "    #                 'solid','solid','solid','solid']\n",
    "    \n",
    "    #linestyle_list2=['dashed','dashed','dashed','dashed',\\\n",
    "    #                 'dashed','dashed','dashed','dashed']\n",
    "    \n",
    "    colors1 =['green','red','gold','pink',\\\n",
    "              'black','gray','chocolate','darkviolet']\n",
    "\n",
    "    NUM_COLORS = len(env_var_list)\n",
    "    cm         = plt.get_cmap('nipy_spectral')\n",
    "    fig, ax    = plt.subplots(figsize=(8,6))\n",
    "    ax2        = ax.twinx() \n",
    "\n",
    "    for ev,cc,ls,co in zip(env_var_list,all_corr_list1,linestyle_list1,colors1):\n",
    "        print('    plotting ',ev,' ',ls)\n",
    "        ax.plot(env_widths/1000.0,cc,label=ev,linestyle=ls,color=co)\n",
    "        ax.set_xlabel('environmental width (km)')\n",
    "        ax.set_ylabel('correlation')\n",
    "        plt.title('Correlation b/w initial environment and storm lifetime max '+storm_prop)\n",
    "        \n",
    "    #for ev,cc,ls,co in zip(env_var_list,all_corr_list2,linestyle_list2,colors1):\n",
    "    #    print('    plotting ',ev,' ',ls)\n",
    "    #    ax.plot(env_widths/1000.0,cc,linestyle=ls,color=co)\n",
    "\n",
    "     \n",
    "    ud_radii = cell_properties_df.groupby('cell')['calcellipseradius_m'].max().values*2.0/1000.0\n",
    "    sns.kdeplot(ud_radii,ax=ax2, color='green',cut=0,fill='True')    \n",
    "    ax2.set_ylabel('PDF of updraft diameter')\n",
    "    ax.legend(bbox_to_anchor=(1.10, 1.0), loc='upper left')\n",
    "    \n",
    "    png_name = storm_prop+'_lifetime_env_vars_pearson_correlation_comb_track_filt_01_02_50_02_sr5017_setpos.png'\n",
    "    print('saving to ',png_name)\n",
    "    plt.savefig(png_name,dpi=150,bbox_inches='tight')\n",
    "#print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc42b8-706d-47b0-9126-7e9a52cbc647",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_properties_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf018f3-6bc8-4ce7-97c6-766e1e5a3c96",
   "metadata": {},
   "source": [
    "#### Kinematic quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac8c37-8684-4012-b74b-e592f017c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "thermo_csv_file = csv_folder+'thermodynamic_indices_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos_100_cells_other_uds_not_masked.csv'\n",
    "print('thermo file is : ',thermo_csv_file)\n",
    "thermo_df = pd.read_csv(thermo_csv_file,index_col=0)\n",
    "thermo_df['time'] = pd.to_datetime(thermo_df['time'],format='%Y%m%d%H%M%S')\n",
    "\n",
    "\n",
    "env_widths=np.arange(10,210,10)*100.\n",
    "\n",
    "env_var_list=['0-1 km Shear','0-3 km Shear', '0-6 km Shear']\n",
    "\n",
    "\n",
    "for storm_prop in ['wcentroid_mps']:#,'area_m2','calcellipseradius_m','eccentricity','IWP_mm','ITC_mm','rainmax_mm_per_hr','rain_mean',]:\n",
    "    \n",
    "    print('working on storm property: ',storm_prop)\n",
    "    \n",
    "    all_corr_list1 = [calc_corr_coeff(thermo_df1,aa,storm_prop,unique_cells,env_widths,'pearson') for aa in env_var_list]\n",
    "    all_corr_list2 = [calc_corr_coeff(thermo_df2,aa,storm_prop,unique_cells,env_widths,'pearson') for aa in env_var_list]\n",
    "\n",
    "    ### plotting\n",
    "    linestyle_list1=['solid','solid','solid']\n",
    "    linestyle_list2=['dashed','dashed','dashed']\n",
    "    colors1 =['green','red','gold']\n",
    "\n",
    "    NUM_COLORS = len(env_var_list)\n",
    "    cm         = plt.get_cmap('nipy_spectral')\n",
    "    fig, ax    = plt.subplots(figsize=(8,6))\n",
    "    ax2        = ax.twinx() \n",
    "\n",
    "    for ev,cc,ls,co in zip(env_var_list,all_corr_list1,linestyle_list1,colors1):\n",
    "        print('plotting ',ev,' ',ls)\n",
    "        ax.plot(env_widths/1000.0,cc,label=ev,linestyle=ls,color=co)\n",
    "        ax.set_xlabel('environmental width (km)')\n",
    "        ax.set_ylabel('correlation')\n",
    "        plt.title('Correlation b/w initial environment and storm lifetime max '+storm_prop)\n",
    "        \n",
    "    for ev,cc,ls,co in zip(env_var_list,all_corr_list2,linestyle_list2,colors1):\n",
    "        print('plotting ',ev,' ',ls)\n",
    "        ax.plot(env_widths/1000.0,cc,linestyle=ls,color=co)\n",
    "     \n",
    "    ud_radii = cell_properties_df.groupby('cell')['calcellipseradius_m'].max().values*2.0/1000.0\n",
    "    sns.kdeplot(ud_radii,ax=ax2, color='green',cut=0,fill='True')    \n",
    "    ax2.set_ylabel('PDF of updraft diameter')\n",
    "    ax.legend(bbox_to_anchor=(1.10, 1.0), loc='upper left')\n",
    "    \n",
    "#plt.savefig(storm_prop+'_lifetime_env_vars_pearson_correlation_comb_track_filt_01_02_50_02_sr5017_setpos.png',dpi=150,bbox_inches='tight')\n",
    "#print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c8bff-45ea-43fa-bfdc-f2f49a619dcf",
   "metadata": {},
   "source": [
    "### Variation of distance CC with environmental width for all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4019f80-e0c4-4b8c-8d29-2bf20fc9f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_widths=np.arange(10,210,10)*100.\n",
    "\n",
    "env_var_list=['MLCAPE'        ,'MLCIN'          , 'MLLCL'        , 'MLLFC',\\\n",
    "              'SFC-800 hPa RH', '800-400 hPa RH','400-100 hPa RH', 'Precipitable Water']\n",
    "\n",
    "\n",
    "for storm_prop in ['wcentroid_mps']:#,'area_m2','calcellipseradius_m','eccentricity','IWP_mm','ITC_mm','rainmax_mm_per_hr','rain_mean',]:\n",
    "    \n",
    "    print('working on storm property: ',storm_prop)\n",
    "    \n",
    "    all_corr_list1 = [calc_corr_coeff(thermo_df1,aa,storm_prop,unique_cells,env_widths,'distance_correlation') for aa in env_var_list]\n",
    "    #all_corr_list2 = [calc_corr_coeff(thermo_df2,aa,storm_prop,unique_cells,env_widths,'distance_correlation') for aa in env_var_list]\n",
    "\n",
    "    ### plotting\n",
    "    #linestyle_list=['solid','dotted','dashed','dashdot',\\\n",
    "    #                'solid','dotted','dashed','dashdot']\n",
    "    \n",
    "    linestyle_list1=['solid','solid','solid','solid',\\\n",
    "                     'solid','solid','solid','solid']\n",
    "    \n",
    "    linestyle_list2=['dashed','dashed','dashed','dashed',\\\n",
    "                     'dashed','dashed','dashed','dashed']\n",
    "    \n",
    "    colors1 =['green','red','gold','pink',\\\n",
    "              'black','gray','chocolate','darkviolet']\n",
    "\n",
    "    NUM_COLORS = len(env_var_list)\n",
    "    cm         = plt.get_cmap('nipy_spectral')\n",
    "    fig, ax    = plt.subplots(figsize=(8,6))\n",
    "    ax2        = ax.twinx() \n",
    "\n",
    "    for ev,cc,ls,co in zip(env_var_list,all_corr_list1,linestyle_list1,colors1):\n",
    "        print('plotting ',ev,' ',ls)\n",
    "        ax.plot(env_widths/1000.0,cc,label=ev,linestyle=ls,color=co)\n",
    "        ax.set_xlabel('environmental width (km)')\n",
    "        ax.set_ylabel('correlation')\n",
    "        plt.title('Correlation b/w initial environment and storm lifetime max '+storm_prop)\n",
    "        \n",
    "#     for ev,cc,ls,co in zip(env_var_list,all_corr_list2,linestyle_list2,colors1):\n",
    "#         print('plotting ',ev,' ',ls)\n",
    "#         ax.plot(env_widths/1000.0,cc,linestyle=ls,color=co)\n",
    "\n",
    "     \n",
    "    ud_radii = cell_properties_df.groupby('cell')['calcellipseradius_m'].max().values*2.0/1000.0\n",
    "    sns.kdeplot(ud_radii,ax=ax2, color='green',cut=0,fill='True')    \n",
    "    ax2.set_ylabel('PDF of updraft diameter')\n",
    "    ax.legend(bbox_to_anchor=(1.10, 1.0), loc='upper left')\n",
    "    \n",
    "#plt.savefig(storm_prop+'_lifetime_env_vars_pearson_correlation_comb_track_filt_01_02_50_02_sr5017_setpos.png',dpi=150,bbox_inches='tight')\n",
    "#print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58859b48-4871-4e69-8b23-4e949f6d4845",
   "metadata": {},
   "source": [
    "### Variation of CC with environmental width for various categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072ed12-9e64-4f1d-9ece-3f463a22d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_widths=np.arange(10,210,10)*100.\n",
    "\n",
    "env_var_list=['MLCAPE']#,'MLCIN' , 'MLLCL' , 'MLLFC',\\\n",
    "#              'SFC-800 hPa RH', '800-400 hPa RH','400-100 hPa RH', 'Precipitable Water', \\\n",
    "#              '0-1 km Shear','0-3 km Shear', '0-6 km Shear']\n",
    "\n",
    "\n",
    "def divide_cell_into_categories(CELL_PROPS_DF,CELL_PROP,THRESHOLDS):\n",
    "    cell_max_radii  = CELL_PROPS_DF.groupby('cell')[CELL_PROP].max()\n",
    "    threshold_pairs = [(THRESHOLDS[i], THRESHOLDS[i + 1]) for i in range(0, len(THRESHOLDS), 2)]\n",
    "    category_list   = []\n",
    "    for value1, value2 in threshold_pairs:\n",
    "        print(value1, value2)\n",
    "        category_list.append(cell_max_radii[(cell_max_radii > value1) & (cell_max_radii < value2)])\n",
    "    return category_list\n",
    "\n",
    "\n",
    "fig, ax    = plt.subplots(figsize=(8,6))\n",
    "ax2        = ax.twinx() \n",
    "\n",
    "\n",
    "\n",
    "for storm_prop in ['wcentroid_mps']:#,'area_m2','calcellipseradius_m','eccentricity','IWP_mm','ITC_mm','rainmax_mm_per_hr','rain_mean',]:\n",
    "\n",
    "    print('working on storm property: ',storm_prop)\n",
    "\n",
    "    all_corr_list = [calc_corr_coeff(thermo_df,'MLCAPE',storm_prop,category_pd_series.index.to_list(),env_widths,'pearson') for category_pd_series in divide_cell_into_categories(cell_properties_df,'calcellipseradius_m',[0,1500,1501,2600,2601,5000,5001,20000])]\n",
    "\n",
    "    ### plotting\n",
    "    linestyle_list=['solid','dotted','dashed','dashdot',\\\n",
    "                    'solid','dotted','dashed','dashdot',\\\n",
    "                    'solid','dotted','dashed','dashdot',\\\n",
    "                    'solid','dotted','dashed','dashdot',\\\n",
    "                    'solid','dotted','dashed']\n",
    "\n",
    "    for cc in all_corr_list:\n",
    "        #print('plotting ',ev,' ',ls)\n",
    "        ax.plot(env_widths/1000.0,cc,label=ev)#,linestyle=ls)\n",
    "        ax.set_xlabel('environmental width (km)')\n",
    "        ax.set_ylabel('correlation')\n",
    "        plt.title('Distance correlation b/w initial environment and storm lifetime max '+storm_prop)\n",
    "\n",
    "#     ud_radii = cell_properties_df.groupby('cell')['calcellipseradius_m'].max().values*2.0/1000.0\n",
    "#     sns.kdeplot(ud_radii,ax=ax2, color='green',cut=0,fill='True')    \n",
    "#     ax2.set_ylabel('PDF of updraft diameter')\n",
    "#     ax.legend(bbox_to_anchor=(1.10, 1.0), loc='upper left')\n",
    "\n",
    "# plt.savefig(storm_prop+'_lifetime_env_vars_distance_correlations_comb_track_filt_01_02_50_02_sr5017_setpos.png',dpi=150,bbox_inches='tight')\n",
    "# #print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d4b74",
   "metadata": {},
   "source": [
    "### Correlation heatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a6af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "thermo_df.corr()\n",
    "# Increase the size of the heatmap.\n",
    "plt.figure(figsize=(16, 6))\n",
    "# Store heatmap object in a variable to easily access it when you want to include more features (such as title).\n",
    "# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.\n",
    "#heatmap = sns.heatmap(dataframe.corr(), vmin=-1, vmax=1, annot=True)\n",
    "# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\n",
    "#heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)\n",
    "\n",
    "# define the mask to set the values in the upper triangle to True\n",
    "mask = np.triu(np.ones_like(thermo_df.corr(), dtype=np.bool))\n",
    "heatmap = sns.heatmap(thermo_df.corr(), mask=mask, vmin=-1, vmax=1, annot=False)#, cmap='BrBG')\n",
    "heatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e371940",
   "metadata": {},
   "source": [
    "### Relating updraft characteristics with environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a42a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scatter_plot(DOMAIN,THERMO_DF,CELL_PROP_DF,ENV_RADIUS,THERMO_QUANTITY,UNITS):\n",
    "\n",
    "    thermo_df_index_cell = THERMO_DF[THERMO_DF.env_width==ENV_RADIUS].groupby('cell').filter().first()\n",
    "    cell_prop_df_index_cell = CELL_PROP_DF.groupby('cell').max()\n",
    "    \n",
    "    total_df = pd.merge(thermo_df_index_cell, cell_prop_df_index_cell, left_index=True, right_index=True)\n",
    "    print(total_df)\n",
    "    sc_plot = plt.scatter(total_df[THERMO_QUANTITY],total_df.wcentroid_mps,c=total_df.zmn)\n",
    "    ###############\n",
    "    ###############\n",
    "    plt.xlabel(THERMO_QUANTITY+' ('+UNITS+')')\n",
    "    plt.ylabel('w max (m/s) for cells')\n",
    "    plt.title(DOMAIN+'\\n'+THERMO_QUANTITY+' v. '+' w_max')\n",
    "    plt.colorbar()\n",
    "    filename = 'scatter_plot_'+THERMO_QUANTITY.replace(\" \", \"_\")+'_'+'w_max'+'_'+DOMAIN+'.png'\n",
    "    print('output png: ',filename)\n",
    "    #plt.savefig(filename,dpi=200)\n",
    "    \n",
    "create_scatter_plot('DRC1.1-R',thermo_df2,cell_properties_df,4000,'MLCAPE','J/kg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d7843",
   "metadata": {},
   "source": [
    "### PDFs of storm properties: area, radius, wmax, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def make_pdf_storm_property(DOMAIN,STORM_PROPERTY,CELL_PROPS_DF):\n",
    "    #csv_folder        ='/home/isingh/code/scratch/environmental_assessment/'                 # CSU machine\n",
    "    #cell_prop_csv_file   = csv_folder+'thermodynamic_indices_'+DOMAIN+'_comb_track_filt_01_02_50_02_sr5017_setpos.csv'\n",
    "    #print('thermo file is : ',thermo_csv_file)\n",
    "    #cell_prop_df_index_cell = cell_properties_df.groupby('cell').max()\n",
    "    \n",
    "    # Group by 'group_column' and define a custom aggregation function\n",
    "    def get_middle_value(group):\n",
    "        middle_index = len(group) // 2\n",
    "        return group.iloc[middle_index]\n",
    "\n",
    "    cell_prop_df_index_cell1 = CELL_PROPS_DF.groupby('cell').first()\n",
    "    cell_prop_df_index_cell2 = CELL_PROPS_DF.groupby('cell').last()\n",
    "    cell_prop_df_index_cell3 = CELL_PROPS_DF.groupby('cell').apply(get_middle_value)\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ss1 = sns.kdeplot(cell_prop_df_index_cell1[STORM_PROPERTY],label='initial',fill=False)    \n",
    "    ss2 = sns.kdeplot(cell_prop_df_index_cell2[STORM_PROPERTY],label='final',fill=False)  \n",
    "    ss3 = sns.kdeplot(cell_prop_df_index_cell3[STORM_PROPERTY],label='middle',fill=False)  \n",
    "    plt.xlabel(STORM_PROPERTY)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('PDF of '+STORM_PROPERTY)\n",
    "    #plt.yscale('log')\n",
    "    #plt.xlim([1,65])\n",
    "    plt.legend()\n",
    "    filenamepng='cells_wmax_pdf_lifetime_'+'.png'\n",
    "    print(filenamepng)\n",
    "    plt.savefig(filenamepng,dpi=150)\n",
    "    \n",
    "make_pdf_storm_property('DRC1.1-R','wcentroid_mps',cell_properties_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17047db",
   "metadata": {},
   "source": [
    "### PDFs of environmental properties like CAPE, CIN, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93907a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def make_pdf_env_variable(DOMAIN,ENV_RADII,VAR):\n",
    "    csv_folder        ='/home/isingh/code/scratch/environmental_assessment/'                 # CSU machine\n",
    "    thermo_csv_file   = glob.glob(csv_folder+'area_avgd_annulus_envwidth_*m_2mps_mean_sounding_cell_*_*_'+DOMAIN+'_comb_track_filt_01_02_50_02_sr5017_setpos_other_uds_masked.csv')\n",
    "    #print('thermo file is : ',thermo_csv_file)\n",
    "    thermo_df         = pd.read_csv(thermo_csv_file,index_col=0)\n",
    "    thermo_df['time'] = pd.to_datetime(thermo_df['time'],format='%Y%m%d%H%M%S') \n",
    "    thermo_df         = thermo_df.sort_values(['time'],ascending=True)\n",
    "    \n",
    "    # Group by 'group_column' and define a custom aggregation function\n",
    "    def get_middle_value(group):\n",
    "        middle_index = len(group) // 2\n",
    "        return group.iloc[middle_index]\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    for ENV_RADIUS in ENV_RADII:\n",
    "        print(ENV_RADIUS)\n",
    "        thermo_df_index_cell1 = thermo_df[thermo_df.env_width==ENV_RADIUS].groupby('cell').first()\n",
    "        #thermo_df_index_cell2 = thermo_df[thermo_df.env_width==ENV_RADIUS].groupby('cell').last()\n",
    "        #thermo_df_index_cell3 = thermo_df[thermo_df.env_width==ENV_RADIUS].groupby('cell').apply(get_middle_value)\n",
    "\n",
    "        ss1 = sns.kdeplot(thermo_df_index_cell1[VAR],label=str(ENV_RADIUS)+' m',fill=False,cut=0)    \n",
    "        #ss2 = sns.kdeplot(thermo_df_index_cell2.MLCAPE,label='final',fill=False,cut=0)  \n",
    "        #ss3 = sns.kdeplot(thermo_df_index_cell3.MLCAPE,label='middle',fill=False,cut=0)  \n",
    "        plt.xlabel(VAR)\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('PDF of '+VAR)\n",
    "    \n",
    "    #plt.yscale('log')\n",
    "    #plt.xlim([1,65])\n",
    "    plt.legend()\n",
    "    filenamepng='PDF_'+VAR+'DRC1.1-R.png'\n",
    "    print(filenamepng)\n",
    "    plt.savefig(filenamepng,dpi=150)\n",
    "    \n",
    "make_pdf_env_variable('DRC1.1-R',[1000,5000,10000,20000],'Precipitable Water')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d727468-71fc-4a53-ae3e-287bcd9fd1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def make_pdf_storm_property(DOMAIN,STORM_PROPERTY,CELL_PROPS_DF):\n",
    "    #csv_folder        ='/home/isingh/code/scratch/environmental_assessment/'                 # CSU machine\n",
    "    #cell_prop_csv_file   = csv_folder+'thermodynamic_indices_'+DOMAIN+'_comb_track_filt_01_02_50_02_sr5017_setpos.csv'\n",
    "    #print('thermo file is : ',thermo_csv_file)\n",
    "    #cell_prop_df_index_cell = cell_properties_df.groupby('cell').max()\n",
    "    \n",
    "    # Group by 'group_column' and define a custom aggregation function\n",
    "    def get_middle_value(group):\n",
    "        middle_index = len(group) // 2\n",
    "        return group.iloc[middle_index]\n",
    "    \n",
    "    cell_prop_df_index_cell1 = CELL_PROPS_DF.groupby('cell').first()\n",
    "    cell_prop_df_index_cell2 = CELL_PROPS_DF.groupby('cell').last()\n",
    "    cell_prop_df_index_cell3 = CELL_PROPS_DF.groupby('cell').apply(get_middle_value)\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ss1 = sns.kdeplot(cell_prop_df_index_cell1[STORM_PROPERTY],label='initial',fill=False,cut=0)    \n",
    "    ss2 = sns.kdeplot(cell_prop_df_index_cell2[STORM_PROPERTY],label='final',fill=False,cut=0)  \n",
    "    ss3 = sns.kdeplot(cell_prop_df_index_cell3[STORM_PROPERTY],label='middle',fill=False,cut=0)  \n",
    "    plt.xlabel(STORM_PROPERTY)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('PDF of '+STORM_PROPERTY)\n",
    "    #plt.yscale('log')\n",
    "    #plt.xlim([1,65])\n",
    "    plt.legend()\n",
    "    filenamepng='cells_'+STORM_PROPERTY+'_pdf_lifetime_DRC1.1-R'+'.png'\n",
    "    print(filenamepng)\n",
    "    plt.savefig(filenamepng,dpi=150)\n",
    "    \n",
    "#make_pdf_storm_property('DRC1.1-R','MLCAPE',thermo_df1)\n",
    "make_pdf_storm_property('DRC1.1-R','wcentroid_mps',cell_properties_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e8982-b7d0-4f2a-9ac3-a7b01915b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def make_pdf_env_variable(DOMAIN,ENV_WIDTHS,VAR):\n",
    "    csv_folder    ='/home/isingh/code/scratch/environmental_assessment/'                 # CSU machine\n",
    "    sounding_files=sorted(glob.glob(csv_folder+'area_avgd_annulus_envwidth_'+str(ENV_WIDTHS[0])+'m_2mps_mean_sounding_cell_*_*_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos_other_uds_masked.csv'))\n",
    "    print('# files found',len(sounding_files))\n",
    "    \n",
    "    cells_processed = []\n",
    "    env_radii       = []\n",
    "    timestr_list    = []\n",
    "    \n",
    "    for fil in sounding_files:\n",
    "        fname = os.path.basename(fil)\n",
    "        split_filename = fname.split(\"_\")\n",
    "        #print(split_filename)\n",
    "        env_radius = re.sub('[^0-9]','', split_filename[4])\n",
    "        #print(env_radius)\n",
    "        cellno     = split_filename[9]\n",
    "        timestr    = split_filename[10]\n",
    "        cells_processed.append(int(cellno))\n",
    "        env_radii.append(int(env_radius))\n",
    "        timestr_list.append(pd.to_datetime(int(timestr),format='%Y%m%d%H%M%S'))\n",
    "  \n",
    "    print('unique cells processed: ',len(np.unique(np.array(cells_processed))),' cells namely: ',np.unique(np.array(cells_processed)))\n",
    "\n",
    "    data = {'cell': cells_processed, 'time': timestr_list}\n",
    "    cells_first_time_df =  pd.DataFrame(data)\n",
    "    print(type(cells_first_time_df))\n",
    "    print(cells_first_time_df)\n",
    "    cells_initial_times = cells_first_time_df.groupby('cell')['time'].min()\n",
    "    print(cells_initial_times)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    for ENV_WIDTH in ENV_WIDTHS:\n",
    "        csv_files_envwidth_initial_time = []\n",
    "        for index, value in cells_initial_times.iteritems():\n",
    "            #print(index,'  ',value)\n",
    "            csv_files_envwidth_initial_time.append(sorted(glob.glob(csv_folder+'area_avgd_annulus_envwidth_'+str(ENV_WIDTH)+'m_2mps_mean_sounding_cell_'+str(index)+'_'+value.strftime('%Y%m%d%H%M%S')+'_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos_other_uds_masked.csv'))[0])\n",
    "\n",
    "        print(len(csv_files_envwidth_initial_time))  \n",
    "        df_list = [pd.read_csv(fil) for fil in csv_files_envwidth_initial_time]\n",
    "        var_values = [df.loc[0, VAR] for df in df_list]\n",
    "        ss1 = sns.kdeplot(var_values,label=str(ENV_WIDTH)+' m',fill=False,cut=0)    \n",
    "    \n",
    "    \n",
    "    plt.xlabel(VAR)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('PDF of '+VAR)\n",
    "    plt.legend()\n",
    "    filenamepng='tobac_tracking_stats_pdf_lifetime_'+'.png'\n",
    "    print(filenamepng)\n",
    "    #plt.savefig(filenamepng,dpi=150)\n",
    "\n",
    "make_pdf_env_variable('DRC1.1-R',[1000,5000,10000,20000],'temp_degC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730080cd-b918-405a-b4ea-aa75067701b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multiple thermo indices files\n",
    "\n",
    "domain='DRC1.1-R'\n",
    "csv_folder='/home/isingh/code/scratch/environmental_assessment/'                 # CSU machine\n",
    "thermo_csv_files = glob.glob(csv_folder+'thermodynamic_indices_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos*.csv')\n",
    "print(thermo_csv_files)\n",
    "df_list=[]\n",
    "for file in thermo_csv_files:\n",
    "    df_list.append(pd.read_csv(file,index_col=0))\n",
    "\n",
    "thermo_df=pd.concat(df_list,axis=0)\n",
    "thermo_df['time'] = pd.to_datetime(thermo_df['time'],format='%Y%m%d%H%M%S')\n",
    "unique_cells=thermo_df.cell.unique()\n",
    "thermo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1878df7-aa5d-4052-b281-a82c5f429086",
   "metadata": {},
   "source": [
    "### Variability in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc849b1d-9f6d-463a-9880-bb0696b23942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "csv_folder     = '/home/isingh/code/scratch/environmental_assessment/'                 # CSU machine\n",
    "sounding_files = glob.glob(csv_folder + 'area_avgd_annulus_envwidth_'+ \\\n",
    "                '*m_2mps_std_sounding_cell_*_*_DRC1.1-R_comb_track_filt_01_02_50_02_sr5017_setpos_other_uds_masked.csv')\n",
    "\n",
    "print(len(sounding_files))\n",
    "\n",
    "def get_(ENV_WIDTHS,ENV_VAR):\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    colors = ['silver','peachpuff']\n",
    "    \n",
    "    for ENV_WIDTH in ENV_WIDTHS:\n",
    "        print('working on width ',ENV_WIDTH)\n",
    "        cells_processed = []\n",
    "        timestr_list    = []\n",
    "\n",
    "        csv_files_envwidth=sorted(glob.glob(csv_folder+'area_avgd_annulus_envwidth_'+str(ENV_WIDTH)+'m_2mps_mean_sounding_cell_*_*_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos_other_uds_masked.csv'))\n",
    "        print('found ',len(csv_files_envwidth))\n",
    "        for fil in csv_files_envwidth:\n",
    "            fname = os.path.basename(fil)\n",
    "            split_filename = fname.split(\"_\")\n",
    "            cellno     = split_filename[9]\n",
    "            timestr    = split_filename[10]\n",
    "            cells_processed.append(int(cellno))\n",
    "            timestr_list.append(pd.to_datetime(int(timestr),format='%Y%m%d%H%M%S'))\n",
    "\n",
    "        data = {'cell': cells_processed, 'time': timestr_list}\n",
    "\n",
    "        cells_first_time_df =  pd.DataFrame(data).groupby('cell').time.min()\n",
    "\n",
    "        #print(cells_first_time_df)\n",
    "\n",
    "        #print('------\\n\\n')\n",
    "\n",
    "        csv_files_envwidth = []\n",
    "        for index, value in cells_first_time_df.iteritems():\n",
    "            #print(index,'  ',value)\n",
    "            csv_files_envwidth.append(sorted(glob.glob(csv_folder+'area_avgd_annulus_envwidth_'+str(ENV_WIDTH)+'m_2mps_std_sounding_cell_'+str(index)+'_'+value.strftime('%Y%m%d%H%M%S')+'_'+domain+'_comb_track_filt_01_02_50_02_sr5017_setpos_other_uds_masked.csv'))[0])\n",
    "\n",
    "        print(len(csv_files_envwidth))\n",
    "        \n",
    "        df_list_var = []\n",
    "        df_list_hgt = []\n",
    "        for ff in csv_files_envwidth:\n",
    "            dddf = pd.read_csv(ff)\n",
    "            #plt.plot(dddf.temp_degC,dddf.height_m,linewidth=0.5,color=color)\n",
    "            df_list_var.append(dddf[ENV_VAR])\n",
    "            df_list_hgt.append(dddf.height_m)\n",
    "         \n",
    "        df_mean_var = pd.concat(df_list_var, axis=1)\n",
    "        df_mean_hgt = pd.concat(df_list_hgt, axis=1)\n",
    "        # Calculate the mean along the desired axis (0 for column-wise mean)\n",
    "        mean_var = df_mean_var.mean(axis=1)\n",
    "        mean_hgt = df_mean_hgt.mean(axis=1)\n",
    "        plt.plot(mean_var,mean_hgt,label=str(ENV_WIDTH)+' m')\n",
    "        plt.title('mean standard deviation of '+ENV_VAR)\n",
    "        plt.ylim([0,17000])\n",
    "        plt.legend()\n",
    "        png_name = 'mean_std_dev_with_height_for_'+ENV_VAR+'.png'\n",
    "        print(png_name)\n",
    "        plt.savefig(png_name,dpi=150)\n",
    "\n",
    "for vv in ['temp_degC','dewpt_degC','uwnd_mps','vwnd_mps']:\n",
    "    get_([1000,5000,10000,15000,20000],vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8b227-b946-4fd1-ad66-ee806ed35d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of Pandas Series (replace with your own Series)\n",
    "series1 = pd.Series([1, 2, 3, 4, 5])\n",
    "series2 = pd.Series([6, 7, 8, 9, 10])\n",
    "series3 = pd.Series([11, 12, 13, 14, 15])\n",
    "\n",
    "# Create a list of Series\n",
    "series_list = [series1, series2, series3]\n",
    "print(series_list)\n",
    "# Combine the Series into a DataFrame\n",
    "df = pd.concat(series_list, axis=1)\n",
    "print(df)\n",
    "# Calculate the mean along the desired axis (0 for column-wise mean)\n",
    "mean_series = df.mean(axis=1)\n",
    "print(mean_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83318bc",
   "metadata": {},
   "source": [
    "### Dividing thermals into categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47733e7",
   "metadata": {},
   "source": [
    "#### Divide thermals into strong and weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce087f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tracks_lt_10min=tdata_subset[wp_cents_list.groupby(\"cell\")[\"WP_centroid\"].transform(\"max\") <= \\\n",
    "                                  20.0]\n",
    "\n",
    "tracks_gt_10min=tdata_subset[wp_cents_list.groupby(\"cell\")[\"WP_centroid\"].transform(\"max\") > \\\n",
    "                                  20.0]\n",
    "\n",
    "cells_gt_10min = tracks_gt_10min.cell.unique()\n",
    "cells_gt_10min\n",
    "gt_10_min_files=sorted([csv_folder+'area_avgd_annulus_2mps_init_sounding_cell'+str(cc)+'_'+domain+'.csv' for cc in cells_gt_10min])\n",
    "print(len(gt_10_min_files))\n",
    "\n",
    "#gt_10_min_files\n",
    "\n",
    "df_gt_concat=pd.concat([pd.read_csv(fil) for fil in gt_10_min_files])\n",
    "df_gt_means = df_gt_concat.groupby(df_gt_concat.index).mean()\n",
    "df_gt_means\n",
    "\n",
    "cells_lt_10min = tracks_lt_10min.cell.unique()\n",
    "cells_lt_10min\n",
    "lt_10_min_files=sorted([csv_folder+'area_avgd_annulus_2mps_init_sounding_cell'+str(cc)+'_'+domain+'.csv' for cc in cells_lt_10min])\n",
    "print(len(lt_10_min_files))\n",
    "#lt_10_min_files\n",
    "\n",
    "df_lt_concat=pd.concat([pd.read_csv(fil) for fil in lt_10_min_files])\n",
    "df_lt_means = df_lt_concat.groupby(df_lt_concat.index).mean()\n",
    "df_lt_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadbc6d",
   "metadata": {},
   "source": [
    "#### Divide thermals into long and short lived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "division_condition='lt_5_gt_20_'\n",
    "\n",
    "tracks_lt_10min=tdata_subset[tdata_subset.groupby(\"cell\")[\"time_cell\"].transform(\"max\") <= \\\n",
    "                                  datetime.timedelta(minutes=5)]\n",
    "\n",
    "tracks_gt_10min=tdata_subset[tdata_subset.groupby(\"cell\")[\"time_cell\"].transform(\"max\") > \\\n",
    "                                  datetime.timedelta(minutes=20)]\n",
    "\n",
    "cells_gt_10min = tracks_gt_10min.cell.unique()\n",
    "cells_gt_10min\n",
    "gt_10_min_files=sorted([csv_folder+'area_avgd_annulus_2mps_init_sounding_cell'+str(cc)+'_'+domain+'.csv' for cc in cells_gt_10min])\n",
    "print(len(gt_10_min_files))\n",
    "\n",
    "#gt_10_min_files\n",
    "\n",
    "df_gt_concat=pd.concat([pd.read_csv(fil) for fil in gt_10_min_files])\n",
    "df_gt_means = df_gt_concat.groupby(df_gt_concat.index).mean()\n",
    "df_gt_means\n",
    "\n",
    "cells_lt_10min = tracks_lt_10min.cell.unique()\n",
    "cells_lt_10min\n",
    "lt_10_min_files=sorted([csv_folder+'area_avgd_annulus_2mps_init_sounding_cell'+str(cc)+'_'+domain+'.csv' for cc in cells_lt_10min])\n",
    "print(len(lt_10_min_files))\n",
    "#lt_10_min_files\n",
    "\n",
    "df_lt_concat=pd.concat([pd.read_csv(fil) for fil in lt_10_min_files])\n",
    "df_lt_means = df_lt_concat.groupby(df_lt_concat.index).mean()\n",
    "df_lt_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_skewT([df_lt_means.temp_degC.values *units('degC')  ,df_gt_means.temp_degC.values*units('degC')] ,\\\n",
    "           [df_lt_means.dewpt_degC.values*units('degC')  ,df_gt_means.dewpt_degC.values*units('degC')],\\\n",
    "           [df_lt_means.uwnd_mps.values  *units('m/s')   ,df_gt_means.uwnd_mps.values*units('m/s')]   ,\\\n",
    "           [df_lt_means.vwnd_mps.values  *units('m/s')   ,df_gt_means.vwnd_mps.values*units('m/s')]   ,\\\n",
    "           [df_lt_means.height_m.values  *units('meters'),df_gt_means.height_m.values*units('meters')],\\\n",
    "           [df_lt_means.pressure_hPa.values*units('hPa') ,df_gt_means.pressure_hPa.values*units('hPa')],\\\n",
    "           domain+' Initial area-averaged environment sounding around short- and long-lived cells',\\\n",
    "           'initial_area_avgd_sounding_short_long_lived_cells_'+domain+'_'+division_condition+'.png', True, False, ['Cell lifetime < 5 mins','Cell lifetime > 20 mins'],\\\n",
    "           BARB_INTERVAL=6,PLOT_PARCEL_PROFILE=None, PROF_TV=None, PRINT_INDICES=None, BUNKERS=None,\\\n",
    "           WRITE_SRH_OBS=None, L_OR_R=None, U_STORM_OBS=None, V_STORM_OBS=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_lt_10min = tracks_lt_10min.cell.unique()\n",
    "cells_lt_10min\n",
    "lt_10_min_files=sorted([csv_folder+'area_avgd_annulus_2mps_init_sounding_cell'+str(cc)+'_'+domain+'.csv' for cc in cells_lt_10min])\n",
    "print(len(lt_10_min_files))\n",
    "#lt_10_min_files\n",
    "\n",
    "df_lt_concat=pd.concat([pd.read_csv(fil) for fil in lt_10_min_files])\n",
    "df_lt_means = df_lt_concat.groupby(df_lt_concat.index).mean()\n",
    "df_lt_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_skewT([df_lt_means.temp_degC.values *units('degC')  ,df_gt_means.temp_degC.values*units('degC')] ,\\\n",
    "           [df_lt_means.dewpt_degC.values*units('degC')  ,df_gt_means.dewpt_degC.values*units('degC')],\\\n",
    "           [df_lt_means.uwnd_mps.values  *units('m/s')   ,df_gt_means.uwnd_mps.values*units('m/s')]   ,\\\n",
    "           [df_lt_means.vwnd_mps.values  *units('m/s')   ,df_gt_means.vwnd_mps.values*units('m/s')]   ,\\\n",
    "           [df_lt_means.height_m.values  *units('meters'),df_gt_means.height_m.values*units('meters')],\\\n",
    "           [df_lt_means.pressure_hPa.values*units('hPa') ,df_gt_means.pressure_hPa.values*units('hPa')],\\\n",
    "           'Initial area-averaged environment sounding around strong and weak cells',\\\n",
    "           'initial_area_avgd_sounding_weak_strong_lived_cells_'+domain+'.png', True, False, ['w_max =< 20 m/s','w_max > 20 m/s'],\\\n",
    "           BARB_INTERVAL=6,PLOT_PARCEL_PROFILE=None, PROF_TV=None, PRINT_INDICES=None, BUNKERS=None,\\\n",
    "           WRITE_SRH_OBS=None, L_OR_R=None, U_STORM_OBS=None, V_STORM_OBS=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32eb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_skewT(df_lt_means.temp_degC.values*units('degC'),df_lt_means.dewpt_degC.values*units('degC'),df_lt_means.uwnd_mps.values*units('m/s'),df_lt_means.vwnd_mps.values*units('m/s'),\\\n",
    "              df_lt_means.height_m.values*units('meters'),df_lt_means.pressure_hPa.values*units('hPa'),'Initial, area-averaged environment sounding around short-lived cell',\\\n",
    "              'area_avgd_initial_sounding_short-lived_cells_'+domain+'.png',BARB_INTERVAL=6,PLOT_PARCEL_PROFILE=None, PROF_TV=None, PRINT_INDICES=None, BUNKERS=None,\\\n",
    "              WRITE_SRH_OBS=None, L_OR_R=None, U_STORM_OBS=None, V_STORM_OBS=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9016d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_skewT(df_gt_means.temp_degC.values*units('degC'),df_gt_means.dewpt_degC.values*units('degC'),df_gt_means.uwnd_mps.values*units('m/s'),df_gt_means.vwnd_mps.values*units('m/s'),\\\n",
    "              df_gt_means.height_m.values*units('meters'),df_gt_means.pressure_hPa.values*units('hPa'),'Initial, area-averaged environment sounding around long-lived cell',\\\n",
    "              'area_avgd_lifetime_sounding_long-lived_cells_'+domain+'.png',BARB_INTERVAL=6,PLOT_PARCEL_PROFILE=None, PROF_TV=None, PRINT_INDICES=None, BUNKERS=None,\\\n",
    "              WRITE_SRH_OBS=None, L_OR_R=None, U_STORM_OBS=None, V_STORM_OBS=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbdae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain='WPO1.1-R'\n",
    "\n",
    "csv_folder='/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/'\n",
    "\n",
    "csv_files=sorted(glob.glob(csv_folder+'area_avgd_annulus_2mps_init_sounding_cell*'+domain+'.csv'))\n",
    "\n",
    "cells_processed=[]\n",
    "for fil in csv_files:\n",
    "    fname = os.path.basename(fil)\n",
    "    cellno = fname.split(\"_\")[6][4:]\n",
    "    cells_processed.append(int(cellno))\n",
    "  \n",
    "print(len(cells_processed))\n",
    "print(cells_processed)\n",
    "# remove problematic cells based on WP snapshots\n",
    "cells_processed = [item for item in cells_processed if item not in (5665, 7202, 3340, 3836, 4896, 5987)]#WPO1.1-R\n",
    "print(len(cells_processed))\n",
    "cells_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sounding_files = sorted(glob.glob('/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/area_avgd_annulus_2mps_init_sounding_cell*_DRC1.1-R.csv'))\n",
    "print(len(sounding_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe00e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_processed=[]\n",
    "for fil in sounding_files:\n",
    "    fname = os.path.basename(fil)\n",
    "    cellno = fname.split(\"_\")[6][4:]\n",
    "    cells_processed.append(int(cellno))\n",
    "    \n",
    "print(len(cells_processed))\n",
    "print(cells_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1de77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_from_RAMS_file(INPUT_FILE):\n",
    "    cur_time = os.path.split(INPUT_FILE)[1][4:21] # Grab time string from RAMS file\n",
    "    pd_time = pd.to_datetime(cur_time[0:10]+' '+cur_time[11:13]+\":\"+cur_time[13:15]+\":\"+cur_time[15:17])\n",
    "    return pd_time.strftime('%Y-%m-%d %H:%M:%S'), pd_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "domain='DRC1.1-R'\n",
    "\n",
    "path = '/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/' # Pleiades\n",
    "savepath = './'\n",
    "tobac_data='/nobackup/pmarines/DATA_FM/'+domain+'/tobac_data/' # Pleiades\n",
    "\n",
    "tobac_filename  = 'comb_track_filt_01_02_05_10_20.p'\n",
    "tobac_filepath  = tobac_data+tobac_filename\n",
    "\n",
    "\n",
    "# Grab all the rams files \n",
    "h5filepath = path+'a-L*g3.h5'\n",
    "h5files1 = sorted(glob.glob(h5filepath))\n",
    "hefilepath = path+'a-L*head.txt'\n",
    "hefiles1 = sorted(glob.glob(hefilepath))\n",
    "#print(h5files1)\n",
    "start_time=get_time_from_RAMS_file(h5files1[0])[0]\n",
    "end_time=get_time_from_RAMS_file(h5files1[-1])[0]\n",
    "print('Simulation name: ',domain)\n",
    "print('starting time in simulations: ',start_time)\n",
    "print('ending time in simulations: ',end_time)\n",
    "\n",
    "ds=xr.open_dataset(h5files1[-1],engine='h5netcdf', phony_dims='sort')\n",
    "#ds#.TOPT.values\n",
    "\n",
    "domain_z_dim,domain_y_dim,domain_x_dim=np.shape(ds.WP)\n",
    "print(domain_z_dim)\n",
    "print(domain_y_dim)\n",
    "print(domain_x_dim)\n",
    "\n",
    "zm, zt, nx, ny, dxy, npa = RAMS_fx.read_head(hefiles1[0],h5files1[0])\n",
    "\n",
    "#******************\n",
    "plotting_range = 125 # 25 km # need to know now for filtering cells close to edges\n",
    "#******************\n",
    "\n",
    "##### read in tobac data #####\n",
    "print('reading ',tobac_filepath)\n",
    "tdata = pd.read_pickle(tobac_filepath)\n",
    "\n",
    "def filt_positive_vert_vel(g):\n",
    "    return ((g.zmn.max() >= 1500.0) & (g.zmn.min() <= 15000.) & (g.X.max() <= domain_x_dim-plotting_range-1) & (g.X.min() >= plotting_range+1) &\n",
    "            (g.Y.max() <= domain_y_dim-plotting_range-1) & (g.Y.min() >= plotting_range+1) & (g.threshold_value.max() > 1.0))\n",
    "\n",
    "tdata_temp=tdata.groupby('cell').filter(filt_positive_vert_vel)\n",
    "all_cells = tdata_temp.cell.unique()\n",
    "print('number of unique cells identified: ',len(all_cells))\n",
    "############################################################################\n",
    "\n",
    "list_cells = [jj for jj in all_cells if jj not in cells_processed]\n",
    "print(len(list_cells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de473a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_cells))\n",
    "print(list_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ec99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=pd.read_csv(thermo_indices_data_csv_file)\n",
    "dd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1321af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def create_3dscatter_plot(DOMAIN):\n",
    "\n",
    "    thermo_csv_file = './thermodynamic_indices_init_'+DOMAIN+'.csv'\n",
    "    print('thermo file is : ',thermo_csv_file)\n",
    "    thermo_df = pd.read_csv(thermo_csv_file)\n",
    "    #print(len(thermo_df))\n",
    "    thermo_df = thermo_df.dropna(how='all').reset_index(drop=True)\n",
    "    #print(len(thermo_df))\n",
    "    thermo_df = thermo_df.set_index('cell')\n",
    "    #thermo_df\n",
    "    \n",
    "    wp_file = '/nobackupp11/isingh2/tobac_plots/sounding_csvs_and_WP_snapshots/'+\\\n",
    "                     DOMAIN+'_comb_track_filt_01_02_05_10_20_wp_max.csv'\n",
    "    print('wp_file file is : ',wp_file)\n",
    "    wp_df = pd.read_csv(wp_file)\n",
    "    wp_wmax_df = wp_df.groupby('cell').WP_max.max()\n",
    "    zmn_df = wp_df.groupby('cell').zmn.max()\n",
    "    #print(wp_wmax_df)\n",
    "    \n",
    "    total_df0 = pd.merge(wp_wmax_df,zmn_df, left_index=True, right_index=True)\n",
    "    total_df = pd.merge(thermo_df, total_df0, left_index=True, right_index=True)\n",
    "\n",
    "    fig = px.scatter_3d(total_df, x='SBCAPE', y='800-400 hPa RH', z='WP_max',\n",
    "                    color='zmn')#, symbol='species')\n",
    "    fig.show()\n",
    "    #filename = 'scatter_plot_'+THERMO_QUANTITY.replace(\" \", \"_\")+'_'+'w_max'+'_'+DOMAIN+'.png'\n",
    "    #print('output png: ',filename)\n",
    "    #plt.savefig(filename,dpi=200)\n",
    "    \n",
    "create_3dscatter_plot('DRC1.1-R')#,'SBCAPE','%')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "\n",
    "def randrange(n, vmin, vmax):\n",
    "    \"\"\"\n",
    "    Helper function to make an array of random numbers having shape (n, )\n",
    "    with each number distributed Uniform(vmin, vmax).\n",
    "    \"\"\"\n",
    "    return (vmax - vmin)*np.random.rand(n) + vmin\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "n = 100\n",
    "\n",
    "# For each set of style and range settings, plot n random points in the box\n",
    "# defined by x in [23, 32], y in [0, 100], z in [zlow, zhigh].\n",
    "for m, zlow, zhigh in [('o', -50, -25), ('^', -30, -5)]:\n",
    "    xs = randrange(n, 23, 32)\n",
    "    ys = randrange(n, 0, 100)\n",
    "    zs = randrange(n, zlow, zhigh)\n",
    "    ax.scatter(xs, ys, zs, marker=m)\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcdc7e9-6559-43d2-b0a3-063d382310d4",
   "metadata": {},
   "source": [
    "# RAMS trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5176a3-28e0-4dc7-93b7-541efcd9c8c9",
   "metadata": {},
   "source": [
    "## Read in RAMS data\n",
    "\n",
    "* User must insert path to data\n",
    "    * If model output is one file use ***xr.open_dataset***\n",
    "    * If model output is in multiple files use ***xr.openmfdataset***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebcdda3-8da3-4b22-8b77-7f2ec8b87e72",
   "metadata": {},
   "source": [
    "### Load in tobac file and select a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e773c-7ff8-4abe-a227-33449684341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time\n",
    "from RAMS_functions import read_head, get_time_from_RAMS_file\n",
    "from scipy import interpolate\n",
    "\n",
    "domain = 'DRC1.1-R'\n",
    "\n",
    "tobac_filepath  = '/monsoon/pmarin/Tracking/Updrafts/'+domain+'/tobac_data/comb_track_filt_01_02_05_10_20.p' # CSU machine\n",
    "##### read in tobac data #####\n",
    "print('reading ',tobac_filepath)\n",
    "tdata = pd.read_pickle(tobac_filepath)\n",
    "\n",
    "print('number of unique cells identified: ',len(tdata.cell.unique()))\n",
    "all_cells=np.array(tdata.cell.unique())\n",
    "\n",
    "cl = random.choice(all_cells) \n",
    "\n",
    "print('# unique cells ',len(all_cells))\n",
    "print('randomly selected cell ',cl)\n",
    "\n",
    "tdata_subset=tdata[tdata.cell==cl]\n",
    "print('subset of tdata for this cell: ',tdata_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62151d4e-9b31-4cbd-aec7-d67f500e6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobac_cell_xpos =  int(tdata_subset.hdim_2.values[-1])   \n",
    "tobac_cell_ypos =  int(tdata_subset.hdim_1.values[-1])\n",
    "tobac_cell_zpos =  int(tdata_subset.vdim.values[-1])\n",
    "\n",
    "print('tobac_cell_xpos: ',tobac_cell_xpos)\n",
    "print('tobac_cell_ypos: ',tobac_cell_ypos)\n",
    "print('tobac_cell_zpos: ',tobac_cell_zpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099e8d6-a6cd-4750-b7b0-9dee01ce929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use xarray to open model output and specify chunking if data set is large (set by user)\n",
    "times_tracked = tdata_subset.timestr.values\n",
    "print('times for this cells: ',times_tracked)\n",
    "\n",
    "tim_pd   = pd.to_datetime(times_tracked)\n",
    "\n",
    "fi_list = []\n",
    "\n",
    "#rams_fil=glob.glob('/nobackup/pmarines/DATA_FM/'+domain+'/LES_data/a-L-'+tim_pd.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5')[0] # Pleiades\n",
    "for tim in tim_pd:\n",
    "    fi_list.append(glob.glob('/monsoon/LES_MODEL_DATA/'+domain+'/G3/out_30s/'+'a-L-'+tim.strftime(\"%Y-%m-%d-%H%M%S\")+'-g3.h5')[0]) # CSU machine\n",
    "\n",
    "    \n",
    "rams_header_file = sorted(glob.glob(os.path.dirname(fi_list[0])+'/a-L*head.txt'))[0]\n",
    "print('rams header file: ',rams_header_file)\n",
    "zm, zt, nx, ny, dxy, npa = read_head(rams_header_file,fi_list[0])\n",
    "\n",
    "for fi in fi_list:\n",
    "    print(fi)\n",
    "    \n",
    "print(len(fi_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb51e9-958e-4162-959a-d44a1feff491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_dimenion(ds):\n",
    "    source_file = ds.encoding[\"source\"]\n",
    "    source_time = pd.to_datetime(get_time_from_RAMS_file(source_file)[0])\n",
    "    return ds.assign(time=source_time)[['UP','VP','WP','RV','TOPT']].rename_dims({'phony_dim_3':'z','phony_dim_2':'y','phony_dim_1':'x'})\n",
    " \n",
    "\n",
    "ds_sample = xr.open_dataset(fi_list[0],engine='h5netcdf', phony_dims='sort')\n",
    "ds = xr.open_mfdataset(fi_list,engine='h5netcdf', phony_dims='sort', preprocess = add_time_dimenion, combine='nested', concat_dim='time')#,chunks={'time': 1}\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a131947-7aff-45ec-82b9-9fa32af0f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbdea24-3e6e-4c98-a6d8-30d6660b09e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get model output dimensions\n",
    "temp_var = ds_sample.UP\n",
    "nx = np.shape(temp_var)[2] #Number of gridpoints in x\n",
    "ny = np.shape(temp_var)[1] #Number of gridpoints in y\n",
    "nz = np.shape(temp_var)[0] #Number of gridpoints in z\n",
    "\n",
    "x = np.arange(0,nx,1)\n",
    "y = np.arange(0,ny,1)\n",
    "z = np.arange(0,nz,1)\n",
    "\n",
    "print('nx: ',nx)\n",
    "print('ny: ',ny)\n",
    "print('nz: ',nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1ae68-c1dd-4d8f-ab92-9f7399bddef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ni = ds.dims['ni']\n",
    "# nj = ds.dims['nj']\n",
    "# nk = ds.dims['nk']\n",
    "\n",
    "dx = 100.0\n",
    "dy = 100.0\n",
    "\n",
    "xh = np.linspace(dx/2,dx*nx,nx)\n",
    "yh = np.linspace(dy/2,dy*ny,ny)\n",
    "zs = ds_sample.TOPT.values\n",
    "zh = zs[np.newaxis,:,:]+zt[:,np.newaxis,np.newaxis]\n",
    "\n",
    "\n",
    "print('xh :',xh)\n",
    "print('yh :',yh)\n",
    "#print('zh :',zh)\n",
    "\n",
    "print('xh shape: ',np.shape(xh))\n",
    "print('yh shape: ',np.shape(yh))\n",
    "print('zh shape: ',np.shape(zh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca312a6-db0f-4fde-aff0-27d7660e95a2",
   "metadata": {},
   "source": [
    "## Initialize Parcels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d087078b-f579-439c-ad8c-96e1a2d95be0",
   "metadata": {},
   "source": [
    "User must enter desired trajectory characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3d24e-3e13-44bc-a6f0-4beb95c8443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of parcels in vertical (can be more than number of vertical levels; set by user) \n",
    "num_seeds_z = 4#205\n",
    "\n",
    "#Number of parcels in y (set by user) \n",
    "num_seeds_y = 4#300\n",
    "\n",
    "#Number of parcels in x (set by user) \n",
    "num_seeds_x = 4#125\n",
    "\n",
    "#Number of time steps to run trajectories back (set by user) \n",
    "time_steps = len(fi_list)-1\n",
    "print('number of time steps: ',time_steps)\n",
    "incre = 1\n",
    "\n",
    "#Time step to start backward trajectories at (set by user) \n",
    "start_time_step = len(fi_list) - 1 \n",
    "print('start_time_step: ',start_time_step)\n",
    "\n",
    "#Variable to record at each parcel's location throughout trajectory (code can be easily modified to add more; set by user) \n",
    "var_name1 = 'RV'\n",
    "#var_name2 = 'qv'\n",
    "#var_name3 = 'prs'\n",
    "\n",
    "#var_name4 = 'qvb_mp'\n",
    "#var_name5 = 'qvb_pbl'\n",
    "\n",
    "#var_name6 = 'ub_pbl'\n",
    "#var_name7 = 'ub_pgrad'\n",
    "#var_name8 = 'ub_cor'\n",
    "\n",
    "#var_name9 = 'vb_pbl'\n",
    "#var_name10 = 'vb_pgrad'\n",
    "#var_name11 = 'vb_cor'\n",
    "\n",
    "#var_name12 = 'wb_pgrad'\n",
    "#var_name13 = 'wb_buoy'\n",
    "\n",
    "#var_name14 = 'ptb_div'\n",
    "#var_name15 = 'ptb_mp'\n",
    "#var_name16 = 'ptb_pbl'\n",
    "#var_name17 = 'ptb_rad'\n",
    "\n",
    "#var_name18 = 'xvort'\n",
    "#var_name19 = 'yvort'\n",
    "#var_name20 = 'zvort'\n",
    "\n",
    "#Set as 'Y' or 'N' for 'yes' or 'no' if the u, v, and w model output is on the staggered grid \n",
    "#(unless you have interpolated u, v, and w to the scalar grid, they are most likely on the staggered grid (set by user)\n",
    "staggered = 'N'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91a6a2-1658-46ad-94f2-ced3f228eb3f",
   "metadata": {},
   "source": [
    "**Model output info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b7639-77dc-4b8b-ac60-d5ca32043aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Horizontal resolution of model output (meters)\n",
    "hor_resolution = 100.0 #(ds.xf[0,1].values-ds.xf[0,0].values)*1000.0\n",
    "#print('horizontal resolution is '+str(hor_resolution))\n",
    "#Vertical resolution of model output (meters). Changes in x and y, if there is terrain, and z, if grid is stretched.\n",
    "#try:\n",
    "vert_resolution = zh[1:,:,:]-zh[:-1,:,:]\n",
    "print('shape of vertical resolution is ',np.shape(vert_resolution))\n",
    "print('vertical resolution is ',vert_resolution)\n",
    "print('Output has terrain')\n",
    "    \n",
    "# except:\n",
    "#     vert_res1d = (ds.z[1:].values-ds.z[:-1].values)*1000\n",
    "#     vert_res2d = np.repeat(vert_res1d,ds.ny, axis = 0).reshape(ds.nz-1, ds.ny)\n",
    "#     vert_resolution = np.repeat(vert_res2d,ds.nx, axis = 0).reshape(ds.nz-1, ds.ny, ds.nx)\n",
    "#     print('vertical resolution is '+str(vert_resolution))\n",
    "#     print('Output does not have terrain')\n",
    "            \n",
    "#Model output time step length (seconds)\n",
    "time_step_length = 30 # (ds.time[1].values - ds.time[0].values)/np.timedelta64(1, 's')*incre\n",
    "print('time step in seconds is '+str(time_step_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d97e6-c671-4730-afbb-9fc9291c6e27",
   "metadata": {},
   "source": [
    "**Create empty arrays to store x, y, and z positions of parcels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab4dea-894f-48ff-8921-123ee8b77f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpos      = np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #x-location (grid points on staggered grid)\n",
    "print('size of xpos is ',np.shape(xpos))\n",
    "ypos      = np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #y-location (grid points on staggered grid)\n",
    "zpos      = np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #z-location (grid points on staggered grid)\n",
    "zpos_heightASL \\\n",
    "          = np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #Height above sea level (meters)\n",
    "zpos_vert_res  \\\n",
    "          = np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #Vertical grid spacing at parcel location (meters)\n",
    "variable1 =    \\\n",
    "            np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable2 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable3 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "\n",
    "# variable4 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable5 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable6 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable7 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable8 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable9 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable10 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable11 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable12 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable13 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable14 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable15 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable16 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable17 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable18 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable19 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track\n",
    "# variable20 =    \\\n",
    "#             np.zeros((time_steps, num_seeds_z, num_seeds_y, num_seeds_x)) #User specified variable to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ba23a-6d64-484f-910a-95150e4a96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x-position\n",
    "nn = 10\n",
    "\n",
    "for i in range(num_seeds_x):\n",
    "    xpos[0,:,:,i] =  int(tobac_cell_xpos-nn*(num_seeds_x/2)) + nn*i #This example initializes all seeds at same x-position (1000th x-grpt, set by user)\n",
    "\n",
    "#y-position   \n",
    "for i in range(num_seeds_y):\n",
    "    ypos[0,:,i,:] =  int(tobac_cell_ypos-nn*(num_seeds_y/2)) + nn*i #This example initializes seeds evenly in y-dimension (0th, 4th, 8th, etc. y-grpt; set by user)\n",
    "\n",
    "#z-position\n",
    "for i in range(num_seeds_z):\n",
    "    zpos[0,i,:,:] = int(tobac_cell_zpos-(num_seeds_z/2)) + i  #This example initializes seeds evenly starting in z-dimension (0th, 1st, 2nd, etc., z-grpt; set by user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32828b2a-3631-4b12-9e0c-9f9b802e1894",
   "metadata": {},
   "source": [
    "## Determine Initial Height of Parcels Above Sea Level\n",
    "Use the height of the models levels (meters above sea level) to convert from terrain following grid points to height above seal level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a2d59-0b0c-45fd-8faf-823c1017698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get height of surface\n",
    "# try:\n",
    "#     zs = ds.TOPO[0,:,:].values\n",
    "#     print('Output has terrain')\n",
    "# except: \n",
    "#     zs = np.zeros((ds.ny,ds.nx))\n",
    "#     print('Output does not have terrain')\n",
    "\n",
    "# #Get height of vertical coordinates (scalar grid)\n",
    "# try:\n",
    "#     zh = ds.zh[0,:,:,:].values\n",
    "#     print('Output has terrain')\n",
    "# except:\n",
    "#     zh1d = (ds.z[:].values)*1000\n",
    "#     zh2d = np.repeat(zh1d,ds.ny, axis = 0).reshape(ds.nz, ds.ny)\n",
    "#     zh = np.repeat(zh2d,ds.nx, axis = 0).reshape(ds.nz, ds.ny, ds.nx)\n",
    "#     print('Output does not have terrain')\n",
    "\n",
    "#Create list of initial coordinates to get height\n",
    "xloc = (xpos[0,:,:,:]).flatten()\n",
    "yloc = (ypos[0,:,:,:]).flatten()\n",
    "zloc = (zpos[0,:,:,:]).flatten()\n",
    "coord_height = []\n",
    "for i in range(len(xloc)):\n",
    "    coord_height.append((zloc[i], yloc[i], xloc[i]))\n",
    "\n",
    "#Get the actual inital height of the parcels in meters above sea level\n",
    "zpos_heightASL[0,:,:] = np.reshape(interpolate.interpn((z,y,x), zh, coord_height, method='linear', bounds_error=False, fill_value= 0), (num_seeds_z, num_seeds_y, num_seeds_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7570a3bc-d5bb-4622-bda8-1676d675542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "levels_terrain = np.arange(100.0,1000.0,100.0)\n",
    "terr_ct = plt.contourf(zs,cmap=plt.get_cmap('terrain'),alpha=0.2) #linewidths=0.75\n",
    "#C1 = plt.contourf(ds.zs[0,:,:],levels_terrain,cmap=\"terrain\",alpha=0.2)\n",
    "\n",
    "for ii in range(num_seeds_x):\n",
    "    for jj in range(num_seeds_y):\n",
    "        plt.plot(xpos[0,1,jj,ii],ypos[0,1,jj,ii],color='orangered',label='model lev 1',marker='.',ms=1)\n",
    "        \n",
    "        \n",
    "plt.plot(tobac_cell_xpos,tobac_cell_ypos,marker='^',color='black')\n",
    "\n",
    "plt.xlabel('West-East distance (km)',fontsize=16)\n",
    "plt.ylabel('North-south distance (km)',fontsize=16)\n",
    "#plt.title('Trajectories: Top and Bottom',fontsize=22)\n",
    "#plt.axvline(x=594,color='lime',linestyle='--',lw=2)\n",
    "#matplotlib.rc('xtick', labelsize=16) \n",
    "#matplotlib.rc('ytick', labelsize=16) \n",
    "#plt.legend()\n",
    "#plt.savefig('trajectories.png',dpi=200.)\n",
    "\n",
    "plt.colorbar(terr_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7d4ff-eaa4-4075-89e4-d286a9a8179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x-locations of particles are:\")\n",
    "print('np.shape of xloc ',np.shape(xloc))\n",
    "print(xloc)\n",
    "print(\"y-locations of particles are:\")\n",
    "print('np.shape of yloc ',np.shape(yloc))\n",
    "print(yloc)\n",
    "print(\"z-locations of particles are:\")\n",
    "#print(zloc)\n",
    "print('zpos_heightASL: ', zpos_heightASL)\n",
    "print('coord_height: ', coord_height)\n",
    "print('------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef8ea29-8c21-4aa1-92ed-cd5aef9e7824",
   "metadata": {},
   "source": [
    "## Calculate Trajectories\n",
    "The method used to calculate trajectories is a second-order semi-implicit discretization in space and time. The method is described in depth in section 2.1 pf Miltenberger et al. 2013 (https://www.geosci-model-dev.net/6/1989/2013/gmd-6-1989-2013.pdf)\n",
    "Unless user is changing trajectories from backwards to forwards, nothing should be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c4fe07-0e0b-44aa-bdf5-ad6aed00810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop over all time steps and compute trajectory\n",
    "print(\"no. of steps to be taken: \"+str(time_steps))\n",
    "\n",
    "for t in range(time_steps-1):\n",
    "    print('t = ',t)\n",
    "    \n",
    "    start = time.time() #Timer\n",
    "    \n",
    "    ##########################################################################################################\n",
    "    ##########################################################################################################   \n",
    "    ##################### Get data for 'first guess' step of integration scheme ##############################\n",
    "    ##########################################################################################################\n",
    "    ##########################################################################################################\n",
    "    \n",
    "    xmin = int(np.nanmin(xpos[t,:,:])-2)\n",
    "    xmin = 0 if xmin < 0 else xmin\n",
    "    \n",
    "    xmax = int(np.nanmax(xpos[t,:,:])+2)\n",
    "    xmax = nx if xmax > nx else xmax\n",
    "    \n",
    "    ymin = int(np.nanmin(ypos[t,:,:])-2)\n",
    "    ymin = 0 if ymin < 0 else ymin\n",
    "    \n",
    "    ymax = int(np.nanmax(ypos[t,:,:])+2)\n",
    "    ymax = ny if ymax > ny else ymax\n",
    "    \n",
    "    zmin = int(np.nanmin(zpos[t,:,:])-2)\n",
    "    zmin = 0 if zmin < 0 else zmin\n",
    "    \n",
    "    zmax = int(np.nanmax(zpos[t,:,:])+2)\n",
    "    zmax = nz if zmax > nz else zmax\n",
    "    \n",
    "    print('xmin: ',xmin)\n",
    "    print('xmax: ',xmax)\n",
    "    print('ymin: ',ymin)\n",
    "    print('ymax: ',ymax)\n",
    "    print('zmin: ',zmin)\n",
    "    print('zmax: ',zmax)\n",
    "    \n",
    "    x_fast = np.arange(0,xmax-xmin)\n",
    "    y_fast = np.arange(0,ymax-ymin)\n",
    "    z_fast = np.arange(0,zmax-zmin)\n",
    "    \n",
    "    #Get model data\n",
    "    t = int(t * incre)\n",
    "    \n",
    "    u = ds.UP[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values\n",
    "    v = ds.VP[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values\n",
    "    w = ds.WP[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values\n",
    "    \n",
    "    #var1 = getattr(ds,var_name1)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values     #th\n",
    "    exner = getattr(ds,\"PI\")[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values/1004.0\n",
    "    var1  = getattr(ds,\"THETA\")[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values*exner\n",
    "    #print('density: ',density)\n",
    "    #var2 = getattr(ds,var_name2)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values     #qv\n",
    "    #var3 = getattr(ds,var_name3)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values     #prs\n",
    "    #var4 = getattr(ds,var_name4)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values     #qvb_mp\n",
    "    #var5 = getattr(ds,var_name5)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values     #qvb_pbl\n",
    "    \n",
    "    #var6 = wrf.destagger(getattr(ds2,var_name6)[start_time_step-t,:,:,:],2,meta=True)\n",
    "    #var6 = var6[zmin:zmax,ymin:ymax,xmin:xmax].values                                         #ub_pbl\n",
    "    #var7 = wrf.destagger(getattr(ds2,var_name7)[start_time_step-t,:,:,:],2,meta=True)\n",
    "    #var7 = var7[zmin:zmax,ymin:ymax,xmin:xmax].values                                         #ub_pgrad\n",
    "    #var8 = wrf.destagger(getattr(ds2,var_name8)[start_time_step-t,:,:,:],2,meta=True)\n",
    "    #var8 = var8[zmin:zmax,ymin:ymax,xmin:xmax].values                                         #ub_cor\n",
    "    \n",
    "\n",
    "    #var9 = wrf.destagger(getattr(ds2,var_name9)[start_time_step-t,:,:,:],1,meta=True)\n",
    "    #var9 = var9[zmin:zmax,ymin:ymax,xmin:xmax].values                                         #vb_pbl\n",
    "    #var10 = wrf.destagger(getattr(ds2,var_name10)[start_time_step-t,:,:,:],1,meta=True)\n",
    "    #var10 = var10[zmin:zmax,ymin:ymax,xmin:xmax].values                                       #vb_pgrad\n",
    "    #var11 = wrf.destagger(getattr(ds2,var_name11)[start_time_step-t,:,:,:],1,meta=True)\n",
    "    #var11 = var11[zmin:zmax,ymin:ymax,xmin:xmax].values                                       #vb_cor\n",
    "    \n",
    "    #var12 = wrf.destagger(getattr(ds2,var_name12)[start_time_step-t,:,:,:],0,meta=True)\n",
    "    #var12 = var12[zmin:zmax,ymin:ymax,xmin:xmax].values                                       #wb_pgrad\n",
    "    #var13 = wrf.destagger(getattr(ds2,var_name13)[start_time_step-t,:,:,:],0,meta=True)\n",
    "    #var13 = var13[zmin:zmax,ymin:ymax,xmin:xmax].values                                       #wb_buoy\n",
    "    \n",
    "    #var14 = getattr(ds2,var_name14)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values   #ptb_div\n",
    "    #var15 = getattr(ds2,var_name15)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values   #ptb_mp\n",
    "    #var16 = getattr(ds2,var_name16)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values   #ptb_pbl\n",
    "    #var17 = getattr(ds2,var_name17)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values   #ptb_rad\n",
    "\n",
    "    #var18 = getattr(ds,var_name18)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values   #xvort\n",
    "    #var19 = getattr(ds,var_name19)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values   #yvort\n",
    "    #var20 = getattr(ds,var_name20)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values   #zvort\n",
    "\n",
    "    \n",
    "    t = int(t/incre) \n",
    "    \n",
    "#     #Smooth data\n",
    "#     smooth = 20\n",
    "#     u = np.copy(scipy.ndimage.filters.uniform_filter(u, smooth))\n",
    "#     v = np.copy(scipy.ndimage.filters.uniform_filter(v, smooth))\n",
    "#     w = np.copy(scipy.ndimage.filters.uniform_filter(w, smooth))\n",
    "    \n",
    "    ############## Generate coordinates for interpolations ###############\n",
    "\n",
    "    #x, y, and z on staggered and scalar grids\n",
    "    xloc = np.copy(xpos[t,:,:]).flatten()-xmin\n",
    "    xloc_stag = np.copy(xpos[t,:,:]+0.5).flatten()-xmin\n",
    "    yloc = np.copy(ypos[t,:,:]).flatten()-ymin\n",
    "    yloc_stag = np.copy(ypos[t,:,:]+0.5).flatten()-ymin\n",
    "    zloc = np.copy(zpos[t,:,:]).flatten()-zmin\n",
    "    zloc_stag = np.copy(zpos[t,:,:]+0.5).flatten()-zmin\n",
    "\n",
    "    #If u, v, and w are staggered, generate three staggered sets of coordinates:\n",
    "    #    1) u-grid (staggered in x)\n",
    "    #    2) v-grid (staggered in y)\n",
    "    #    3) w-grid (staggered in z)\n",
    "    \n",
    "    if staggered == 'Y':\n",
    "        coord_u = []\n",
    "        coord_v = []\n",
    "        coord_w = []\n",
    "        for i in range(len(xloc)):\n",
    "            coord_u.append((zloc[i], yloc[i], xloc_stag[i])) \n",
    "            coord_v.append((zloc[i], yloc_stag[i], xloc[i])) \n",
    "            coord_w.append((zloc_stag[i], yloc[i], xloc[i])) \n",
    "    \n",
    "    #If not, generate scalar coordinates\n",
    "    else: \n",
    "        coord_u = []\n",
    "        coord_v = []\n",
    "        coord_w = []\n",
    "        for i in range(len(xloc)):\n",
    "            coord_u.append((zloc[i], yloc[i], xloc[i])) \n",
    "            coord_v.append((zloc[i], yloc[i], xloc[i])) \n",
    "            coord_w.append((zloc[i], yloc[i], xloc[i])) \n",
    "    \n",
    "    #Scalar coordinates for all other variables\n",
    "    coord = []\n",
    "    coord_fast = []\n",
    "    for i in range(len(xloc)):\n",
    "        coord.append((zloc[i]+zmin, yloc[i]+ymin, xloc[i]+xmin)) \n",
    "        coord_fast.append((zloc[i], yloc[i], xloc[i])) \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########################################################################################################   \n",
    "    ########################## Integrate 'first guess' of parcel's new location ##############################\n",
    "    ##########################################################################################################   \n",
    "\n",
    "    \n",
    "    #########################   Calc 'first guess' new xpos in grdpts   #######################################\n",
    "    dx_0 = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), u, coord_u, method='linear', bounds_error=False, fill_value=np.nan)*time_step_length/hor_resolution, (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "    xpos_1 = xpos[t,:,:] - dx_0\n",
    "\n",
    "    #########################   Calc 'first guess' new ypos in grdpts   #######################################\n",
    "    dy_0 = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), v, coord_v, method='linear', bounds_error=False, fill_value=np.nan)*time_step_length/hor_resolution, (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "    ypos_1 = ypos[t,:,:] - dy_0\n",
    "\n",
    "    #########################   Calc 'first guess' new zpos in meters above sea level ######################################\n",
    "    dz_0 = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), w, coord_w, method='linear', bounds_error=False, fill_value= 0)*time_step_length, (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "    zpos_heightASL_1 = zpos_heightASL[t,:,:] - dz_0\n",
    "    \n",
    "    ############# Convert zpos from meters above sea level to gridpts abve surface for interpolation #########\n",
    "    #Get vertical grid spacing at each parcel's location\n",
    "    zpos_vert_res[t,:,:] = np.reshape(interpolate.interpn((z[:-1],y,x), vert_resolution, coord, method='linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "\n",
    "\n",
    "    #Calculate change in surface height and change in parcel height\n",
    "    xloc = np.copy(xpos[t,:,:]).flatten()\n",
    "    yloc = np.copy(ypos[t,:,:]).flatten()\n",
    "    coord_zs1 = []\n",
    "    for i in range(len(xloc)):\n",
    "        coord_zs1.append((yloc[i], xloc[i]))\n",
    "        \n",
    "    xloc = np.copy(xpos_1).flatten()\n",
    "    yloc = np.copy(ypos_1).flatten()\n",
    "    coord_zs2 = []\n",
    "    for i in range(len(xloc)):\n",
    "        coord_zs2.append((yloc[i], xloc[i]))\n",
    "    \n",
    "    #Change in surface height over last timestep\n",
    "    zs1 = interpolate.interpn((y,x), zs, coord_zs1, method='linear', bounds_error=False, fill_value= np.nan)\n",
    "    zs2 = interpolate.interpn((y,x), zs, coord_zs2, method='linear', bounds_error=False, fill_value= np.nan)\n",
    "    zs_change = zs2-zs1\n",
    "    \n",
    "    #Change in parcel height over last times step\n",
    "    zpos_heightASL_change = zpos_heightASL_1.flatten()-zpos_heightASL[t,:,:].flatten()\n",
    "    \n",
    "    #Calculate zpos in grdpts above surface\n",
    "    zpos_1 = zpos[t,:,:] + np.reshape((zpos_heightASL_change - zs_change)/zpos_vert_res[t,:,:].flatten(), (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "    \n",
    "    #Prevent parcels from going into the ground\n",
    "    zpos_heightASL_1 = zpos_heightASL_1.clip(min=0)\n",
    "    zpos_1 = zpos_1.clip(min=0)\n",
    "    \n",
    "    \n",
    "    #Calculate value of variable at each parcel's location\n",
    "    variable1[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var1, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable2[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var2, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable3[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var3, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable4[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var4, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable5[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var5, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable6[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var6, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable7[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var7, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable8[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var8, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable9[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var9, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable10[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var10, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "    #variable11[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var11, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x))##########################################################################################################\n",
    "    #variable12[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var12, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable13[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var13, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable14[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var14, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable15[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var15, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable16[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var16, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "    #variable17[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var17, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x))##########################################################################################################\n",
    "    #variable18[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var18, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x)) \n",
    "    #variable19[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var19, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "    #variable20[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var20, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x))##########################################################################################################\n",
    "    \n",
    "    ##########################################################################################################   \n",
    "    ##################### Get data for 'correction' step of integration scheme ###############################\n",
    "    ##########################################################################################################\n",
    "    ##########################################################################################################\n",
    "    \n",
    "    xmin = int(np.nanmin(xpos_1)-2)\n",
    "    xmin = 0 if xmin < 0 else xmin\n",
    "    \n",
    "    xmax = int(np.nanmax(xpos_1)+2)\n",
    "    xmax = nx if xmax > nx else xmax\n",
    "    \n",
    "    ymin = int(np.nanmin(ypos_1)-2)\n",
    "    ymin = 0 if ymin < 0 else ymin\n",
    "    \n",
    "    ymax = int(np.nanmax(ypos_1)+2)\n",
    "    ymax = ny if ymax > ny else ymax\n",
    "    \n",
    "    zmin = int(np.nanmin(zpos_1)-2)\n",
    "    zmin = 0 if zmin < 0 else zmin\n",
    "    \n",
    "    zmax = int(np.nanmax(zpos_1)+2)\n",
    "    zmax = nz if zmax > nz else zmax\n",
    "    \n",
    "    x_fast = np.arange(0,xmax-xmin)\n",
    "    y_fast = np.arange(0,ymax-ymin)\n",
    "    z_fast = np.arange(0,zmax-zmin)\n",
    "    \n",
    "    #Get model data for next time step\n",
    "    t = t + 1\n",
    "    t = int(t * incre)\n",
    "    \n",
    "    u = ds.UP[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values\n",
    "    v = ds.VP[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values\n",
    "    w = ds.WP[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values\n",
    "    \n",
    "    t = int(t/incre)\n",
    "    t = t - 1\n",
    "    \n",
    "#     #Smooth data\n",
    "#     smooth = 20\n",
    "#     u = np.copy(scipy.ndimage.filters.uniform_filter(u, smooth))\n",
    "#     v = np.copy(scipy.ndimage.filters.uniform_filter(v, smooth))\n",
    "#     w = np.copy(scipy.ndimage.filters.uniform_filter(w, smooth))\n",
    "#     var1 = np.copy(scipy.ndimage.filters.uniform_filter(var1, smooth))\n",
    "    \n",
    "        \n",
    "        \n",
    "    ############## Generate coordinates for interpolations ###############\n",
    "\n",
    "    #x, y, and z on staggered and scalar grids\n",
    "    xloc = np.copy(xpos_1).flatten()-xmin\n",
    "    xloc_stag = np.copy(xpos_1+0.5).flatten()-xmin\n",
    "    yloc = np.copy(ypos_1).flatten()-ymin\n",
    "    yloc_stag = np.copy(ypos_1+0.5).flatten()-ymin\n",
    "    zloc = np.copy(zpos_1).flatten()-zmin\n",
    "    zloc_stag = np.copy(zpos_1+0.5).flatten()-zmin\n",
    "\n",
    "    #If u, v, and w are staggered, generate three staggered sets of coordinates:\n",
    "    #    1) u-grid (staggered in x)\n",
    "    #    2) v-grid (staggered in y)\n",
    "    #    3) w-grid (staggered in z)\n",
    "    \n",
    "    if staggered == 'Y':\n",
    "        coord_u = []\n",
    "        coord_v = []\n",
    "        coord_w = []\n",
    "        for i in range(len(xloc)):\n",
    "            coord_u.append((zloc[i], yloc[i], xloc_stag[i])) \n",
    "            coord_v.append((zloc[i], yloc_stag[i], xloc[i])) \n",
    "            coord_w.append((zloc_stag[i], yloc[i], xloc[i])) \n",
    "    \n",
    "    #If not, generate scalar coordinates\n",
    "    else: \n",
    "        coord_u = []\n",
    "        coord_v = []\n",
    "        coord_w = []\n",
    "        for i in range(len(xloc)):\n",
    "            coord_u.append((zloc[i], yloc[i], xloc[i])) \n",
    "            coord_v.append((zloc[i], yloc[i], xloc[i])) \n",
    "            coord_w.append((zloc[i], yloc[i], xloc[i])) \n",
    "    \n",
    "    #Scalar coordinates for all other variables\n",
    "    coord = []\n",
    "    coord_fast = []\n",
    "    for i in range(len(xloc)):\n",
    "        coord.append((zloc[i]+zmin, yloc[i]+ymin, xloc[i]+xmin)) \n",
    "        coord_fast.append((zloc[i], yloc[i], xloc[i])) \n",
    "    \n",
    "    ##########################################################################################################   \n",
    "    ########################## Integrate 'correction' of parcel's new location ###############################\n",
    "    ##########################################################################################################   \n",
    "\n",
    "    \n",
    "    #########################   Calc 'correction' new xpos in grdpts   #######################################\n",
    "    dx_1 = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), u, coord_u, method='linear', bounds_error=False, fill_value=np.nan)*time_step_length/hor_resolution, (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "    xpos[t+1,:,:] = xpos[t,:,:] - (dx_0 + dx_1)/2\n",
    "\n",
    "    #########################   Calc 'correction' new ypos in grdpts   #######################################\n",
    "    dy_1 = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), v, coord_v, method='linear', bounds_error=False, fill_value=np.nan)*time_step_length/hor_resolution, (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "    ypos[t+1,:,:] = ypos[t,:,:] - (dy_0 + dy_1)/2\n",
    "\n",
    "    #########################   Calc 'correction' new zpos in meters above sea level ######################################\n",
    "    dz_1 = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), w, coord_w, method='linear', bounds_error=False, fill_value= 0)*time_step_length, (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "    zpos_heightASL[t+1,:,:] = zpos_heightASL[t,:,:] - (dz_0 + dz_1)/2\n",
    "    \n",
    "    \n",
    "    \n",
    "    ############# Convert zpos from meters above sea level to gridpts abve surface for interpolation #########\n",
    "    #Get vertical grid spacing at each parcel's location\n",
    "    zpos_vert_res[t,:,:] = np.reshape(interpolate.interpn((z[:-1],y,x), vert_resolution, coord, method='linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "\n",
    "    \n",
    "    #Calculate change in surface height and change in parcel height\n",
    "    xloc = np.copy(xpos[t,:,:]).flatten()\n",
    "    yloc = np.copy(ypos[t,:,:]).flatten()\n",
    "    coord_zs1 = []\n",
    "    for i in range(len(xloc)):\n",
    "        coord_zs1.append((yloc[i], xloc[i]))\n",
    "        \n",
    "    xloc = np.copy(xpos[t+1,:,:]).flatten()\n",
    "    yloc = np.copy(ypos[t+1,:,:]).flatten()\n",
    "    coord_zs2 = []\n",
    "    for i in range(len(xloc)):\n",
    "        coord_zs2.append((yloc[i], xloc[i]))\n",
    "    \n",
    "    #Change in surface height over last timestep\n",
    "    zs1 = interpolate.interpn((y,x), zs, coord_zs1, method='linear', bounds_error=False, fill_value= np.nan)\n",
    "    zs2 = interpolate.interpn((y,x), zs, coord_zs2, method='linear', bounds_error=False, fill_value= np.nan)\n",
    "    zs_change = zs2-zs1\n",
    "    \n",
    "    #Change in parcel height over last times step\n",
    "    zpos_heightASL_change = zpos_heightASL[t+1,:,:].flatten()-zpos_heightASL[t,:,:].flatten()\n",
    "    \n",
    "    #Calculate zpos in grdpts above surface\n",
    "    zpos[t+1,:,:] = zpos[t,:,:] + np.reshape((zpos_heightASL_change - zs_change)/zpos_vert_res[t,:,:].flatten(), (num_seeds_z, num_seeds_y, num_seeds_x))\n",
    "    \n",
    "\n",
    "    ##########################################################################################################\n",
    "    \n",
    "    \n",
    "    #Prevent parcels from going into the ground\n",
    "    zpos = zpos.clip(min=0)\n",
    "    zpos_heightASL = zpos_heightASL.clip(min=0)\n",
    "    \n",
    "    #Timer\n",
    "    stop = time.time()\n",
    "    print(\"Integration {:01d} took {:.2f} seconds\".format(t, stop-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af26cf3-29ab-42c5-94b3-f6264c4f3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(z_fast))\n",
    "print(np.shape(y_fast))\n",
    "print(np.shape(x_fast))\n",
    "print(np.shape(u))\n",
    "print(np.shape(coord_u))\n",
    "print((num_seeds_z))\n",
    "print((num_seeds_y))\n",
    "print((num_seeds_x))\n",
    "print(time_step_length)\n",
    "print(hor_resolution)\n",
    "print(interpolate.interpn((z_fast,y_fast,x_fast), u, coord_u, method='linear', bounds_error=False, fill_value=np.nan)*time_step_length/hor_resolution)\n",
    "#dx_0 = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), u, coord_u, method='linear', bounds_error=False, fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e70fb8-a0bc-4fd2-be33-c1ae763b5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56f0a6-5243-4297-bf53-e23e727354a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name1 = 'th'\n",
    "var_name2 = 'qv'\n",
    "var_name3 = 'prs'\n",
    "\n",
    "\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/RW_LLJ_50_percent_xpos_'+str(start_time_step)+'_'+str(time_steps), xpos)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/RW_LLJ_50_percent_ypos_'+str(start_time_step)+'_'+str(time_steps), ypos)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/RW_LLJ_50_percent_zpos_'+str(start_time_step)+'_'+str(time_steps), zpos_heightASL)\n",
    "\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/RW_LLJ_50_percent_th_traj_'+str(start_time_step)+'_'+str(time_steps), variable1)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/RW_LLJ_50_percentqv_traj_'+str(start_time_step)+'_'+str(time_steps), variable2)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/RW_LLJ_50_percentprs_traj_'+str(start_time_step)+'_'+str(time_steps), variable3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9f32a3-b2ba-45d8-8043-f47902c5b4e1",
   "metadata": {},
   "source": [
    "## Plot trajectories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c445e-cb3b-4982-8388-d4a672925bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpos=np.load('RW_LLJ_50_percent_xpos_66_61.npy')\n",
    "ypos=np.load('RW_LLJ_50_percent_ypos_66_61.npy')\n",
    "zpos=np.load('RW_LLJ_50_percent_zpos_66_61.npy')\n",
    "\n",
    "th=np.load('RW_LLJ_50_percent_th_traj_66_61.npy')\n",
    "qv=np.load('RW_LLJ_50_percentqv_traj_66_61.npy')\n",
    "prs=np.load('RW_LLJ_50_percentprs_traj_66_61.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6fc23-6c31-4171-a213-186c681e3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpos_xr=xr.DataArray(xpos,coords=[('time',pd.date_range(end='2018-11-10 17:00:00',periods=61,freq='5min')),('nk',np.arange(0,30,1)),('nj',np.arange(400,700,1))\\\n",
    "                              ,('ni',np.arange(500,625,1))],attrs={'short_name':'xpos','long_name':'x-position','units':'km'})\n",
    "ypos_xr=xr.DataArray(ypos,coords=[('time',pd.date_range(end='2018-11-10 17:00:00',periods=61,freq='5min')),('nk',np.arange(0,30,1)),('nj',np.arange(400,700,1))\\\n",
    "                              ,('ni',np.arange(500,625,1))],attrs={'short_name':'ypos','long_name':'y-position','units':'km'})\n",
    "zpos_xr=xr.DataArray(zpos,coords=[('time',pd.date_range(end='2018-11-10 17:00:00',periods=61,freq='5min')),('nk',np.arange(0,30,1)),('nj',np.arange(400,700,1))\\\n",
    "                              ,('ni',np.arange(500,625,1))],attrs={'short_name':'zpos','long_name':'z-position','units':'km'})\n",
    "th_xr=xr.DataArray(th,coords=[('time',pd.date_range(end='2018-11-10 17:00:00',periods=61,freq='5min')),('nk',np.arange(0,30,1)),('nj',np.arange(400,700,1))\\\n",
    "                              ,('ni',np.arange(500,625,1))],attrs={'short_name':'th','long_name':'potential temperature','units':'K'})\n",
    "prs_xr=xr.DataArray(prs,coords=[('time',pd.date_range(end='2018-11-10 17:00:00',periods=61,freq='5min')),('nk',np.arange(0,30,1)),('nj',np.arange(400,700,1))\\\n",
    "                              ,('ni',np.arange(500,625,1))],attrs={'short_name':'prs','long_name':'pressure','units':'Pa'})\n",
    "qv_xr=xr.DataArray(qv,coords=[('time',pd.date_range(end='2018-11-10 17:00:00',periods=61,freq='5min')),('nk',np.arange(0,30,1)),('nj',np.arange(400,700,1))\\\n",
    "                              ,('ni',np.arange(500,625,1))],attrs={'short_name':'qv','long_name':'mixing ratio','units':'kg/kg'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea759a2e-b422-4e4b-b1f6-0f0653b83b59",
   "metadata": {},
   "source": [
    "## Save Trajectory Data\n",
    "The x, y, and z positions and user-specified variable values are saved in 3D numpy arrays. The first dimension is time and the other two are the positions and values of variables of all the parcels at that specifc time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dbbdc4-0441-4a7d-b9ae-e3df1842799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name1 = 'th'\n",
    "var_name2 = 'qv'\n",
    "var_name3 = 'prs'\n",
    "\n",
    "var_name4 = 'qvb_mp'\n",
    "var_name5 = 'qvb_pbl'\n",
    "\n",
    "var_name6 = 'ub_pbl'\n",
    "var_name7 = 'ub_pgrad'\n",
    "var_name8 = 'ub_cor'\n",
    "\n",
    "var_name9 = 'vb_pbl'\n",
    "var_name10 = 'vb_pgrad'\n",
    "var_name11 = 'vb_cor'\n",
    "\n",
    "var_name12 = 'wb_pgrad'\n",
    "var_name13 = 'wb_buoy'\n",
    "\n",
    "var_name14 = 'ptb_div'\n",
    "var_name15 = 'ptb_mp'\n",
    "var_name16 = 'ptb_pbl'\n",
    "var_name17 = 'ptb_rad'\n",
    "\n",
    "var_name18 = 'xvort'\n",
    "var_name19 = 'yvort'\n",
    "var_name20 = 'zvort'\n",
    "\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/xpos_'+str(start_time_step)+'_'+str(time_steps), xpos)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/ypos_'+str(start_time_step)+'_'+str(time_steps), ypos)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/zpos_'+str(start_time_step)+'_'+str(time_steps), zpos_heightASL)\n",
    "\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/th_traj_'+str(start_time_step)+'_'+str(time_steps), variable1)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/qv_traj_'+str(start_time_step)+'_'+str(time_steps), variable2)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/prs_traj_'+str(start_time_step)+'_'+str(time_steps), variable3)\n",
    "\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/qvb_mp_traj_'+str(start_time_step)+'_'+str(time_steps), variable4)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/qvb_pbl_traj_'+str(start_time_step)+'_'+str(time_steps), variable5)\n",
    "\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/ub_pbl_traj_'+str(start_time_step)+'_'+str(time_steps), variable6)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/ub_pgrad_traj_'+str(start_time_step)+'_'+str(time_steps), variable7)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/ub_cor_traj_'+str(start_time_step)+'_'+str(time_steps), variable8)\n",
    "\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/vb_pbl_traj_'+str(start_time_step)+'_'+str(time_steps), variable9)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/vb_pgrad_traj_'+str(start_time_step)+'_'+str(time_steps), variable10)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/vb_cor_traj_'+str(start_time_step)+'_'+str(time_steps), variable11)\n",
    "\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/wb_pgrad_traj_'+str(start_time_step)+'_'+str(time_steps), variable12)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/wb_buoy_traj_'+str(start_time_step)+'_'+str(time_steps), variable13)\n",
    "\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/ptb_div_traj_'+str(start_time_step)+'_'+str(time_steps), variable14)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/ptb_mp_traj_'+str(start_time_step)+'_'+str(time_steps), variable15)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/ptb_pbl_traj_'+str(start_time_step)+'_'+str(time_steps), variable16)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/ptb_rad_traj_'+str(start_time_step)+'_'+str(time_steps), variable17)\n",
    "\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/xvort_traj_'+str(start_time_step)+'_'+str(time_steps), variable18)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/yvort_traj_'+str(start_time_step)+'_'+str(time_steps), variable19)\n",
    "np.save('/glade/u/home/isingh9/jupyter_nbks/zvort_traj_'+str(start_time_step)+'_'+str(time_steps), variable20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "685px",
    "left": "36px",
    "top": "111.141px",
    "width": "287.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
